./CLAUDE.md
---
Always run things via uv run $file

Always add packages via uv add $dep

Import things via absolute imports


Design code so you can run things and validate as you go (without requiring too much time to run each file, always add debug mode that runs quickly), and break into as small changes as possible and test them incrementally

Use git! after each change, make git commits etc

IMPORTANT: You must always break changes into the smallest unit possible -- if I ask you to do something, NEVER just go off and start editing files unless it's trivial. Instead, propose a plan, ask for my input, and then make the smallest increment change possible and run it every time to confirm it works

Prioritize simple code that works on one path -- if there's a bug with data shape, find the source of the bug rather than adding defnesive checks. Similarly, when refactoring unless I explicitly ask don't create legacy versions of the code to support both -- just update it so everything works with the new one.

---
./CONCLUSIONS.md
---
# Hard Negative Metrics: Conclusions

Based on our comprehensive analysis and testing, we've identified and addressed two separate issues with hard negative metrics:

## Issue 1: Bug in Metric Calculation

**Problem**: The original `evaluate.py` implementation had a bug in how it calculated hard negative rankings, leading to mathematically impossible MRR values (>1.0).

**Fix**: We created `evaluate_fixed.py` with corrected implementation that:
- Properly tracks question IDs and their answer indices
- Handles different question ID formats (tensors, strings, etc.)
- Calculates metrics correctly for each question group

**Results**: The fixed metrics show more realistic values that follow the expected MRR range (0-1).

## Issue 2: Flat Hard Negative Metrics During Training

**Problem**: Even with the fixed evaluation, hard negative metrics remain flat during training at around 0.8, showing little to no improvement.

**Root Cause Analysis**:
1. **High Similarity Values**: Our similarity analysis shows all embeddings have high similarity (0.90-0.95), with little differentiation between correct and incorrect answers
2. **Minimal Separation**: The answer separation (difference between self-similarity and hard negative similarity) is minimal, centered around 0
3. **Loss Function Limitations**: Standard InfoNCE focuses on separating answers to different questions but doesn't specifically target hard negatives

**Solution**: We created an enhanced InfoNCE loss that adds an explicit penalty for high similarity between questions and incorrect answers to the same question.

```python
# Add extra penalty for hard negatives
hard_negative_loss = 0.0
for q_id, indices in question_groups.items():
    if len(indices) <= 1:
        continue
    
    for idx in indices:
        other_indices = [j for j in indices if j != idx]
        hard_neg_sim = similarity[idx, other_indices]
        hard_negative_loss += torch.mean(hard_neg_sim)

# Scale and add to original loss
total_loss = infonce_loss + (hard_negative_loss / hard_negative_pairs) * hard_negative_weight
```

**Simulation Results**: Our training simulation confirms that:
1. The original implementation produces invalid metrics that get worse during training
2. The fixed implementation shows more realistic values that improve with training
3. Answer separation gradually increases during training with proper weight adjustments

## Recommendations

1. **Use Fixed Evaluation**: Always use the fixed implementation from `evaluate_fixed.py`
2. **Use Enhanced Loss**: Use `EnhancedInfoNCELoss` with weight 2.0-3.0 for training
3. **Consider Additional Approaches**:
   - Hard negative mining to focus on the most difficult cases
   - Different similarity functions (beyond dot product)
   - Modified embedding architecture to allow for greater separation

## Validation

Our analysis has been validated through:
1. **Mathematical Analysis**: Ensuring metrics respect their defined bounds
2. **Controlled Tests**: Testing on synthetic data with known properties
3. **Code Analysis**: Identifying and fixing the specific bug
4. **Simulation**: Demonstrating expected behavior during training

These findings provide a solid foundation for improving the model's ability to distinguish between correct and incorrect answers to the same question, which is crucial for effective question-answering systems.

---
./README.md
---
# QA Embeddings Loss Comparison

A system to compare different loss functions for embedding models on question-answer retrieval tasks.

## Overview

This project compares how different loss functions perform when training embedding models for retrieving relevant answers to questions. The project uses StackExchange data to train models on real-world question-answer pairs with upvote-based relevance scores.

## Features

- Flexible dataset with configurable batch transformations:
  - InfoNCE with in-batch negatives
  - Multiple positives per question
  - Hard negative sampling
  - Triplet format
  - Listwise ranking format
- Comprehensive loss function implementations:
  - Standard InfoNCE (with temperature scaling)
  - Rank-weighted InfoNCE
  - Hard negative InfoNCE
  - Multiple positives loss
  - Triplet loss
  - Listwise ranking loss
- Standardized evaluation metrics:
  - MRR (Mean Reciprocal Rank)
  - NDCG@k (Normalized Discounted Cumulative Gain)
  - MAP@k (Mean Average Precision)
  - Hard negative accuracy
- Modular architecture for easy extension and experimentation
- Integration with Weights & Biases for experiment tracking
- Hardware acceleration detection (MPS, CUDA, CPU)

## Getting Started

### Prerequisites

- Python 3.8+
- [uv](https://github.com/astral-sh/uv) (Python package management)
- Kaggle API credentials (for dataset download)

### Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/yourusername/qa-embeddings-loss-comparison.git
cd qa-embeddings-loss-comparison
uv pip install -e .
```

### Running Experiments

To train a model with a predefined configuration:

```bash
uv run rank-test --config configs/test.json
```

To create a custom configuration, you can copy and modify one of the existing configuration files in `configs/`.

## Batch Transformation Strategies

The system uses a flexible data processing approach with various batch transformation strategies:

| Strategy | Description | Best Used With |
|----------|-------------|----------------|
| `infonce` | Standard InfoNCE format with each question paired with a top answer | StandardInfoNCELoss, RankInfoNCELoss |
| `multiple_positives` | Each question appears multiple times with different positive answers | MultiplePositivesLoss |
| `hard_negative` | Explicitly includes lower-ranked answers as hard negatives | HardNegativeInfoNCELoss |
| `triplet` | Creates triplets of (query, positive answer, negative answer) | TripletLoss |
| `listwise` | Prepares multiple ranked answers per question for listwise ranking | ListwiseRankingLoss |
| `standardized_test` | Standard evaluation format for fair comparison | Used automatically for evaluation |

Each transformation prepares data in a specific format suitable for its corresponding loss function. The transforms can be configured with additional parameters:

- `take_top`: Whether to use the highest-ranked answer (True) or a random answer (False)
- `pos_count`: Maximum number of positive answers to include per question
- `neg_strategy`: Strategy for selecting negatives in triplet format ("hard_negative", "in_batch", "mixed")
- `max_answers`: Maximum number of answers to include per question in listwise format

## Loss Functions

The project implements the following loss functions:

### StandardInfoNCELoss

Standard InfoNCE contrastive loss that contrasts positive pairs against in-batch negatives.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)

### RankInfoNCELoss

InfoNCE loss that leverages rank or score information from the dataset.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)
- `use_ranks`: Whether to use ordinal rank information (default: True)
- `use_scores`: Whether to use cardinal score information (default: False)
- `rank_weight`: Weight for rank-based penalties (default: 0.1)
- `score_weight`: Weight for score-based adjustments (default: 0.01)

### HardNegativeInfoNCELoss

InfoNCE loss with additional penalty for hard negatives.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)
- `hard_negative_weight`: Weight for the hard negative component (default: 1.0)

### MultiplePositivesLoss

Loss function that handles multiple positive answers per question.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)
- `rank_weight`: Weight for rank-based penalties (default: 0.1)

### TripletLoss

Triplet loss for query-positive-negative triplets.

**Parameters:**
- `margin`: Margin between positive and negative similarities (default: 0.3)

### ListwiseRankingLoss

Listwise ranking loss for learning to rank multiple answers.

**Parameters:**
- `temperature`: Temperature for scaling similarities (default: 1.0)

## Configuration Options

The system uses a strong typing configuration system based on Pydantic. Key configuration options include:

**Dataset and Data Processing:**
- `data_path`: Path to the JSON dataset with ranked QA pairs
- `limit`: Maximum number of questions to process
- `test_size`: Fraction of data to use for testing
- `seed`: Random seed for reproducibility
- `batch_size`: Batch size for training
- `dataset_strategy`: Dataset strategy to use (currently supports "flexible")
- `batch_transform`: Batch transformation strategy to use

**Model Architecture:**
- `embed_dim`: Embedding dimension for the base model
- `projection_dim`: Projection dimension for the final embeddings

**Training Parameters:**
- `epochs`: Number of training epochs
- `learning_rate`: Learning rate for optimization
- `eval_steps`: Number of steps between evaluations
- `eval_at_zero`: Whether to evaluate before training
- `checkpoint_interval`: Number of epochs between checkpoints

**Loss Function Parameters:**
- `loss_type`: Type of loss function to use
- `temperature`: Temperature parameter for scaling similarity scores
- `margin`: Margin for triplet loss
- `use_ranks`: Whether to use rank information
- `use_scores`: Whether to use score information
- `rank_weight`: Weight for rank-based adjustments
- `score_weight`: Weight for score-based adjustments
- `hard_negative_weight`: Weight for hard negative penalty

**Output and Logging:**
- `output_dir`: Directory to save outputs
- `log_to_wandb`: Whether to log to Weights & Biases
- `wandb_project`: Weights & Biases project name

Example configuration:
```json
{
  "name": "Test",
  "data_path": "data/ranked_qa.json",
  "limit": 100,
  "test_size": 0.02,
  "seed": 42,
  "embed_dim": 768,
  "projection_dim": 128,
  "batch_size": 16,
  "epochs": 1,
  "learning_rate": 2e-5,
  "eval_steps": 50,
  "eval_at_zero": true,
  "debug": false,
  "output_dir": "models/flexible",
  "log_to_wandb": false,
  "dataset_strategy": "flexible",
  "batch_transform": "infonce",
  "pos_count": 3,
  "loss_type": "infonce",
  "temperature": 0.1,
  "use_ranks": true,
  "use_scores": false,
  "rank_weight": 0.1
}
```

## Evaluation Metrics

The system provides comprehensive evaluation metrics:

- **MRR**: Mean Reciprocal Rank - measures how highly the first relevant document is ranked
- **Accuracy@k**: Percentage of queries where a relevant document is ranked in the top k
- **NDCG@k**: Normalized Discounted Cumulative Gain - measures ranking quality considering the graded relevance of documents
- **MAP@k**: Mean Average Precision - measures precision at different recall levels
- **Hard Negative Accuracy**: Measures the model's ability to distinguish between positive answers and hard negative answers (lower-ranked answers to the same question)

## Project Structure

- `src/rank_test/`
  - `dataset.py`: Dataset class and data loading utilities
  - `transforms.py`: Batch transformation strategies
  - `models.py`: Model architecture definitions
  - `losses.py`: Loss function implementations
  - `train.py`: Training loop and utilities
  - `evaluate.py`: Evaluation metrics and utilities
  - `config.py`: Experiment configuration
  - `run_experiment.py`: Experiment runner for comparing multiple losses

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---
./README_FLEXIBLE_DATASET.md
---
# Flexible QA Ranking Dataset

This module provides a flexible framework for working with ranked question-answer datasets. It allows for various sampling strategies and loss functions to experiment with different approaches to QA ranking.

## Key Features

- **Flexible Data Sampling**: Control which QA pairs are included in training
- **Multiple Loss Functions**: Support for InfoNCE, Triplet, Listwise and other losses
- **Rank vs. Score Handling**: Properly distinguishes between ordinal ranks and cardinal scores
- **Customizable Batching**: Different batch structures for different loss functions

## Quick Start

Here's a simple example of how to use the flexible dataset:

```python
from rank_test.flexible_dataset import FlexibleQADataset, infonce_batch_transform
from rank_test.flexible_losses import StandardInfoNCELoss

# Create dataset with standard InfoNCE format
dataset = FlexibleQADataset(
    'data/ranked_qa.json',
    batch_transform_fn=infonce_batch_transform,
    batch_size=16,
    take_top=True  # Use top-ranked answer for each question
)

# Create dataloader
dataloader = FlexibleQADataset.get_dataloader(dataset)

# Create loss function
loss_fn = StandardInfoNCELoss(temperature=0.1)

# In your training loop...
for batch in dataloader:
    # Get embeddings
    q_embeddings = model(batch['q_input_ids'])
    a_embeddings = model(batch['a_input_ids'])
    
    # Calculate loss
    loss, metrics = loss_fn(q_embeddings, a_embeddings)
    
    # Backpropagate etc.
```

## Transformation Strategies

### 1. InfoNCE Batch Transform

Standard contrastive learning approach where each question is paired with one answer (default: top-ranked).

```python
dataset = FlexibleQADataset(
    data_path,
    batch_transform_fn=infonce_batch_transform,
    take_top=True  # or False to randomly select answers
)
```

### 2. Multiple Positives Batch Transform

Each question appears multiple times with different positive answers.

```python
dataset = FlexibleQADataset(
    data_path,
    batch_transform_fn=multiple_positives_batch_transform,
    pos_count=3  # Number of positives per question
)
```

### 3. Hard Negative Batch Transform

Explicitly includes lower-ranked answers as hard negatives.

```python
dataset = FlexibleQADataset(
    data_path,
    batch_transform_fn=hard_negative_batch_transform
)
```

### 4. Triplet Batch Transform

Creates query-positive-negative triplets for triplet loss training.

```python
dataset = FlexibleQADataset(
    data_path,
    batch_transform_fn=triplet_batch_transform,
    neg_strategy="hard_negative"  # or "in_batch" or "mixed"
)
```

### 5. Listwise Batch Transform

Groups multiple answers per question for listwise ranking.

```python
dataset = FlexibleQADataset(
    data_path,
    batch_transform_fn=listwise_batch_transform,
    max_answers=5  # Maximum answers per question
)
```

## Loss Functions

### 1. Standard InfoNCE Loss

```python
loss_fn = StandardInfoNCELoss(temperature=0.1)
```

### 2. Rank-Aware InfoNCE Loss

Supports both ordinal rank and cardinal score information:

```python
# Rank-based weighting
loss_fn = RankInfoNCELoss(
    temperature=0.1, 
    use_ranks=True, 
    rank_weight=0.1
)

# Score-based weighting
loss_fn = RankInfoNCELoss(
    temperature=0.1, 
    use_scores=True, 
    score_weight=0.05
)

# Combined approach
loss_fn = RankInfoNCELoss(
    temperature=0.1, 
    use_ranks=True, 
    use_scores=True,
    rank_weight=0.1,
    score_weight=0.05
)
```

### 3. Hard Negative InfoNCE Loss

```python
loss_fn = HardNegativeInfoNCELoss(
    temperature=0.1, 
    hard_negative_weight=1.0
)
```

### 4. Multiple Positives Loss

```python
loss_fn = MultiplePositivesLoss(
    temperature=0.1, 
    rank_weight=0.1
)
```

### 5. Triplet Loss

```python
loss_fn = TripletLoss(margin=0.3)
```

### 6. Listwise Ranking Loss

```python
loss_fn = ListwiseRankingLoss(temperature=1.0)
```

## Using the Factory Function

You can also create losses using the factory function:

```python
from rank_test.flexible_losses import create_flexible_loss

# Basic losses
loss_fn = create_flexible_loss('infonce', temperature=0.1)
loss_fn = create_flexible_loss('triplet', margin=0.3)

# Specialized InfoNCE with options in the name
loss_fn = create_flexible_loss('infonce_rank0.2_score0.1_t0.1')
```

## Best Practices

1. **Ranks vs. Scores**:
   - **Ordinal Ranks**: Position in sorted order (0 = best)
   - **Cardinal Scores**: Actual score values (higher = better)

2. **Batch Size**: 
   - Make sure your batch contains enough variety for contrastive learning
   - For techniques like InfoNCE, larger batches provide more negatives

3. **Temperature**:
   - Lower values make model more confident
   - Common values range from 0.05 to 1.0
   - Experiment to find the right balance

4. **Negative Mining**:
   - Hard negatives improve model discrimination
   - Consider mixing in-batch and hard negatives

## Demo Example

See `examples/flexible_dataset_demo.py` for a complete demonstration of the different dataset strategies and loss functions.

```bash
# Run the demo
uv run examples/flexible_dataset_demo.py
```

---
./plan.md
---
# Implementation Plan for Comparing Loss Functions in QA Retrieval

## Overview
This plan outlines how we'll compare different loss functions for retrieving relevant answers from a list of answers. We'll implement a system that allows for easy comparison of models trained with different loss functions.

## Loss Functions to Implement
1. InfoNCE loss (current implementation)
2. InfoNCE without in-batch negatives
3. InfoNCE with no hard negatives (only query-top answer pairs)
4. MSE loss on normalized upvotes
5. Triplet loss with upvote-based positives
6. Listwise ranking losses (with and without in-batch negatives)

## Evaluation Metrics
- NDCG@k
- MAP@k
- MRR (Mean Reciprocal Rank)

## Architecture Design

### 1. Core Components
- **Loss Module**: Implement all loss functions with a consistent interface
- **Dataset Module**: Support different batching strategies required by each loss
- **Model Module**: Flexible encoding architecture that works with all losses
- **Evaluation Module**: Standardized metrics and evaluation pipeline
- **Training Module**: Unified training loop for all losses
- **Experiment Config**: Configuration system for easy experimentation

### 2. Directory Structure
```
src/rank_test/
  ├── dataset.py      # Dataset loading and preprocessing
  ├── losses.py       # Loss function implementations
  ├── models.py       # Model architecture
  ├── train.py        # Training loop
  ├── evaluate.py     # Evaluation metrics
  ├── config.py       # Experiment configuration
  ├── run_experiment.py # Main experiment runner
```

## Implementation Steps

### Phase 1: Loss Functions (2 days)
- Create a base loss class with a common interface
- Implement all required loss functions
- Add tests to verify each loss function works correctly

### Phase 2: Dataset Handling (1 day)
- Extend the existing dataset loader to support train/test splits
- Create specialized dataset classes for different losses
- Implement batching strategies for each loss type

### Phase 3: Evaluation Pipeline (1 day)
- Implement evaluation metrics
- Create standardized evaluation procedures
- Add support for model comparison

### Phase 4: Training Pipeline (1 day)
- Create a unified training loop that works with any loss
- Add support for early stopping and checkpointing
- Integrate with wandb for experiment tracking

### Phase 5: Experiment System (1 day)
- Create a configuration system for experiment parameters
- Implement experiment running script
- Add support for hyperparameter tuning

### Phase 6: Experimentation and Analysis (2 days)
- Run experiments with different losses
- Compare results and analyze performance
- Create visualizations to illustrate differences

## Specific Design Details

### Loss Functions Interface
```python
class BaseLoss:
    def __call__(self, q_embeddings, a_embeddings, **kwargs):
        """Calculate loss and metrics"""
        raise NotImplementedError
        
    def get_batch_metrics(self, batch_output):
        """Get metrics for logging during training"""
        raise NotImplementedError
```

### Dataset Design
```python
class QADataset:
    """Base dataset with train/test/val splitting"""
    
    def get_train_dataloader(self, batch_size):
        """Return dataloader for training data"""
        
    def get_test_dataloader(self, batch_size):
        """Return dataloader for test data"""
```

### Evaluation Design
```python
def evaluate_model(model, test_dataloader, metrics=['ndcg@5', 'ndcg@10', 'map@5', 'mrr']):
    """Evaluate model performance with given metrics"""
    
def compare_models(model_results):
    """Compare multiple models across metrics"""
```

### Experiment Configuration
```python
class ExperimentConfig:
    """Configuration for a single experiment"""
    
    def __init__(self, loss_type, **kwargs):
        # Set defaults and override with provided values
        
    def create_loss(self):
        """Create loss function based on configuration"""
```

## Testing Strategy
1. Create a small test dataset subset for quick validation
2. Compare loss values against expected values with known inputs
3. Verify gradient flow and model updates during training
4. Check evaluation metrics against known ranking orders

## Success Criteria
- All loss functions successfully implemented and tested
- Standard evaluation metrics correctly implemented
- Easy to run experiments with different losses
- Clear comparison of loss function performance

---
./run_hard_negative_tests.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Script to run all tests related to the hard negative issue.
"""

import os
import sys
import pytest
import argparse


def parse_args():
    parser = argparse.ArgumentParser(description="Run tests for hard negative issue.")
    parser.add_argument("--unit", action="store_true", help="Run unit tests")
    parser.add_argument("--integration", action="store_true", help="Run integration tests")
    parser.add_argument("--debug", action="store_true", help="Run debugging-focused tests")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
    parser.add_argument("--all", action="store_true", help="Run all tests")
    
    args = parser.parse_args()
    
    # If no test type is specified, run all tests
    if not (args.unit or args.integration or args.debug or args.all):
        args.all = True
    
    return args


def main():
    args = parse_args()
    
    # Base pytest options
    pytest_args = []
    if args.verbose:
        pytest_args.append("-v")
    
    # Collect test files to run
    test_files = []
    
    if args.all or args.unit:
        test_files.append("tests/unit/test_hard_negative_evaluation.py")
    
    if args.all or args.integration:
        test_files.append("tests/integration/test_hard_negative_pipeline.py")
    
    if args.all or args.debug:
        test_files.append("tests/unit/test_hard_negative_debugging.py")
    
    # Run tests
    print(f"Running tests: {', '.join(test_files)}")
    
    if not test_files:
        print("No tests selected to run.")
        return
    
    pytest_args.extend(test_files)
    
    return pytest.main(pytest_args)


if __name__ == "__main__":
    sys.exit(main() or 0)

---
./tests/unit/test_flexible_dataset.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Unit tests for flexible dataset module.
Tests the different batch transformation strategies.
"""

import os
import pytest
import torch
import json
import tempfile
from transformers import DistilBertTokenizer

from rank_test.flexible_dataset import (
    FlexibleQADataset,
    infonce_batch_transform,
    multiple_positives_batch_transform,
    hard_negative_batch_transform,
    triplet_batch_transform,
    listwise_batch_transform,
    get_batch_transform
)

# Create a mock dataset for testing
@pytest.fixture
def mock_qa_data():
    """Create a mock dataset for testing"""
    data = [
        {
            "question": {
                "id": "q1",
                "title": "Question 1",
                "body": "What is the best way to test code?"
            },
            "answers": [
                {
                    "id": "a1",
                    "body": "Use pytest for testing",
                    "score": 95  # Cardinal score (e.g., user rating or upvotes)
                },
                {
                    "id": "a2",
                    "body": "Use unittest for testing",
                    "score": 82
                },
                {
                    "id": "a3",
                    "body": "Use pytest with coverage for testing",
                    "score": 78
                },
                {
                    "id": "a4",
                    "body": "Use a combination of pytest and unittest",
                    "score": 65
                },
                {
                    "id": "a5",
                    "body": "Manual testing works too",
                    "score": 35
                }
            ]
        },
        {
            "question": {
                "id": "q2",
                "title": "Question 2",
                "body": "How to implement a dataset in PyTorch?"
            },
            "answers": [
                {
                    "id": "a6",
                    "body": "Inherit from torch.utils.data.Dataset",
                    "score": 92
                },
                {
                    "id": "a7",
                    "body": "Override __getitem__ and __len__",
                    "score": 88
                },
                {
                    "id": "a8",
                    "body": "Create a class with __getitem__, __len__, and collate_fn",
                    "score": 75
                },
                {
                    "id": "a9",
                    "body": "Use DataLoader with your custom Dataset class",
                    "score": 62
                }
            ]
        },
        {
            "question": {
                "id": "q3",
                "title": "Question 3",
                "body": "What is a loss function?"
            },
            "answers": [
                {
                    "id": "a10",
                    "body": "A function that measures model error",
                    "score": 90
                },
                {
                    "id": "a11",
                    "body": "A metric that quantifies the difference between predicted and actual values",
                    "score": 85
                },
                {
                    "id": "a12",
                    "body": "A mathematical function that evaluates how well a model performs",
                    "score": 70
                }
            ]
        }
    ]
    
    # Create a temporary file with the data
    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as f:
        json.dump(data, f)
        temp_path = f.name
    
    yield temp_path
    
    # Cleanup
    os.unlink(temp_path)

@pytest.fixture
def mock_tokenizer():
    """Return a mock tokenizer for testing"""
    # For testing, we can use a simple tokenizer
    class MockTokenizer:
        def __call__(self, text, max_length=10, padding=None, truncation=None, return_tensors=None):
            # Just return a fixed tensor for testing
            return {
                'input_ids': torch.ones(1, max_length, dtype=torch.long),
                'attention_mask': torch.ones(1, max_length, dtype=torch.long)
            }
    
    return MockTokenizer()


def test_get_batch_transform():
    """Test getting batch transform functions by name"""
    # Test valid transform names
    assert get_batch_transform('infonce') == infonce_batch_transform
    assert get_batch_transform('multiple_positives') == multiple_positives_batch_transform
    assert get_batch_transform('hard_negative') == hard_negative_batch_transform
    assert get_batch_transform('triplet') == triplet_batch_transform
    assert get_batch_transform('listwise') == listwise_batch_transform
    
    # Test invalid transform name
    with pytest.raises(ValueError):
        get_batch_transform('invalid_transform')


def test_infonce_batch_transform(mock_qa_data, mock_tokenizer):
    """Test infonce batch transform"""
    with open(mock_qa_data, 'r') as f:
        data = json.load(f)
    
    # Test with top answers
    batch = infonce_batch_transform(data, mock_tokenizer, max_length=10, take_top=True)
    
    # Validate batch format
    assert 'q_input_ids' in batch
    assert 'a_input_ids' in batch
    assert 'question_ids' in batch
    assert 'ranks' in batch
    assert 'scores' in batch
    
    # Check shapes
    assert batch['q_input_ids'].shape[0] == len(data)
    assert batch['a_input_ids'].shape[0] == len(data)
    assert len(batch['question_ids']) == len(data)
    
    # Test all ranks are 0 (top answers)
    assert (batch['ranks'] == 0).all()
    
    # Test with random answers
    batch_random = infonce_batch_transform(data, mock_tokenizer, max_length=10, take_top=False)
    
    # Validate batch format
    assert 'q_input_ids' in batch_random
    assert 'a_input_ids' in batch_random
    assert 'question_ids' in batch_random
    assert 'ranks' in batch_random
    
    # Check shapes
    assert batch_random['q_input_ids'].shape[0] == len(data)
    assert batch_random['a_input_ids'].shape[0] == len(data)
    assert len(batch_random['question_ids']) == len(data)


def test_multiple_positives_batch_transform(mock_qa_data, mock_tokenizer):
    """Test multiple positives batch transform"""
    with open(mock_qa_data, 'r') as f:
        data = json.load(f)
    
    # Test with up to 2 positives per question
    batch = multiple_positives_batch_transform(data, mock_tokenizer, max_length=10, pos_count=2)
    
    # Validate batch format
    assert 'q_input_ids' in batch
    assert 'a_input_ids' in batch
    assert 'question_ids' in batch
    assert 'ranks' in batch
    assert 'scores' in batch  # Check that scores are included
    
    # Check number of samples (with 2 positives per question)
    # Q1: 2 answers, Q2: 2 answers, Q3: 2 answers = 6 total samples
    expected_samples = 6
    assert batch['q_input_ids'].shape[0] == expected_samples
    assert batch['a_input_ids'].shape[0] == expected_samples
    assert len(batch['question_ids']) == expected_samples
    
    # Verify ranks are correct
    assert torch.sum(batch['ranks'] == 0) > 0  # Some rank 0
    assert torch.sum(batch['ranks'] == 1) > 0  # Some rank 1
    
    # Check that we have scores
    assert batch['scores'].shape[0] == expected_samples


def test_hard_negative_batch_transform(mock_qa_data, mock_tokenizer):
    """Test hard negative batch transform"""
    with open(mock_qa_data, 'r') as f:
        data = json.load(f)
    
    batch = hard_negative_batch_transform(data, mock_tokenizer, max_length=10)
    
    # Validate batch format - this returns a list of items
    assert isinstance(batch, list)
    assert len(batch) == 3  # All questions now have multiple answers
    
    # Check first question
    assert 'q_input_ids' in batch[0]
    assert 'q_attention_mask' in batch[0]
    assert 'answers' in batch[0]
    assert 'question_id' in batch[0]
    
    # Check answers format
    answers = batch[0]['answers']
    assert isinstance(answers, list)
    assert len(answers) == 5  # Q1 has 5 answers
    
    # Verify answer properties
    assert 'input_ids' in answers[0]
    assert 'attention_mask' in answers[0]
    assert 'id' in answers[0]
    assert 'score' in answers[0]
    assert 'rank' in answers[0]
    
    # Check second question
    answers = batch[1]['answers']
    assert len(answers) == 4  # Q2 has 4 answers
    
    # Check third question
    answers = batch[2]['answers']
    assert len(answers) == 3  # Q3 has 3 answers


def test_triplet_batch_transform(mock_qa_data, mock_tokenizer):
    """Test triplet batch transform"""
    with open(mock_qa_data, 'r') as f:
        data = json.load(f)
    
    # Test with hard negatives
    batch = triplet_batch_transform(data, mock_tokenizer, max_length=10, neg_strategy="hard_negative")
    
    # Validate batch format
    assert 'q_input_ids' in batch
    assert 'a_pos_input_ids' in batch
    assert 'a_neg_input_ids' in batch
    assert 'question_ids' in batch
    assert 'pos_scores' in batch
    assert 'neg_scores' in batch
    
    # Check shapes - all 3 questions have multiple answers in our updated mock data
    assert batch['q_input_ids'].shape[0] == 3
    assert batch['a_pos_input_ids'].shape[0] == 3
    assert batch['a_neg_input_ids'].shape[0] == 3
    assert len(batch['question_ids']) == 3
    
    # Test with in-batch negatives
    batch_in_batch = triplet_batch_transform(data, mock_tokenizer, max_length=10, neg_strategy="in_batch")
    
    # Should have samples for all questions
    assert batch_in_batch['q_input_ids'].shape[0] == 3


def test_listwise_batch_transform(mock_qa_data, mock_tokenizer):
    """Test listwise batch transform"""
    with open(mock_qa_data, 'r') as f:
        data = json.load(f)
    
    batch = listwise_batch_transform(data, mock_tokenizer, max_length=10, max_answers=3)
    
    # Validate batch format - this returns a list of items
    assert isinstance(batch, list)
    assert len(batch) == 3  # All questions have multiple answers
    
    # Check first question
    assert 'q_input_ids' in batch[0]
    assert 'q_attention_mask' in batch[0]
    assert 'a_input_ids' in batch[0]
    assert 'a_attention_masks' in batch[0]
    assert 'scores' in batch[0]
    assert 'question_id' in batch[0]
    
    # Check shapes
    assert batch[0]['a_input_ids'].shape[0] == 3  # Limited to max_answers=3 (Q1 has 5 total)
    assert batch[1]['a_input_ids'].shape[0] == 3  # Limited to max_answers=3 (Q2 has 4 total)
    assert batch[2]['a_input_ids'].shape[0] == 3  # Q3 has 3 answers
    
    # Check scores are normalized
    assert torch.max(batch[0]['scores']).item() == 1.0
    assert torch.max(batch[1]['scores']).item() == 1.0
    assert torch.max(batch[2]['scores']).item() == 1.0


def test_flexible_dataset(mock_qa_data, mock_tokenizer):
    """Test flexible dataset class"""
    # Test with default transform (infonce)
    dataset = FlexibleQADataset(
        mock_qa_data,
        batch_size=2,
        tokenizer=mock_tokenizer,
        max_length=10
    )
    
    # Check dataset length
    # With batch_size=2 and 3 questions, should have 2 batches
    assert len(dataset) == 2
    
    # Check first batch
    batch = dataset[0]
    assert 'q_input_ids' in batch
    assert 'a_input_ids' in batch
    assert 'question_ids' in batch
    
    # Test with multiple positives transform
    multi_pos_dataset = FlexibleQADataset(
        mock_qa_data,
        batch_transform_fn=multiple_positives_batch_transform,
        batch_size=2,
        tokenizer=mock_tokenizer,
        max_length=10,
        pos_count=2
    )
    
    # Check dataset
    assert len(multi_pos_dataset) > 0
    
    # Test dataloader creation
    dataloader = FlexibleQADataset.get_dataloader(multi_pos_dataset)
    assert dataloader is not None
    
    # Get a batch from the dataloader
    for batch in dataloader:
        assert 'q_input_ids' in batch
        assert 'a_input_ids' in batch
        assert 'question_ids' in batch
        assert 'ranks' in batch
        break


def test_dataset_with_all_transforms(mock_qa_data, mock_tokenizer):
    """Test dataset with all transform types"""
    # List of all transform functions to test
    transforms = [
        infonce_batch_transform,
        multiple_positives_batch_transform,
        hard_negative_batch_transform,
        triplet_batch_transform,
        listwise_batch_transform
    ]
    
    for transform_fn in transforms:
        # Create dataset with this transform
        dataset = FlexibleQADataset(
            mock_qa_data,
            batch_transform_fn=transform_fn,
            batch_size=2,
            tokenizer=mock_tokenizer,
            max_length=10
        )
        
        # Basic validation
        assert len(dataset) > 0
        
        # Get first batch
        batch = dataset[0]
        assert batch is not None

---
./tests/unit/test_flexible_losses.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Unit tests for flexible loss functions.
Tests each loss function with appropriate input.
"""

import pytest
import torch
import numpy as np

from rank_test.flexible_losses import (
    StandardInfoNCELoss,
    RankInfoNCELoss,
    HardNegativeInfoNCELoss,
    MultiplePositivesLoss,
    TripletLoss,
    ListwiseRankingLoss,
    create_flexible_loss
)

@pytest.fixture
def mock_embeddings():
    """Create mock embeddings for testing"""
    # Create random embeddings and normalize them
    torch.manual_seed(42)  # For reproducibility
    q_embeddings = torch.randn(8, 128)
    a_embeddings = torch.randn(8, 128)
    
    # Normalize
    q_embeddings = q_embeddings / torch.norm(q_embeddings, dim=1, keepdim=True)
    a_embeddings = a_embeddings / torch.norm(a_embeddings, dim=1, keepdim=True)
    
    return q_embeddings, a_embeddings

@pytest.fixture
def mock_question_ids():
    """Create mock question IDs for testing"""
    # 8 embeddings with duplication pattern
    return ["q1", "q2", "q3", "q1", "q4", "q2", "q5", "q3"]

@pytest.fixture
def mock_ranks():
    """Create mock ranks for testing"""
    # 8 ranks corresponding to the embeddings
    return torch.tensor([0, 0, 0, 1, 0, 1, 0, 1], dtype=torch.long)

@pytest.fixture
def mock_triplet_data():
    """Create mock data for triplet loss"""
    torch.manual_seed(42)
    q_embeddings = torch.randn(5, 128)
    a_pos_embeddings = torch.randn(5, 128)
    a_neg_embeddings = torch.randn(5, 128)
    
    # Normalize
    q_embeddings = q_embeddings / torch.norm(q_embeddings, dim=1, keepdim=True)
    a_pos_embeddings = a_pos_embeddings / torch.norm(a_pos_embeddings, dim=1, keepdim=True)
    a_neg_embeddings = a_neg_embeddings / torch.norm(a_neg_embeddings, dim=1, keepdim=True)
    
    # Make positives more similar to queries than negatives for some examples
    for i in range(3):
        # Positive should be more similar
        a_pos_embeddings[i] = 0.8 * q_embeddings[i] + 0.2 * a_pos_embeddings[i]
        a_pos_embeddings[i] = a_pos_embeddings[i] / torch.norm(a_pos_embeddings[i])
        
        # Negative should be less similar
        a_neg_embeddings[i] = 0.2 * q_embeddings[i] + 0.8 * a_neg_embeddings[i]
        a_neg_embeddings[i] = a_neg_embeddings[i] / torch.norm(a_neg_embeddings[i])
    
    return q_embeddings, a_pos_embeddings, a_neg_embeddings

@pytest.fixture
def mock_listwise_data():
    """Create mock data for listwise ranking loss"""
    torch.manual_seed(42)
    
    # Create 3 questions, each with multiple answers
    q_embeddings = []
    a_list_embeddings = []
    a_list_scores = []
    
    for i in range(3):
        # Number of answers for this question
        num_answers = i + 2  # 2, 3, 4 answers
        
        # Create question embedding
        q_embed = torch.randn(1, 128)
        q_embed = q_embed / torch.norm(q_embed)
        
        # Create answer embeddings
        a_embeds = torch.randn(num_answers, 128)
        a_embeds = a_embeds / torch.norm(a_embeds, dim=1, keepdim=True)
        
        # Create scores
        scores = torch.tensor([float(num_answers - j) for j in range(num_answers)])
        
        q_embeddings.append(q_embed)
        a_list_embeddings.append(a_embeds)
        a_list_scores.append(scores)
    
    # Make sure all lists are same length
    # First item will have 3 elements in each list
    return q_embeddings[:1], a_list_embeddings[:1], a_list_scores[:1]


def test_standard_infonce_loss(mock_embeddings):
    """Test standard InfoNCE loss"""
    q_embeddings, a_embeddings = mock_embeddings
    
    # Create loss function
    loss_fn = StandardInfoNCELoss(temperature=0.1)
    
    # Calculate loss
    loss, metrics = loss_fn(q_embeddings, a_embeddings)
    
    # Basic validation
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0  # Scalar tensor
    assert loss.item() > 0  # Loss should be positive
    
    # Check metrics
    assert 'loss' in metrics
    assert 'q2a_acc' in metrics
    assert 'a2q_acc' in metrics
    assert 'avg_acc' in metrics
    
    # Check that metrics are reasonable
    assert 0 <= metrics['q2a_acc'] <= 1
    assert 0 <= metrics['a2q_acc'] <= 1
    assert metrics['avg_acc'] == (metrics['q2a_acc'] + metrics['a2q_acc']) / 2


def test_rank_infonce_loss(mock_embeddings, mock_question_ids, mock_ranks):
    """Test rank-based InfoNCE loss"""
    q_embeddings, a_embeddings = mock_embeddings
    
    # Create loss function with rank weighting
    loss_fn = RankInfoNCELoss(temperature=0.1, use_ranks=True, rank_weight=0.1)
    
    # Calculate loss with rank information
    loss, metrics = loss_fn(q_embeddings, a_embeddings, 
                            question_ids=mock_question_ids,
                            ranks=mock_ranks)
    
    # Basic validation
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0  # Scalar tensor
    assert loss.item() > 0  # Loss should be positive
    
    # Check metrics
    assert 'loss' in metrics
    assert 'q2a_acc' in metrics
    assert 'a2q_acc' in metrics
    
    # Create loss function without rank weighting
    loss_fn_no_rank = RankInfoNCELoss(temperature=0.1, use_ranks=False)
    
    # Calculate loss without rank information
    loss_no_rank, metrics_no_rank = loss_fn_no_rank(q_embeddings, a_embeddings)
    
    # Basic validation
    assert isinstance(loss_no_rank, torch.Tensor)
    assert loss_no_rank.dim() == 0  # Scalar tensor
    assert loss_no_rank.item() > 0  # Loss should be positive


def test_hard_negative_infonce_loss(mock_embeddings, mock_question_ids):
    """Test hard negative InfoNCE loss"""
    q_embeddings, a_embeddings = mock_embeddings
    
    # Create loss function
    loss_fn = HardNegativeInfoNCELoss(temperature=0.1, hard_negative_weight=1.0)
    
    # Calculate loss with hard negatives
    loss, metrics = loss_fn(q_embeddings, a_embeddings, question_ids=mock_question_ids)
    
    # Basic validation
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0  # Scalar tensor
    assert loss.item() > 0  # Loss should be positive
    
    # Check metrics
    assert 'loss' in metrics
    assert 'base_loss' in metrics
    assert 'hard_neg_loss' in metrics
    assert 'accuracy' in metrics
    assert 'hard_neg_count' in metrics
    
    # Check that we found hard negatives
    assert metrics['hard_neg_count'] > 0
    assert metrics['hard_neg_loss'] > 0
    
    # Calculate loss without hard negatives
    loss_no_hn, metrics_no_hn = loss_fn(q_embeddings, a_embeddings)  # No question_ids
    
    # Should fall back to standard InfoNCE
    assert metrics_no_hn['hard_neg_count'] == 0
    assert metrics_no_hn['hard_neg_loss'] == 0
    assert metrics_no_hn['base_loss'] == metrics_no_hn['loss']


def test_multiple_positives_loss(mock_embeddings, mock_question_ids, mock_ranks):
    """Test multiple positives loss"""
    q_embeddings, a_embeddings = mock_embeddings
    
    # Create loss function
    loss_fn = MultiplePositivesLoss(temperature=0.1, rank_weight=0.1)
    
    # Calculate loss
    loss, metrics = loss_fn(q_embeddings, a_embeddings, 
                          question_ids=mock_question_ids,
                          ranks=mock_ranks)
    
    # Basic validation
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0  # Scalar tensor
    assert loss.item() > 0  # Loss should be positive
    
    # Check metrics
    assert 'loss' in metrics
    assert 'accuracy' in metrics
    assert 'total_samples' in metrics
    
    # Check that metrics are reasonable
    assert 0 <= metrics['accuracy'] <= 1
    assert metrics['total_samples'] == len(mock_question_ids)


def test_triplet_loss(mock_triplet_data):
    """Test triplet loss"""
    q_embeddings, a_pos_embeddings, a_neg_embeddings = mock_triplet_data
    
    # Create loss function
    loss_fn = TripletLoss(margin=0.3)
    
    # Calculate loss
    loss, metrics = loss_fn(q_embeddings, a_pos_embeddings, a_neg_embeddings)
    
    # Basic validation
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0  # Scalar tensor
    assert loss.item() >= 0  # Loss should be non-negative
    
    # Check metrics
    assert 'loss' in metrics
    assert 'acc' in metrics
    assert 'avg_pos_sim' in metrics
    assert 'avg_neg_sim' in metrics
    assert 'margin_violations' in metrics
    
    # Check that metrics are reasonable
    assert 0 <= metrics['acc'] <= 1
    assert metrics['avg_pos_sim'] > metrics['avg_neg_sim']  # Should be true on average
    
    # Test with different margin
    loss_fn_large_margin = TripletLoss(margin=1.0)
    loss_large, metrics_large = loss_fn_large_margin(q_embeddings, a_pos_embeddings, a_neg_embeddings)
    
    # Larger margin should lead to more violations and potentially higher loss
    assert metrics_large['margin_violations'] >= metrics['margin_violations']


def test_listwise_ranking_loss(mock_listwise_data):
    """Test listwise ranking loss"""
    q_embeddings, a_list_embeddings, a_list_scores = mock_listwise_data
    
    # Create loss function
    loss_fn = ListwiseRankingLoss(temperature=1.0)
    
    # Calculate loss
    loss, metrics = loss_fn(q_embeddings, a_list_embeddings, a_list_scores)
    
    # Basic validation
    assert isinstance(loss, torch.Tensor)
    assert loss.dim() == 0  # Scalar tensor
    assert loss.item() > 0  # Loss should be positive
    
    # Check metrics
    assert 'loss' in metrics
    assert 'ndcg' in metrics
    
    # Check that metrics are reasonable
    assert 0 <= metrics['ndcg'] <= 1


def test_create_flexible_loss():
    """Test loss factory function"""
    # Test creating all loss types
    losses = {
        'infonce': StandardInfoNCELoss,
        'rank_infonce': RankInfoNCELoss,
        'hard_negative': HardNegativeInfoNCELoss,
        'multiple_positives': MultiplePositivesLoss,
        'triplet': TripletLoss,
        'listwise': ListwiseRankingLoss
    }
    
    for name, loss_class in losses.items():
        loss = create_flexible_loss(name)
        assert isinstance(loss, loss_class)
    
    # Test with parameters
    loss = create_flexible_loss('infonce', temperature=0.5)
    assert loss.temperature == 0.5
    
    # Test with invalid name
    with pytest.raises(ValueError):
        create_flexible_loss('invalid_loss')

---
./tests/integration/test_flexible_pipeline.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Integration tests for the flexible dataset and loss function pipeline.
Tests that the different components work well together.
"""

import os
import pytest
import torch
import json
import tempfile
from torch.utils.data import DataLoader

from rank_test.flexible_dataset import (
    FlexibleQADataset,
    infonce_batch_transform,
    multiple_positives_batch_transform,
    hard_negative_batch_transform,
    triplet_batch_transform,
    listwise_batch_transform
)

from rank_test.flexible_losses import (
    StandardInfoNCELoss,
    RankInfoNCELoss,
    HardNegativeInfoNCELoss,
    MultiplePositivesLoss,
    TripletLoss,
    ListwiseRankingLoss
)

from rank_test.models import QAEmbeddingModel

# Create a mock dataset for testing
@pytest.fixture
def mock_qa_data():
    """Create a mock dataset for testing"""
    data = [
        {
            "question": {
                "id": "q1",
                "title": "Question 1",
                "body": "What is the best way to test code?"
            },
            "answers": [
                {
                    "id": "a1",
                    "body": "Use pytest for testing",
                    "score": 95  # Cardinal score (e.g., user rating or upvotes)
                },
                {
                    "id": "a2",
                    "body": "Use unittest for testing",
                    "score": 82
                },
                {
                    "id": "a3",
                    "body": "Use pytest with coverage for testing",
                    "score": 78
                },
                {
                    "id": "a4",
                    "body": "Use a combination of pytest and unittest",
                    "score": 65
                },
                {
                    "id": "a5",
                    "body": "Manual testing works too",
                    "score": 35
                }
            ]
        },
        {
            "question": {
                "id": "q2",
                "title": "Question 2",
                "body": "How to implement a dataset in PyTorch?"
            },
            "answers": [
                {
                    "id": "a6",
                    "body": "Inherit from torch.utils.data.Dataset",
                    "score": 92
                },
                {
                    "id": "a7",
                    "body": "Override __getitem__ and __len__",
                    "score": 88
                },
                {
                    "id": "a8",
                    "body": "Create a class with __getitem__, __len__, and collate_fn",
                    "score": 75
                },
                {
                    "id": "a9",
                    "body": "Use DataLoader with your custom Dataset class",
                    "score": 62
                }
            ]
        },
        {
            "question": {
                "id": "q3",
                "title": "Question 3",
                "body": "What is a loss function?"
            },
            "answers": [
                {
                    "id": "a10",
                    "body": "A function that measures model error",
                    "score": 90
                },
                {
                    "id": "a11",
                    "body": "A metric that quantifies the difference between predicted and actual values",
                    "score": 85
                },
                {
                    "id": "a12",
                    "body": "A mathematical function that evaluates how well a model performs",
                    "score": 70
                }
            ]
        }
    ]
    
    # Create a temporary file with the data
    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as f:
        json.dump(data, f)
        temp_path = f.name
    
    yield temp_path
    
    # Cleanup
    os.unlink(temp_path)

@pytest.fixture
def mock_tokenizer():
    """Return a mock tokenizer for testing"""
    # For testing, we can use a simple tokenizer
    class MockTokenizer:
        def __call__(self, text, max_length=10, padding=None, truncation=None, return_tensors=None):
            # Just return a fixed tensor for testing
            return {
                'input_ids': torch.ones(1, max_length, dtype=torch.long),
                'attention_mask': torch.ones(1, max_length, dtype=torch.long)
            }
    
    return MockTokenizer()

@pytest.fixture
def mock_model():
    """Return a mock model for testing"""
    model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
    # Replace forward method with a simple mock
    def mock_forward(input_ids, attention_mask=None):
        batch_size = input_ids.shape[0]
        # Return normalized random embeddings
        embeddings = torch.randn(batch_size, 128)
        return embeddings / torch.norm(embeddings, dim=1, keepdim=True)
    
    model.forward = mock_forward
    return model


def test_infonce_pipeline(mock_qa_data, mock_tokenizer, mock_model):
    """Test InfoNCE pipeline with standard dataset"""
    # Create dataset
    dataset = FlexibleQADataset(
        mock_qa_data,
        batch_transform_fn=infonce_batch_transform,
        batch_size=3,
        tokenizer=mock_tokenizer,
        max_length=10
    )
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = StandardInfoNCELoss(temperature=0.1)
    
    # Test full pipeline
    for batch in dataloader:
        # Get embeddings
        q_embeddings = mock_model(batch['q_input_ids'])
        a_embeddings = mock_model(batch['a_input_ids'])
        
        # Calculate loss
        loss, metrics = loss_fn(q_embeddings, a_embeddings)
        
        # Basic validation
        assert isinstance(loss, torch.Tensor)
        assert loss.dim() == 0  # Scalar tensor
        assert loss.item() > 0  # Loss should be positive
        
        # Check metrics
        assert 'loss' in metrics
        assert 'q2a_acc' in metrics
        assert 'a2q_acc' in metrics
        
        # Only need to test one batch
        break


def test_rank_infonce_pipeline(mock_qa_data, mock_tokenizer, mock_model):
    """Test rank-aware InfoNCE pipeline"""
    # Create dataset with multiple positives
    dataset = FlexibleQADataset(
        mock_qa_data,
        batch_transform_fn=multiple_positives_batch_transform,
        batch_size=3,
        tokenizer=mock_tokenizer,
        max_length=10,
        pos_count=2
    )
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = RankInfoNCELoss(temperature=0.1, use_ranks=True, rank_weight=0.1)
    
    # Test full pipeline
    for batch in dataloader:
        # Get embeddings
        q_embeddings = mock_model(batch['q_input_ids'])
        a_embeddings = mock_model(batch['a_input_ids'])
        
        # Calculate loss
        loss, metrics = loss_fn(
            q_embeddings, 
            a_embeddings,
            question_ids=batch['question_ids'],
            ranks=batch['ranks']
        )
        
        # Basic validation
        assert isinstance(loss, torch.Tensor)
        assert loss.dim() == 0  # Scalar tensor
        assert loss.item() > 0  # Loss should be positive
        
        # Only need to test one batch
        break


def test_hard_negative_pipeline(mock_qa_data, mock_tokenizer, mock_model):
    """Test hard negative pipeline"""
    # Create dataset
    dataset = FlexibleQADataset(
        mock_qa_data,
        batch_transform_fn=hard_negative_batch_transform,
        batch_size=3,
        tokenizer=mock_tokenizer,
        max_length=10
    )
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = HardNegativeInfoNCELoss(temperature=0.1, hard_negative_weight=1.0)
    
    # Test full pipeline
    for batch_items in dataloader:
        for item in batch_items:
            # Process each question with its answers
            q_embedding = mock_model(item['q_input_ids'].unsqueeze(0))
            
            # Get embeddings for all answers
            a_embeddings = []
            a_ids = []
            for answer in item['answers']:
                a_embedding = mock_model(answer['input_ids'].unsqueeze(0))
                a_embeddings.append(a_embedding)
                a_ids.append(answer['id'])
            
            a_embeddings = torch.cat(a_embeddings, dim=0)
            
            # Create question_ids list (all same ID for this question's answers)
            question_ids = [item['question_id']] * len(a_ids)
            
            # Calculate loss
            loss, metrics = loss_fn(
                q_embedding.repeat(len(a_ids), 1),  # Repeat question embedding
                a_embeddings,
                question_ids=question_ids
            )
            
            # Basic validation
            assert isinstance(loss, torch.Tensor)
            assert loss.dim() == 0  # Scalar tensor
            assert loss.item() > 0  # Loss should be positive
            
            # Check metrics
            assert 'loss' in metrics
            assert 'hard_neg_count' in metrics
            
            # Only need to test one item
            break
        break


def test_triplet_pipeline(mock_qa_data, mock_tokenizer, mock_model):
    """Test triplet loss pipeline"""
    # Create dataset
    dataset = FlexibleQADataset(
        mock_qa_data,
        batch_transform_fn=triplet_batch_transform,
        batch_size=3,
        tokenizer=mock_tokenizer,
        max_length=10,
        neg_strategy="hard_negative"
    )
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = TripletLoss(margin=0.3)
    
    # Test full pipeline
    for batch in dataloader:
        # Get embeddings
        q_embeddings = mock_model(batch['q_input_ids'])
        a_pos_embeddings = mock_model(batch['a_pos_input_ids'])
        a_neg_embeddings = mock_model(batch['a_neg_input_ids'])
        
        # Calculate loss
        loss, metrics = loss_fn(q_embeddings, a_pos_embeddings, a_neg_embeddings)
        
        # Basic validation
        assert isinstance(loss, torch.Tensor)
        assert loss.dim() == 0  # Scalar tensor
        assert loss.item() >= 0  # Loss should be non-negative
        
        # Check metrics
        assert 'loss' in metrics
        assert 'acc' in metrics
        assert 'avg_pos_sim' in metrics
        assert 'avg_neg_sim' in metrics
        
        # Only need to test one batch
        break


def test_listwise_pipeline(mock_qa_data, mock_tokenizer, mock_model):
    """Test listwise ranking pipeline"""
    # Create dataset
    dataset = FlexibleQADataset(
        mock_qa_data,
        batch_transform_fn=listwise_batch_transform,
        batch_size=3,
        tokenizer=mock_tokenizer,
        max_length=10,
        max_answers=3
    )
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = ListwiseRankingLoss(temperature=1.0)
    
    # Test full pipeline
    for batch_items in dataloader:
        # Prepare inputs for listwise loss
        q_embeddings = []
        a_list_embeddings = []
        a_list_scores = []
        
        for item in batch_items:
            # Get question embedding
            q_embedding = mock_model(item['q_input_ids'].unsqueeze(0))
            
            # Get embeddings for all answers
            a_embeddings = mock_model(item['a_input_ids'])
            
            q_embeddings.append(q_embedding)
            a_list_embeddings.append(a_embeddings)
            a_list_scores.append(item['scores'])
        
        # Calculate loss
        loss, metrics = loss_fn(q_embeddings, a_list_embeddings, a_list_scores)
        
        # Basic validation
        assert isinstance(loss, torch.Tensor)
        assert loss.dim() == 0  # Scalar tensor
        assert loss.item() > 0  # Loss should be positive
        
        # Check metrics
        assert 'loss' in metrics
        assert 'ndcg' in metrics
        assert 0 <= metrics['ndcg'] <= 1
        
        # Only need to test one batch
        break


def test_pipeline_combinations(mock_qa_data, mock_tokenizer, mock_model):
    """Test different combinations of dataset transformations and loss functions"""
    # Define combinations to test
    combinations = [
        (infonce_batch_transform, StandardInfoNCELoss(temperature=0.1)),
        (multiple_positives_batch_transform, RankInfoNCELoss(temperature=0.1, use_ranks=True)),
        (hard_negative_batch_transform, HardNegativeInfoNCELoss(temperature=0.1)),
        (triplet_batch_transform, TripletLoss(margin=0.3)),
        (listwise_batch_transform, ListwiseRankingLoss(temperature=1.0))
    ]
    
    for transform_fn, loss_fn in combinations:
        # Create dataset
        dataset = FlexibleQADataset(
            mock_qa_data,
            batch_transform_fn=transform_fn,
            batch_size=3,
            tokenizer=mock_tokenizer,
            max_length=10
        )
        
        # Verify dataset was created
        assert len(dataset) > 0

---
./examples/flexible_dataset_demo.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Demo script for the flexible dataset and loss functions.
Shows how to create different dataset configurations and train with them.
"""

import os
import json
import torch
import argparse
from torch.utils.data import DataLoader
from transformers import DistilBertTokenizer

from rank_test.flexible_dataset import (
    FlexibleQADataset,
    infonce_batch_transform, 
    multiple_positives_batch_transform,
    hard_negative_batch_transform,
    triplet_batch_transform,
    listwise_batch_transform
)

from rank_test.flexible_losses import (
    StandardInfoNCELoss,
    RankInfoNCELoss,
    HardNegativeInfoNCELoss,
    MultiplePositivesLoss,
    TripletLoss,
    ListwiseRankingLoss,
    create_flexible_loss
)

from rank_test.models import QAEmbeddingModel


def print_separator(title):
    """Print a separator with title"""
    print("\n" + "=" * 50)
    print(title)
    print("=" * 50)


def create_sample_data(output_path="data/sample_qa.json"):
    """Create a sample dataset for demonstration"""
    data = [
        {
            "question": {
                "id": "q1",
                "title": "Question 1",
                "body": "What is the best way to test code?"
            },
            "answers": [
                {
                    "id": "a1",
                    "body": "Use pytest for testing",
                    "score": 95 
                },
                {
                    "id": "a2",
                    "body": "Use unittest for testing",
                    "score": 82
                },
                {
                    "id": "a3",
                    "body": "Use pytest with coverage for testing",
                    "score": 78
                },
                {
                    "id": "a4",
                    "body": "Use a combination of pytest and unittest",
                    "score": 65
                },
                {
                    "id": "a5",
                    "body": "Manual testing works too",
                    "score": 35
                }
            ]
        },
        {
            "question": {
                "id": "q2",
                "title": "Question 2",
                "body": "How to implement a dataset in PyTorch?"
            },
            "answers": [
                {
                    "id": "a6",
                    "body": "Inherit from torch.utils.data.Dataset",
                    "score": 92
                },
                {
                    "id": "a7",
                    "body": "Override __getitem__ and __len__",
                    "score": 88
                },
                {
                    "id": "a8",
                    "body": "Create a class with __getitem__, __len__, and collate_fn",
                    "score": 75
                },
                {
                    "id": "a9",
                    "body": "Use DataLoader with your custom Dataset class",
                    "score": 62
                }
            ]
        },
        {
            "question": {
                "id": "q3",
                "title": "Question 3",
                "body": "What is a loss function?"
            },
            "answers": [
                {
                    "id": "a10",
                    "body": "A function that measures model error",
                    "score": 90
                },
                {
                    "id": "a11",
                    "body": "A metric that quantifies the difference between predicted and actual values",
                    "score": 85
                },
                {
                    "id": "a12",
                    "body": "A mathematical function that evaluates how well a model performs",
                    "score": 70
                }
            ]
        }
    ]
    
    # Create directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Write data to file
    with open(output_path, 'w') as f:
        json.dump(data, f, indent=2)
    
    print(f"Created sample dataset at {output_path}")
    return output_path


def demo_infonce_dataset(data_path, tokenizer):
    """Demonstrate InfoNCE dataset and loss"""
    print_separator("InfoNCE Dataset and Loss")
    
    # Create dataset
    dataset = FlexibleQADataset(
        data_path,
        batch_transform_fn=infonce_batch_transform,
        batch_size=3,
        tokenizer=tokenizer,
        max_length=128,
        take_top=True  # Use top-ranked answer for each question
    )
    
    print(f"Created InfoNCE dataset with {len(dataset)} batches")
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = StandardInfoNCELoss(temperature=0.1)
    print(f"Using loss function: {loss_fn.get_name()}")
    
    # Sample model (for demonstration)
    model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
    # Process a batch
    for batch in dataloader:
        print("\nProcessing batch:")
        print(f"  Questions: {len(batch['question_ids'])}")
        print(f"  Input shapes: {batch['q_input_ids'].shape}, {batch['a_input_ids'].shape}")
        
        # Get embeddings
        with torch.no_grad():
            q_embeddings = model(batch['q_input_ids'])
            a_embeddings = model(batch['a_input_ids'])
        
        # Calculate loss
        loss, metrics = loss_fn(q_embeddings, a_embeddings)
        
        print(f"Loss: {loss.item():.4f}")
        print(f"Metrics: {metrics}")
        break


def demo_hard_negative_dataset(data_path, tokenizer):
    """Demonstrate hard negative dataset and loss"""
    print_separator("Hard Negative Dataset and Loss")
    
    # Create dataset
    dataset = FlexibleQADataset(
        data_path,
        batch_transform_fn=hard_negative_batch_transform,
        batch_size=3,
        tokenizer=tokenizer,
        max_length=128
    )
    
    print(f"Created hard negative dataset with {len(dataset)} batches")
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = HardNegativeInfoNCELoss(temperature=0.1, hard_negative_weight=1.0)
    print(f"Using loss function: {loss_fn.get_name()}")
    
    # Sample model (for demonstration)
    model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
    # Process a batch
    for batch_items in dataloader:
        print("\nProcessing batch with questions:")
        for item in batch_items:
            print(f"  Question ID: {item['question_id']} with {item['answer_count']} answers")
            
            # Process this question with its answers
            q_embedding = model(item['q_input_ids'].unsqueeze(0))
            
            # Get embeddings for all answers
            a_embeddings = []
            a_ids = []
            for answer in item['answers']:
                a_embedding = model(answer['input_ids'].unsqueeze(0))
                a_embeddings.append(a_embedding)
                a_ids.append(answer['id'])
                print(f"    Answer ID: {answer['id']}, Score: {answer['score']:.1f}, Rank: {answer['rank']}")
            
            a_embeddings = torch.cat(a_embeddings, dim=0)
            
            # Create a batch with this question and all its answers
            question_ids = [item['question_id']] * len(a_ids)
            
            # Calculate loss
            loss, metrics = loss_fn(
                q_embedding.repeat(len(a_ids), 1),  # Repeat question embedding
                a_embeddings,
                question_ids=question_ids
            )
            
            print(f"  Loss: {loss.item():.4f}")
            print(f"  Metrics: {metrics}")
        break


def demo_multiple_positives_dataset(data_path, tokenizer):
    """Demonstrate multiple positives dataset and loss"""
    print_separator("Multiple Positives Dataset and Loss")
    
    # Create dataset
    dataset = FlexibleQADataset(
        data_path,
        batch_transform_fn=multiple_positives_batch_transform,
        batch_size=3,
        tokenizer=tokenizer,
        max_length=128,
        pos_count=3  # Include up to 3 positive answers per question
    )
    
    print(f"Created multiple positives dataset with {len(dataset)} batches")
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Sample model (for demonstration)
    model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
    # Process a batch
    for batch in dataloader:
        print("\nProcessing batch:")
        
        # Group by question ID
        question_groups = {}
        for i, q_id in enumerate(batch['question_ids']):
            if q_id not in question_groups:
                question_groups[q_id] = []
            question_groups[q_id].append((i, batch['ranks'][i].item(), batch['scores'][i].item()))
        
        # Print batch structure
        for q_id, indices in question_groups.items():
            print(f"  Question {q_id} with {len(indices)} answers:")
            for idx, rank, score in indices:
                print(f"    Answer at position {idx}: Rank {rank}, Score {score:.1f}")
        
        # Get embeddings
        with torch.no_grad():
            q_embeddings = model(batch['q_input_ids'])
            a_embeddings = model(batch['a_input_ids'])
        
        # Test with different loss configurations
        
        # 1. Rank-based loss
        rank_loss_fn = RankInfoNCELoss(
            temperature=0.1, 
            use_ranks=True, 
            use_scores=False, 
            rank_weight=0.1
        )
        print(f"\nUsing rank-based loss: {rank_loss_fn.get_name()}")
        
        rank_loss, rank_metrics = rank_loss_fn(
            q_embeddings, 
            a_embeddings,
            question_ids=batch['question_ids'],
            ranks=batch['ranks']
        )
        
        print(f"Loss: {rank_loss.item():.4f}")
        print(f"Metrics: {rank_metrics}")
        
        # 2. Score-based loss
        score_loss_fn = RankInfoNCELoss(
            temperature=0.1, 
            use_ranks=False, 
            use_scores=True, 
            score_weight=0.05
        )
        print(f"\nUsing score-based loss: {score_loss_fn.get_name()}")
        
        score_loss, score_metrics = score_loss_fn(
            q_embeddings, 
            a_embeddings,
            question_ids=batch['question_ids'],
            scores=batch['scores']
        )
        
        print(f"Loss: {score_loss.item():.4f}")
        print(f"Metrics: {score_metrics}")
        
        # 3. Combined rank and score loss
        combined_loss_fn = RankInfoNCELoss(
            temperature=0.1, 
            use_ranks=True, 
            use_scores=True, 
            rank_weight=0.1,
            score_weight=0.05
        )
        print(f"\nUsing combined loss: {combined_loss_fn.get_name()}")
        
        combined_loss, combined_metrics = combined_loss_fn(
            q_embeddings, 
            a_embeddings,
            question_ids=batch['question_ids'],
            ranks=batch['ranks'],
            scores=batch['scores']
        )
        
        print(f"Loss: {combined_loss.item():.4f}")
        print(f"Metrics: {combined_metrics}")
        
        # 4. Standard InfoNCE (no rank or score information)
        std_loss_fn = StandardInfoNCELoss(temperature=0.1)
        print(f"\nUsing standard InfoNCE: {std_loss_fn.get_name()}")
        
        std_loss, std_metrics = std_loss_fn(q_embeddings, a_embeddings)
        
        print(f"Loss: {std_loss.item():.4f}")
        print(f"Metrics: {std_metrics}")
        
        # Alternative: Use factory function to create loss from name
        print("\nCreating loss via factory function:")
        factory_loss = create_flexible_loss(
            "infonce_rank0.2_score0.1_t0.1", 
            temperature=0.05  # Override temperature
        )
        print(f"Created: {factory_loss.get_name()}")
        
        break


def demo_triplet_dataset(data_path, tokenizer):
    """Demonstrate triplet dataset and loss"""
    print_separator("Triplet Dataset and Loss")
    
    # Create dataset
    dataset = FlexibleQADataset(
        data_path,
        batch_transform_fn=triplet_batch_transform,
        batch_size=3,
        tokenizer=tokenizer,
        max_length=128,
        neg_strategy="hard_negative"  # Use lower-ranked answers as negatives
    )
    
    print(f"Created triplet dataset with {len(dataset)} batches")
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = TripletLoss(margin=0.3)
    print(f"Using loss function: {loss_fn.get_name()}")
    
    # Sample model (for demonstration)
    model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
    # Process a batch
    for batch in dataloader:
        print("\nProcessing batch:")
        print(f"  Questions: {len(batch['question_ids'])}")
        print(f"  Input shapes:")
        print(f"    Questions: {batch['q_input_ids'].shape}")
        print(f"    Positive answers: {batch['a_pos_input_ids'].shape}")
        print(f"    Negative answers: {batch['a_neg_input_ids'].shape}")
        
        # Get embeddings
        with torch.no_grad():
            q_embeddings = model(batch['q_input_ids'])
            a_pos_embeddings = model(batch['a_pos_input_ids'])
            a_neg_embeddings = model(batch['a_neg_input_ids'])
        
        # Print score information
        for i, q_id in enumerate(batch['question_ids']):
            print(f"  Question {q_id}:")
            print(f"    Positive score: {batch['pos_scores'][i].item():.1f}")
            print(f"    Negative score: {batch['neg_scores'][i].item():.1f}")
        
        # Calculate loss
        loss, metrics = loss_fn(q_embeddings, a_pos_embeddings, a_neg_embeddings)
        
        print(f"Loss: {loss.item():.4f}")
        print(f"Metrics: {metrics}")
        break


def demo_listwise_dataset(data_path, tokenizer):
    """Demonstrate listwise dataset and loss"""
    print_separator("Listwise Dataset and Loss")
    
    # Create dataset
    dataset = FlexibleQADataset(
        data_path,
        batch_transform_fn=listwise_batch_transform,
        batch_size=3,
        tokenizer=tokenizer,
        max_length=128,
        max_answers=4  # Include up to 4 answers per question
    )
    
    print(f"Created listwise dataset with {len(dataset)} batches")
    
    # Create dataloader
    dataloader = FlexibleQADataset.get_dataloader(dataset)
    
    # Create loss function
    loss_fn = ListwiseRankingLoss(temperature=1.0)
    print(f"Using loss function: {loss_fn.get_name()}")
    
    # Sample model (for demonstration)
    model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
    # Process a batch
    for batch_items in dataloader:
        print("\nProcessing batch with questions:")
        
        # Prepare inputs for listwise loss
        q_embeddings = []
        a_list_embeddings = []
        a_list_scores = []
        
        for item in batch_items:
            print(f"  Question ID: {item['question_id']} with {item['answer_count']} answers")
            print(f"  Normalized scores: {item['scores'].tolist()}")
            
            # Get question embedding
            q_embedding = model(item['q_input_ids'].unsqueeze(0))
            
            # Get embeddings for all answers
            a_embeddings = model(item['a_input_ids'])
            
            q_embeddings.append(q_embedding)
            a_list_embeddings.append(a_embeddings)
            a_list_scores.append(item['scores'])
        
        # Calculate loss
        loss, metrics = loss_fn(q_embeddings, a_list_embeddings, a_list_scores)
        
        print(f"Loss: {loss.item():.4f}")
        print(f"NDCG: {metrics['ndcg']:.4f}")
        break


def main():
    parser = argparse.ArgumentParser(description="Demo for flexible datasets and losses")
    parser.add_argument("--data-path", type=str, default=None, 
                       help="Path to QA dataset (will create sample if not provided)")
    args = parser.parse_args()
    
    # Create or use dataset
    data_path = args.data_path
    if data_path is None:
        data_path = create_sample_data()
    
    # Create tokenizer
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    
    # Run demos
    demo_infonce_dataset(data_path, tokenizer)
    demo_hard_negative_dataset(data_path, tokenizer)
    demo_multiple_positives_dataset(data_path, tokenizer)
    demo_triplet_dataset(data_path, tokenizer)
    demo_listwise_dataset(data_path, tokenizer)
    
    print_separator("Demo Complete")
    print("This demo showcased different dataset transformations and loss functions.")
    print("For actual training, you would combine these components with a training loop.")


if __name__ == "__main__":
    main()

---
./src/rank_test/__init__.py
---
"""
Ranking test for comparing different loss functions in retrieval tasks
"""

from rank_test.models import QAEmbeddingModel
from rank_test.losses import StandardInfoNCELoss, TripletLoss, ListwiseRankingLoss, create_loss
from rank_test.dataset import QADataset
from rank_test.evaluate import evaluate_model, compare_models
from rank_test.train import train_model
from rank_test.config import ExperimentConfig, PREDEFINED_CONFIGS as default_configs

def main() -> None:
    print("Hello from rank-test!")

---
./src/rank_test/config.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Configuration module using Pydantic for strongly typed configuration.
Provides a simple, flat structure for experiment configuration.
"""

from typing import Dict, Any, Optional, Union, Literal
from pathlib import Path
import json
from pydantic import BaseModel, Field

# Type definitions using literals instead of enums
DatasetStrategyType = Literal["standard", "flexible"]
BatchTransformType = Literal["infonce", "multiple_positives", "hard_negative", "triplet", "listwise"]
NegativeStrategyType = Literal["hard_negative", "in_batch", "mixed"]
LossType = Literal["infonce", "rank_infonce", "hard_negative", "multiple_positives", "triplet", "listwise", "mse"]


class ExperimentConfig(BaseModel):
    """Complete experiment configuration with flat structure"""
    # Experiment metadata
    name: Optional[str] = Field(default=None, description="Name of the experiment")
    
    # Data settings
    data_path: str = Field(default="data/ranked_qa.json", description="Path to the dataset JSON file")
    limit: Optional[int] = Field(default=None, description="Limit the number of samples to process")
    test_size: float = Field(default=0.2, description="Proportion of data to use for testing")
    seed: int = Field(default=42, description="Random seed for reproducibility")
    force_regenerate: bool = Field(default=False, description="Force dataset regeneration")
    
    # Model settings
    embed_dim: int = Field(default=768, description="Embedding dimension from BERT")
    projection_dim: int = Field(default=128, description="Dimension for embeddings projection")
    
    # Training settings
    batch_size: int = Field(default=64, description="Training batch size")
    epochs: int = Field(default=5, description="Number of training epochs")
    learning_rate: float = Field(default=2e-5, description="Learning rate")
    checkpoint_interval: int = Field(default=1, description="Save checkpoint every N epochs")
    eval_steps: Optional[int] = Field(default=None, description="Evaluate every N steps")
    eval_at_zero: bool = Field(default=False, description="Evaluate before training")
    debug: bool = Field(default=False, description="Debug mode with minimal data")
    
    # Output settings
    output_dir: str = Field(default="models", description="Directory to save models and results")
    log_to_wandb: bool = Field(default=True, description="Whether to log metrics to W&B")
    wandb_project: str = Field(default="qa-embeddings-comparison", description="W&B project name")
    use_fixed_evaluation: bool = Field(default=True, description="Use the fixed evaluation implementation")
    
    # Dataset strategy settings
    dataset_strategy: DatasetStrategyType = Field(default="standard", description="Dataset strategy to use")
    batch_transform: BatchTransformType = Field(default="infonce", description="Batch transformation strategy")
    
    # InfoNCE transform options
    take_top: bool = Field(default=True, description="For InfoNCE: use top-ranked answer (True) or random (False)")
    
    # Multiple positives options
    pos_count: int = Field(default=3, description="For multiple_positives: number of positives per question")
    
    # Triplet options
    neg_strategy: NegativeStrategyType = Field(default="hard_negative", description="For triplet: negative sampling strategy")
    
    # Listwise options
    max_answers: int = Field(default=5, description="For listwise: maximum answers per question")
    
    # Loss settings
    loss_type: LossType = Field(default="infonce", description="Loss function to use")
    temperature: float = Field(default=0.1, description="Temperature for InfoNCE and listwise losses")
    margin: float = Field(default=0.2, description="Margin for triplet loss")
    normalize: bool = Field(default=True, description="For MSE: normalize scores")
    
    # Loss weighting options
    use_ranks: bool = Field(default=True, description="Use ordinal ranks in loss calculation")
    use_scores: bool = Field(default=False, description="Use cardinal scores in loss calculation")
    rank_weight: float = Field(default=0.1, description="Weight for rank-based penalties")
    score_weight: float = Field(default=0.05, description="Weight for score-based adjustments")
    hard_negative_weight: float = Field(default=1.0, description="Weight for hard negative penalty")
    
    def auto_select_loss_type(cls, values):
        """Automatically select loss type based on batch transform if not explicitly set"""
        if 'loss_type' not in values or not values['loss_type']:
            batch_transform = values.get('batch_transform')
            if batch_transform:
                if batch_transform == "infonce":
                    values['loss_type'] = "infonce"
                elif batch_transform == "multiple_positives":
                    values['loss_type'] = "rank_infonce"
                elif batch_transform == "hard_negative":
                    values['loss_type'] = "hard_negative"
                elif batch_transform == "triplet":
                    values['loss_type'] = "triplet"
                elif batch_transform == "listwise":
                    values['loss_type'] = "listwise"
        return values
    
    def validate_debug_settings(cls, values):
        """Reduce settings in debug mode"""
        if values.get('debug', False):
            # Reduce batch size and epochs in debug mode
            values['batch_size'] = min(values.get('batch_size', 16), 4)
            values['epochs'] = min(values.get('epochs', 5), 2)
            
            # Limit data
            if values.get('limit') is None or values.get('limit', 0) > 50:
                values['limit'] = 50
                
        return values
    
    def get_loss_kwargs(self) -> Dict[str, Any]:
        """Get keyword arguments for the loss function"""
        kwargs = {}
        
        loss_type = self.loss_type
        
        # Add loss-specific parameters
        if loss_type in ["infonce", "rank_infonce", "hard_negative", "listwise"]:
            kwargs['temperature'] = self.temperature
            
        if loss_type == "rank_infonce":
            kwargs['use_ranks'] = self.use_ranks
            kwargs['use_scores'] = self.use_scores
            kwargs['rank_weight'] = self.rank_weight
            kwargs['score_weight'] = self.score_weight
            
        elif loss_type == "hard_negative":
            kwargs['hard_negative_weight'] = self.hard_negative_weight
            
        elif loss_type == "triplet":
            kwargs['margin'] = self.margin
            
        elif loss_type == "mse":
            kwargs['normalize'] = self.normalize
            
        return kwargs
    
    def get_batch_transform_kwargs(self) -> Dict[str, Any]:
        """Get keyword arguments for the batch transformation function"""
        kwargs = {}
        
        transform = self.batch_transform
        
        if transform == "infonce":
            kwargs['take_top'] = self.take_top
        elif transform == "multiple_positives":
            kwargs['pos_count'] = self.pos_count
        elif transform == "triplet":
            kwargs['neg_strategy'] = self.neg_strategy
        elif transform == "listwise":
            kwargs['max_answers'] = self.max_answers
            
        return kwargs
    
    def get_limit(self) -> Optional[int]:
        """Get effective data limit, accounting for debug mode"""
        if self.debug:
            return min(self.limit or 50, 50)
        return self.limit
    
    def get_batch_size(self) -> int:
        """Get effective batch size, accounting for debug mode"""
        if self.debug:
            return min(self.batch_size, 4)
        return self.batch_size
    
    def get_epochs(self) -> int:
        """Get effective number of epochs, accounting for debug mode"""
        if self.debug:
            return min(self.epochs, 2)
        return self.epochs
    
    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> 'ExperimentConfig':
        """Load configuration from a JSON file"""
        with open(file_path, 'r') as f:
            config_data = json.load(f)
        return cls.parse_obj(config_data)
    
    def save_to_file(self, file_path: Union[str, Path]) -> None:
        """Save configuration to a JSON file"""
        with open(file_path, 'w') as f:
            json.dump(self.dict(), f, indent=2)


# Predefined experiment configurations
PREDEFINED_CONFIGS = {
    # Standard InfoNCE with in-batch negatives
    'infonce_standard': ExperimentConfig(
        name="InfoNCE Standard",
        dataset_strategy="flexible",
        batch_transform="infonce",
        take_top=True,
        loss_type="infonce",
        temperature=0.1
    ),
    
    # InfoNCE with random answer selection
    'infonce_random': ExperimentConfig(
        name="InfoNCE Random",
        dataset_strategy="flexible",
        batch_transform="infonce",
        take_top=False,  # Choose random answer instead of top
        loss_type="infonce",
        temperature=0.1
    ),
    
    # Multiple positives with rank weighting
    'multiple_positives_rank': ExperimentConfig(
        name="Multiple Positives with Ranks",
        dataset_strategy="flexible",
        batch_transform="multiple_positives",
        pos_count=3,
        use_ranks=True,
        use_scores=False,
        rank_weight=0.1,
        loss_type="rank_infonce",
        temperature=0.1
    ),
    
    # Multiple positives with score weighting
    'multiple_positives_score': ExperimentConfig(
        name="Multiple Positives with Scores",
        dataset_strategy="flexible",
        batch_transform="multiple_positives",
        pos_count=3,
        use_ranks=False,
        use_scores=True,
        score_weight=0.05,
        loss_type="rank_infonce",
        temperature=0.1
    ),
    
    # Hard negative approach
    'hard_negative': ExperimentConfig(
        name="Hard Negative InfoNCE",
        dataset_strategy="flexible",
        batch_transform="hard_negative",
        hard_negative_weight=1.0,
        loss_type="hard_negative",
        temperature=0.1
    ),
    
    # Triplet loss with hard negatives
    'triplet_hard_neg': ExperimentConfig(
        name="Triplet with Hard Negatives",
        dataset_strategy="flexible",
        batch_transform="triplet",
        neg_strategy="hard_negative",
        loss_type="triplet",
        margin=0.2
    ),
    
    # Listwise ranking
    'listwise': ExperimentConfig(
        name="Listwise Ranking",
        dataset_strategy="flexible",
        batch_transform="listwise",
        max_answers=5,
        loss_type="listwise",
        temperature=1.0
    )
}

---
./src/rank_test/dataset.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Flexible dataset module that supports various sampling strategies
for QA ranking tasks. This module provides configurable approaches
for creating training data from ranked QA pairs.
"""

import json
import os
import random
import torch
import time
import csv
import sys
from collections import defaultdict
from typing import List, Dict, Callable, Tuple
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm, trange
from transformers import DistilBertTokenizer

from rank_test.transforms import clean_html, get_batch_transform

class QADataset(Dataset):
    """
    Flexible QA dataset that supports various batch transformation strategies.
    This dataset provides configurable approaches for creating training data
    from ranked QA pairs for different loss functions.
    """
    
    def __init__(
        self, 
        data_path: str, 
        batch_transform_fn: Callable = None, 
        batch_size: int = 16, 
        tokenizer = None,
        max_length: int = 128,
        shuffle: bool = True,
        **kwargs
    ):
        """
        Initialize the dataset with a specific batch transformation strategy.
        
        Args:
            data_path: Path to JSON dataset with ranked QA pairs
            batch_transform_fn: Function that transforms raw data to model-ready batches
            batch_size: Batch size for pre-processing
            tokenizer: Tokenizer to use (will create DistilBERT tokenizer if None)
            max_length: Maximum sequence length for tokenization
            shuffle: Whether to shuffle data during batch creation
            **kwargs: Additional parameters for the batch transform function
        """
        # Load raw data
        with open(data_path, 'r') as f:
            self.raw_data = json.load(f)
            
        # Store tokenizer and other parameters
        self.tokenizer = tokenizer or DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        self.max_length = max_length
        
        # Use provided transform or default
        self.batch_transform_fn = batch_transform_fn or infonce_batch_transform
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.kwargs = kwargs
        
        # Pre-process into batches
        self.batches = self._create_batches()
        
    def _create_batches(self):
        """Transform raw data into batches based on the strategy"""
        # Create mini-batches of raw data
        num_items = len(self.raw_data)
        indices = list(range(num_items))
        
        if self.shuffle:
            random.shuffle(indices)
        
        batches = []
        total_docs = 0
        
        for i in trange(0, num_items, self.batch_size, desc="Creating batches"):
            batch_indices = indices[i:i+self.batch_size]
            batch_data = [self.raw_data[idx] for idx in batch_indices]
            
            # Apply the transform to create model-ready batch
            result = self.batch_transform_fn(
                batch_data, 
                self.tokenizer, 
                self.max_length, 
                **self.kwargs
            )
            
            # Handle return value (either just batch or batch with doc count)
            if isinstance(result, tuple) and len(result) == 2:
                processed_batch, batch_docs = result
                total_docs += batch_docs
            else:
                processed_batch = result
                # Estimate docs if not provided (backward compatibility)
                total_docs += len(batch_data) * 2  # Rough estimate: 1 question + 1 answer per item
            
            if processed_batch:  # Skip empty batches
                # Store batch with cumulative doc count
                batches.append((processed_batch, total_docs))
                
        return batches
    
    def __len__(self):
        """Return number of batches"""
        return len(self.batches)
        
    def __getitem__(self, idx):
        """Return a pre-processed batch with document count"""
        return self.batches[idx]
    
    @staticmethod
    def get_dataloader(dataset, shuffle=False):
        """
        Create a DataLoader for this dataset
        
        Since the dataset already returns batches, use batch_size=1
        """
        return DataLoader(
            dataset,
            batch_size=1,
            shuffle=shuffle,
            collate_fn=lambda x: x[0]  # Extract single batch from list
        )

def parse_from_json(data_path: str, limit: int = None) -> list:
    """
    Load QA pairs from a JSON file
    
    Args:
        data_path: Path to the JSON file
        limit: Maximum number of items to load
        
    Returns:
        List of QA pairs
    """
    with open(data_path, 'r') as f:
        data = json.load(f)
        
    if limit is not None:
        data = data[:limit]
        
    return data


def export_to_json(data: list, output_path: str = "data/ranked_qa.json") -> None:
    """
    Export QA pairs to a JSON file
    
    Args:
        data: List of QA pairs
        output_path: Path to save the JSON file
    """
    print(f"Exporting data to {output_path}...")
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"Exported {len(data)} question-answer sets to {output_path}")


def download_dataset():
    """
    Download the StackExchange dataset from Kaggle.
    Uses Kaggle API to download and extract the dataset.
    """
    # Create a directory for the dataset if it doesn't exist
    os.makedirs("data", exist_ok=True)
    
    # Check if files already exist to avoid re-downloading
    if os.path.exists("data/Questions.csv") and os.path.exists("data/Answers.csv"):
        q_size = os.path.getsize("data/Questions.csv")
        a_size = os.path.getsize("data/Answers.csv")
        # If files are reasonable size, assume they're valid
        if q_size > 10000 and a_size > 10000:
            print(f"Found existing dataset files:")
            print(f"  Questions.csv: {q_size/1_000_000:.1f} MB")
            print(f"  Answers.csv: {a_size/1_000_000:.1f} MB")
            return "data"
    
    print("Downloading dataset from Kaggle...")
    
    try:
        # Import the Kaggle API
        from kaggle.api.kaggle_api_extended import KaggleApi
        api = KaggleApi()
        api.authenticate()
        
        # Download the dataset
        dataset_name = 'stackoverflow/statsquestions'
        print(f"Downloading dataset {dataset_name}...")
        api.dataset_download_files(
            dataset_name, 
            path='data',
            unzip=True
        )
        print("Dataset downloaded and extracted successfully.")
        
        # Verify files were downloaded and are not empty
        if os.path.exists("data/Questions.csv") and os.path.exists("data/Answers.csv"):
            q_size = os.path.getsize("data/Questions.csv")
            a_size = os.path.getsize("data/Answers.csv")
            print(f"Dataset files:")
            print(f"  Questions.csv: {q_size/1_000_000:.1f} MB")
            print(f"  Answers.csv: {a_size/1_000_000:.1f} MB")
            return "data"
        else:
            raise FileNotFoundError("Dataset files not found after download")
            
    except (ImportError, ModuleNotFoundError):
        print("Error: Kaggle API not found. Installing kaggle package...")
        os.system("uv add kaggle")
        print("Kaggle package installed. Please run the script again.")
        sys.exit(1)
        
    except Exception as e:
        print(f"Error downloading dataset: {e}")
        
        # Fall back to using sample data for testing
        print("Falling back to sample data for testing")
        return "data"

def parse_posts(data_dir, limit=None):
    """
    Parse questions and answers from CSV files.
    
    Args:
        data_dir: Path to the directory containing Questions.csv and Answers.csv
        limit: Optional limit on the number of questions to process
        
    Returns:
        questions, answers: Dictionaries containing questions and their answers
    """
    questions_file = os.path.join(data_dir, "Questions.csv")
    answers_file = os.path.join(data_dir, "Answers.csv")
    
    # If files don't exist, return sample data
    if not os.path.exists(questions_file) or not os.path.exists(answers_file):
        print("CSV files not found. Using sample data instead.")
        return _create_sample_data()
    
    print(f"Parsing data from {data_dir}...")
    
    questions = {}  # Dictionary to store questions by ID
    answers = defaultdict(list)  # Dictionary to store answers by parent ID
    
    try:
        # First pass: collect all answers
        answer_count = 0
        start_time = time.time()
        last_update_time = start_time
        
        print("First pass: collecting answers...")
        
        with open(answers_file, 'r', encoding='latin-1') as f:
            # Skip header
            header = next(f)
            
            # Count lines for progress bar (subtract 1 for header)
            total_lines = sum(1 for _ in f) 
            f.seek(0)
            next(f)  # Skip header again
            
            # Create CSV reader
            reader = csv.reader(f)
            
            # Create progress bar
            for row in tqdm(reader, total=total_lines, desc="Parsing answers"):
                # Skip empty rows
                if not row:
                    continue
                
                answer_id, owner_id, creation_date, parent_id, score, body = row[:6]
                
                # Store answer data
                answers[parent_id].append({
                    "id": answer_id,
                    "body": body,
                    "score": int(score),
                    "is_accepted": False  # Will be set later
                })
                answer_count += 1
                
                # Print progress every 1000 answers or every 5 seconds
                current_time = time.time()
                if answer_count % 1000 == 0 or (current_time - last_update_time >= 5 and answer_count % 100 == 0):
                    elapsed = current_time - start_time
                    rate = answer_count / elapsed if elapsed > 0 else 0
                    print(f"  Processed {answer_count} answers... ({rate:.1f} answers/sec)")
                    last_update_time = current_time
        
        elapsed = time.time() - start_time
        print(f"Collected {answer_count} answers for {len(answers)} questions in {elapsed:.1f} seconds.")
        
        # Second pass: collect questions that have answers
        print("Second pass: collecting questions with answers...")
        start_time = time.time()
        last_update_time = start_time
        question_count = 0
        
        with open(questions_file, 'r', encoding='latin-1') as f:
            # Skip header
            header = next(f)
            
            # Count lines for progress bar (subtract 1 for header)
            total_lines = sum(1 for _ in f) 
            f.seek(0)
            next(f)  # Skip header again
            
            # Create CSV reader
            reader = csv.reader(f)
            
            # Create progress bar
            for row in tqdm(reader, total=total_lines, desc="Parsing questions"):
                # Skip empty rows
                if not row:
                    continue
                
                # CSV may not have all columns for every row
                row_data = row[:6] if len(row) >= 6 else row + [''] * (6 - len(row))
                question_id, owner_id, creation_date, score, title, body = row_data
                
                # Only process questions that have answers
                if question_id in answers:
                    questions[question_id] = {
                        "id": question_id,
                        "title": title,
                        "body": body,
                        "score": int(score),
                        "view_count": 0,  # Not available in this dataset
                        "tags": "",       # Not available in this dataset
                        "accepted_answer_id": None  # Will try to determine later
                    }
                    
                    question_count += 1
                    
                    # Print progress every 100 questions or every 5 seconds
                    current_time = time.time()
                    if question_count % 100 == 0 or (current_time - last_update_time >= 5 and question_count % 10 == 0):
                        elapsed = current_time - start_time
                        rate = question_count / elapsed if elapsed > 0 else 0
                        print(f"  Processed {question_count} questions with answers... ({rate:.1f} questions/sec)")
                        last_update_time = current_time
                    
                    # Apply limit if specified
                    if limit and question_count >= limit:
                        print(f"Reached limit of {limit} questions.")
                        break
        
        elapsed = time.time() - start_time
        print(f"Collected {question_count} questions with answers in {elapsed:.1f} seconds.")
        
        # Since we don't have accepted answers in this dataset, 
        # we'll consider the highest scored answer as the accepted one
        print("Ranking answers by score...")
        start_time = time.time()
        
        # Rank answers by score for each question
        for q_id in answers:
            # Sort answers by score (highest first)
            answers[q_id].sort(key=lambda x: x["score"], reverse=True)
            
            # If the question exists in our collection and has answers
            if q_id in questions and answers[q_id]:
                # Set the top-scored answer as the accepted answer
                top_answer = answers[q_id][0]
                top_answer["is_accepted"] = True
                questions[q_id]["accepted_answer_id"] = top_answer["id"]
        
        elapsed = time.time() - start_time
        print(f"Ranked all answers and marked highest-scored answers as accepted in {elapsed:.1f} seconds.")
        
        # If no questions were collected or limit is 0, use sample data
        if not questions or (limit is not None and limit <= 0):
            print("No questions collected. Using sample data instead.")
            return _create_sample_data()
        
        return questions, answers
    
    except Exception as e:
        print(f"Error parsing posts: {e}")
        import traceback
        traceback.print_exc()
        
        # Return sample data in case of error
        print("Falling back to sample data due to error.")
        return _create_sample_data()

def _create_sample_data():
    """Create sample QA data for testing"""
    questions = {}
    answers = {}
    
    # Add a few sample Q&A pairs for testing
    sample_q = {
        "id": "q1",
        "title": "Sample Question",
        "body": "This is a sample question for testing"
    }
    
    sample_a1 = {
        "id": "a1",
        "body": "This is a sample answer",
        "score": 5
    }
    
    sample_a2 = {
        "id": "a2",
        "body": "This is another sample answer",
        "score": 3
    }
    
    questions["q1"] = sample_q
    answers["q1"] = [sample_a1, sample_a2]
    
    return questions, answers

def ensure_dataset_exists(data_path: str = 'data/ranked_qa.json', 
                           data_limit: int = None, 
                           force_regenerate: bool = False) -> None:
    """
    Ensure the dataset exists, generating it if necessary.
    
    Args:
        data_path: Path where the JSON dataset should be stored
        data_limit: Limit the number of questions to process
        force_regenerate: Force regeneration even if file exists
    """
    # Check current limit if file exists
    current_limit = None
    regenerate_needed = force_regenerate
    
    if os.path.exists(data_path) and not force_regenerate:
        # Try to determine the current limit from the dataset
        try:
            with open(data_path, 'r') as f:
                data = json.load(f)
                current_limit = len(data)
                print(f"Found existing dataset at {data_path} with {current_limit} items")
                
                # If data_limit is specified and different from current, regenerate
                if data_limit is not None and data_limit != current_limit:
                    print(f"Requested limit ({data_limit}) differs from current dataset size ({current_limit})")
                    regenerate_needed = True
                else:
                    return  # Dataset exists with correct limit
        except Exception as e:
            print(f"Error reading existing dataset: {e}")
            regenerate_needed = True  # Regenerate if there's an issue with the file
    else:
        regenerate_needed = True
    
    if regenerate_needed:
        if os.path.exists(data_path):
            action = "Regenerating" if force_regenerate else "Updating"
            print(f"{action} dataset at {data_path} with limit={data_limit}")
        else:
            print(f"Dataset not found at {data_path}. Generating it with limit={data_limit}")
        
        # Get path for data directory
        data_dir = download_dataset()
            
        # Parse posts from CSV files
        print(f"Using CSV files in {data_dir}")
        questions, answers = parse_posts(data_dir, limit=data_limit)
        
        # Convert to the format expected by our JSON dataset
        data = []
        for q_id, question in questions.items():
            if q_id in answers:
                item = {
                    "question": question,
                    "answers": answers[q_id]
                }
                data.append(item)
        
        # Export to JSON
        export_to_json(data, data_path)

---
./src/rank_test/evaluate.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Fixed implementation of the evaluation code to correctly calculate hard negative accuracy metrics.
"""

import json
import os
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
import wandb
from collections import defaultdict

def ndcg_at_k(relevances, k):
    """
    Calculate Normalized Discounted Cumulative Gain (NDCG) at k
    
    Args:
        relevances: List of relevance scores in rank order
        k: Cutoff for relevance calculation
        
    Returns:
        NDCG@k score
    """
    relevances = np.array(relevances)
    if len(relevances) < k:
        # Pad with zeros if not enough relevances
        relevances = np.pad(relevances, (0, k - len(relevances)))
    relevances = relevances[:k]
    
    # Calculate DCG
    dcg = np.sum(relevances / np.log2(np.arange(2, k + 2)))
    
    # Calculate ideal DCG
    ideal_relevances = np.sort(relevances)[::-1]
    idcg = np.sum(ideal_relevances / np.log2(np.arange(2, k + 2)))
    
    # Return NDCG
    return dcg / idcg if idcg > 0 else 0.0

def map_at_k(relevances, k):
    """
    Calculate Mean Average Precision (MAP) at k
    
    Args:
        relevances: List of relevance scores in rank order
        k: Cutoff for MAP calculation
        
    Returns:
        MAP@k score
    """
    relevances = np.array(relevances)
    if len(relevances) < k:
        # Pad with zeros if not enough relevances
        relevances = np.pad(relevances, (0, k - len(relevances)))
    relevances = relevances[:k]
    
    # Calculate precision at each position
    precisions = np.cumsum(relevances) / np.arange(1, k + 1)
    
    # Calculate average precision
    ap = np.sum(precisions * relevances) / np.sum(relevances) if np.sum(relevances) > 0 else 0.0
    
    return ap

def mrr(relevances):
    """
    Calculate Reciprocal Rank (RR) for a single query
    
    Args:
        relevances: List of relevance scores in rank order
                   (sorted by similarity/relevance in descending order)
        
    Returns:
        Reciprocal Rank score (1/rank of first relevant item)
    """
    relevances = np.array(relevances)
    # Find index of first relevant result
    indices = np.where(relevances > 0)[0]
    if len(indices) > 0:
        rank = indices[0] + 1  # +1 because indices are 0-based
        return 1.0 / rank
    else:
        return 0.0

def evaluate_standardized_test(model, test_dataloader, device, k_values=[1, 5, 10], debug_output=False):
    """
    Evaluate model with standardized test format that includes positive, hard negative,
    and normal negative answers for each question.
    
    Args:
        model: QAEmbeddingModel
        test_dataloader: DataLoader with standardized test data
        device: Device to run evaluation on
        k_values: List of k values for @k metrics
        debug_output: Whether to print detailed debug information
        
    Returns:
        Dictionary of evaluation metrics
    """
    model.eval()
    
    # Initialize metrics
    metrics = {
        # Overall metrics (all answers)
        'mrr': 0.0,
        'accuracy@1': 0.0,
        
        # In-batch negative metrics (ignoring hard negatives)
        'mrr_in_batch': 0.0,
        'accuracy@1_in_batch': 0.0,
        
        # Hard negative metrics (only answers for same question)
        'mrr_hard_neg': 0.0,
        'accuracy@1_hard_neg': 0.0,
    }
    
    for k in k_values:
        metrics[f'ndcg@{k}'] = 0.0
        metrics[f'map@{k}'] = 0.0
    
    # Track per-query metrics for calculating std. dev.
    per_query_metrics = {
        'mrr': [],
        'mrr_in_batch': [],
        'mrr_hard_neg': [],
    }
    
    for k in k_values:
        per_query_metrics[f'ndcg@{k}'] = []
        per_query_metrics[f'map@{k}'] = []
    
    total_questions = 0
    hard_neg_questions = 0
    
    # Process each batch
    with torch.no_grad():
        for batch_data in tqdm(test_dataloader, desc="Calculating embeddings"):
            if isinstance(batch_data, tuple):
                batch, _ = batch_data
            else:
                batch = batch_data
                
            for item in batch:
                # Skip items without answers
                if 'answers' not in item or not item['answers']:
                    continue
                
                total_questions += 1
                
                # Calculate question embedding
                q_embedding = model(
                    item['q_input_ids'].unsqueeze(0).to(device),
                    item['q_attention_mask'].unsqueeze(0).to(device)
                )
                
                # Process all answers for this question
                answers = item['answers']
                all_similarities = []
                has_hard_negative = False
                
                # Get embeddings and metadata for each answer
                for answer in answers:
                    # Calculate answer embedding
                    a_embedding = model(
                        answer['input_ids'].unsqueeze(0).to(device),
                        answer['attention_mask'].unsqueeze(0).to(device)
                    )
                    
                    # Calculate similarity
                    similarity = torch.matmul(q_embedding, a_embedding.t()).item()
                    
                    # Store similarity with metadata
                    all_similarities.append({
                        'similarity': similarity,
                        'is_positive': answer['is_positive'],
                        'is_hard_negative': answer.get('is_hard_negative', False),
                        'score': answer['score'],
                        'rank': answer['rank']
                    })
                    
                    # Check if this is a hard negative
                    if answer.get('is_hard_negative', False):
                        has_hard_negative = True
                
                if has_hard_negative:
                    hard_neg_questions += 1
                
                # Sort by similarity (descending)
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                
                # Create binary relevance array (1 for positive, 0 for negative)
                relevance = [1 if a['is_positive'] else 0 for a in all_similarities]
                
                # 1. Overall evaluation (all answers)
                # Calculate MRR
                query_mrr = mrr(relevance)
                metrics['mrr'] += query_mrr
                per_query_metrics['mrr'].append(query_mrr)
                
                # Calculate accuracy@1
                metrics['accuracy@1'] += 1.0 if relevance[0] == 1 else 0.0
                
                # Calculate NDCG@k and MAP@k
                for k in k_values:
                    query_ndcg = ndcg_at_k(relevance, k)
                    query_map = map_at_k(relevance, k)
                    
                    metrics[f'ndcg@{k}'] += query_ndcg
                    metrics[f'map@{k}'] += query_map
                    
                    per_query_metrics[f'ndcg@{k}'].append(query_ndcg)
                    per_query_metrics[f'map@{k}'].append(query_map)
                
                # 2. In-batch evaluation (exclude hard negatives)
                in_batch_sims = [a for a in all_similarities if not a.get('is_hard_negative', False)]
                in_batch_relevance = [1 if a['is_positive'] else 0 for a in in_batch_sims]
                
                # Calculate MRR for in-batch
                query_mrr_in_batch = mrr(in_batch_relevance)
                metrics['mrr_in_batch'] += query_mrr_in_batch
                per_query_metrics['mrr_in_batch'].append(query_mrr_in_batch)
                
                # Calculate accuracy@1 for in-batch
                metrics['accuracy@1_in_batch'] += 1.0 if in_batch_relevance and in_batch_relevance[0] == 1 else 0.0
                
                # 3. Hard negative evaluation (only for questions with hard negatives)
                if has_hard_negative:
                    # Include only the positive and hard negative answers
                    hard_neg_sims = [a for a in all_similarities if a['is_positive'] or a.get('is_hard_negative', False)]
                    hard_neg_relevance = [1 if a['is_positive'] else 0 for a in hard_neg_sims]
                    
                    # Calculate MRR for hard negatives
                    query_mrr_hard_neg = mrr(hard_neg_relevance)
                    metrics['mrr_hard_neg'] += query_mrr_hard_neg
                    per_query_metrics['mrr_hard_neg'].append(query_mrr_hard_neg)
                    
                    # Calculate accuracy@1 for hard negatives
                    metrics['accuracy@1_hard_neg'] += 1.0 if hard_neg_relevance and hard_neg_relevance[0] == 1 else 0.0
    
    # Average metrics
    if total_questions > 0:
        # Average overall and in-batch metrics
        for key in metrics:
            if key not in ['mrr_hard_neg', 'accuracy@1_hard_neg']:
                metrics[key] /= total_questions
    
    # Average hard negative metrics
    if hard_neg_questions > 0:
        metrics['mrr_hard_neg'] /= hard_neg_questions
        metrics['accuracy@1_hard_neg'] /= hard_neg_questions
    
    # Calculate standard deviations
    for key in per_query_metrics:
        if per_query_metrics[key]:
            metrics[f'{key}_std'] = np.std(per_query_metrics[key])
        else:
            metrics[f'{key}_std'] = 0.0
    
    # Print debug info if requested
    if debug_output:
        print(f"Total questions evaluated: {total_questions}")
        print(f"Questions with hard negatives: {hard_neg_questions}")
        
        print("\nMetrics summary:")
        print(f"Overall MRR: {metrics['mrr']:.4f}")
        print(f"In-batch MRR: {metrics['mrr_in_batch']:.4f}")
        print(f"Hard negative MRR: {metrics['mrr_hard_neg']:.4f}")
        
        print(f"Overall accuracy@1: {metrics['accuracy@1']:.4f}")
        print(f"In-batch accuracy@1: {metrics['accuracy@1_in_batch']:.4f}")
        print(f"Hard negative accuracy@1: {metrics['accuracy@1_hard_neg']:.4f}")
    
    return metrics


def evaluate_model(model, test_dataloader, device, k_values=[1, 5, 10], debug_output=False):
    """
    Smart evaluation function that detects the test dataloader format and 
    uses the appropriate evaluation strategy.
    
    Args:
        model: QAEmbeddingModel
        test_dataloader: DataLoader with test data
        device: Device to run evaluation on
        k_values: List of k values for @k metrics
        debug_output: Whether to print detailed debug information
        
    Returns:
        Dictionary of evaluation metrics
    """
    # Check the first batch to determine the format
    for batch_data in test_dataloader:
        # Extract batch if it comes with document count
        if isinstance(batch_data, tuple) and len(batch_data) == 2:
            batch = batch_data[0]
        else:
            batch = batch_data
            
        # Check if this is a standardized test batch
        if isinstance(batch, list) and 'answers' in batch[0] and isinstance(batch[0]['answers'], list):
            return evaluate_standardized_test(model, test_dataloader, device, k_values, debug_output)
            
        break  # Only need to check first batch
        
    # Default to original implementation for backward compatibility
    model.eval()
    
    # Collect all embeddings, relevance scores, and question IDs
    all_q_embeddings = []
    all_a_embeddings = []
    all_question_ids = []  # To track which answers belong to which questions
    
    # Extract data to track which answers belong to which questions
    question_data = defaultdict(list)  # Map question ID to list of answer indices
    
    # Calculate embeddings for each item
    with torch.no_grad():
        start_idx = 0  # Track the absolute index in the final concatenated tensor
        
        for batch_idx, batch_data in enumerate(tqdm(test_dataloader, desc="Calculating embeddings")):
            # Extract batch if it comes with document count
            if isinstance(batch_data, tuple) and len(batch_data) == 2:
                batch = batch_data[0]
            else:
                batch = batch_data
                
            # Get embeddings
            q_embeddings = model(batch['q_input_ids'].to(device), 
                                batch['q_attention_mask'].to(device))
            a_embeddings = model(batch['a_input_ids'].to(device), 
                                batch['a_attention_mask'].to(device))
            
            # Store embeddings
            all_q_embeddings.append(q_embeddings.cpu())
            all_a_embeddings.append(a_embeddings.cpu())
            
            # Track question IDs from batch
            batch_size = q_embeddings.shape[0]
            
            # Use question_id from batch, or create dummy IDs if not available
            if 'question_id' in batch:
                q_ids = batch['question_id']
            elif 'question_ids' in batch:
                q_ids = batch['question_ids']
            else:
                q_ids = [f"batch{batch_idx}_item{i}" for i in range(batch_size)]
                
            all_question_ids.extend(q_ids)
            
            # Store which answers belong to which questions
            for i in range(batch_size):
                idx = start_idx + i  # Absolute index in the concatenated tensor
                q_id = q_ids[i]
                question_data[q_id].append(idx)
            
            # Update the start index for the next batch
            start_idx += batch_size
    
    # Concatenate all embeddings
    all_q_embeddings = torch.cat(all_q_embeddings, dim=0)
    all_a_embeddings = torch.cat(all_a_embeddings, dim=0)
    
    # Calculate similarity matrix for all question-answer pairs
    similarity = torch.matmul(all_q_embeddings, all_a_embeddings.T)
    
    # Initialize metrics
    metrics = {
        # Overall metrics (all answers)
        'mrr': 0.0,
        'accuracy@1': 0.0,
        
        # In-batch negative metrics (ignoring hard negatives)
        'mrr_in_batch': 0.0,
        'accuracy@1_in_batch': 0.0,
        
        # Hard negative metrics (only answers for same question)
        'mrr_hard_neg': 0.0,
        'accuracy@1_hard_neg': 0.0,
    }
    
    for k in k_values:
        metrics[f'ndcg@{k}'] = 0.0
        metrics[f'map@{k}'] = 0.0
    
    # Dictionary to track metrics for each query
    per_query_metrics = {
        'mrr': [],
        'mrr_in_batch': [],
        'mrr_hard_neg': [],
    }
    
    for k in k_values:
        per_query_metrics[f'ndcg@{k}'] = []
        per_query_metrics[f'map@{k}'] = []
        
    # IMPORTANT: How the three types of accuracy work:
    # 1. Overall accuracy - Ranks across ALL answers in the test set
    #    - For each question, how well its correct answer ranks among all possible answers
    # 2. In-batch negative accuracy - Only considers answers NOT from the same question
    #    - For each question, how well its answer ranks among answers to other questions
    # 3. Hard negative accuracy - Only considers answers to the same question
    #    - For each question, how well its correct answer ranks among all answers for that question
    
    # Calculate metrics for each query
    n_queries = similarity.shape[0]
    for i in range(n_queries):
        # Get similarities for this query
        query_similarities = similarity[i].numpy()
        
        # Create binary relevance (1 for matches, 0 for non-matches)
        relevance = np.zeros_like(query_similarities)
        relevance[i] = 1  # Correct answer is at index i
        
        # Get question ID for this query
        q_id = all_question_ids[i]
        
        # Get indices of all answers to this question (hard negatives)
        hard_negative_indices = question_data[q_id]
        
        # 1. Overall ranking (all answers)
        sorted_indices = np.argsort(-query_similarities)
        sorted_relevances = relevance[sorted_indices]
        
        # Calculate overall MRR and metrics
        query_mrr = mrr(sorted_relevances)
        metrics['mrr'] += query_mrr
        per_query_metrics['mrr'].append(query_mrr)
        
        # Add accuracy@1 - whether the correct answer is ranked first
        metrics['accuracy@1'] += 1.0 if sorted_indices[0] == i else 0.0
        
        # Calculate NDCG@k and MAP@k for overall ranking
        for k in k_values:
            query_ndcg = ndcg_at_k(sorted_relevances, k)
            query_map = map_at_k(sorted_relevances, k)
            
            metrics[f'ndcg@{k}'] += query_ndcg
            metrics[f'map@{k}'] += query_map
            
            per_query_metrics[f'ndcg@{k}'].append(query_ndcg)
            per_query_metrics[f'map@{k}'].append(query_map)
        
        # 2. In-batch negative ranking (excluding hard negatives)
        # Create mask that excludes hard negatives
        in_batch_mask = np.ones_like(query_similarities, dtype=bool)
        for idx in hard_negative_indices:
            if idx != i:  # Keep the correct answer
                in_batch_mask[idx] = False
        
        # Only consider in-batch negatives
        in_batch_similarities = query_similarities[in_batch_mask]
        in_batch_relevance = relevance[in_batch_mask]
        
        # Get correct answer index in this reduced set
        correct_idx = np.where(in_batch_mask)[0].tolist().index(i)
        
        # Sort by similarity (descending)
        in_batch_sorted_indices = np.argsort(-in_batch_similarities)
        in_batch_sorted_relevances = in_batch_relevance[in_batch_sorted_indices]
        
        # Calculate MRR for in-batch negatives
        query_mrr_in_batch = mrr(in_batch_sorted_relevances)
        metrics['mrr_in_batch'] += query_mrr_in_batch
        per_query_metrics['mrr_in_batch'].append(query_mrr_in_batch)
        
        # Add accuracy@1 for in-batch negatives
        metrics['accuracy@1_in_batch'] += 1.0 if in_batch_sorted_indices[0] == correct_idx else 0.0
    
    # 3. Hard negative ranking (only answers to same question)
    # We need a more robust way to track questions with multiple answers
    # Create dict mapping question_id to indices of its answers
    question_to_indices = defaultdict(list)
    for i, q_id in enumerate(all_question_ids):
        # Convert tensor to scalar if needed
        if isinstance(q_id, torch.Tensor):
            q_id = q_id.item()
        # Convert string tensor to string if needed
        elif hasattr(q_id, 'decode'):
            q_id = q_id.decode()
            
        question_to_indices[q_id].append(i)
    
    # Track metrics for each query
    hard_neg_mrr_values = []
    hard_neg_accuracy_values = []
    
    # Process only questions that have multiple answers (for hard negative comparison)
    questions_with_multiple_answers = []
    for q_id, indices in question_to_indices.items():
        if len(indices) <= 1:
            continue
            
        questions_with_multiple_answers.append(q_id)
        
        # For each query in this question
        for query_idx in indices:
            # Get similarities between this query and all answers
            query_similarities = similarity[query_idx].cpu().numpy()
            
            # Create a mask that selects only answers to this question
            mask = np.zeros(len(query_similarities), dtype=bool)
            for idx in indices:
                mask[idx] = True
                
            # Get similarities only for answers to this question
            hard_neg_similarities = query_similarities[mask]
            
            # Create binary relevance scores (1 for the correct answer, 0 for others)
            hard_neg_relevance = np.zeros(len(indices))
            # Find position of query_idx in indices
            query_position = indices.index(query_idx)
            hard_neg_relevance[query_position] = 1
            
            # Sort by similarity (descending)
            hard_neg_sorted_indices = np.argsort(-hard_neg_similarities)
            hard_neg_sorted_relevances = hard_neg_relevance[hard_neg_sorted_indices]
            
            # Calculate MRR
            query_mrr = mrr(hard_neg_sorted_relevances)
            hard_neg_mrr_values.append(query_mrr)
            
            # Calculate accuracy@1 (whether the top result is the correct answer)
            accuracy = 1.0 if hard_neg_sorted_indices[0] == query_position else 0.0
            hard_neg_accuracy_values.append(accuracy)
    
    # Calculate total MRR for hard negatives
    if hard_neg_mrr_values:
        metrics['mrr_hard_neg'] = np.mean(hard_neg_mrr_values)
        per_query_metrics['mrr_hard_neg'] = hard_neg_mrr_values
        metrics['mrr_hard_neg_std'] = np.std(hard_neg_mrr_values)
    else:
        metrics['mrr_hard_neg'] = 0.0
        metrics['mrr_hard_neg_std'] = 0.0
    
    # Calculate accuracy for hard negatives
    if hard_neg_accuracy_values:
        metrics['accuracy@1_hard_neg'] = np.mean(hard_neg_accuracy_values)
    else:
        metrics['accuracy@1_hard_neg'] = 0.0
        
    if debug_output:
        print(f"Questions with multiple answers: {len(questions_with_multiple_answers)}")
        print(f"Total evaluations for hard negatives: {len(hard_neg_mrr_values)}")
        if hard_neg_mrr_values:
            mrr_values = np.array(hard_neg_mrr_values)
            print(f"Hard negative MRR: mean={np.mean(mrr_values):.4f}, "
                  f"min={np.min(mrr_values):.4f}, max={np.max(mrr_values):.4f}")
            print(f"Hard negative accuracy@1: {metrics['accuracy@1_hard_neg']:.4f}")
    
    # Average metrics across all queries for overall and in-batch metrics
    for key in metrics:
        if key not in ['mrr_hard_neg', 'mrr_hard_neg_std', 'accuracy@1_hard_neg']:
            metrics[key] /= n_queries
    
    # Add std dev for regular metrics
    for key in per_query_metrics:
        if key != 'mrr_hard_neg' and per_query_metrics[key]:
            metrics[f'{key}_std'] = np.std(per_query_metrics[key])
    
    # Print debug info if requested
    if debug_output:
        print(f"Total questions: {len(question_to_indices)}")
        print(f"Questions with multiple answers: {len(questions_with_multiple_answers)}")
        print(f"Total evaluations for hard negatives: {len(hard_neg_mrr_values)}")
        
        if hard_neg_mrr_values:
            print("\nHard negative MRR distribution:")
            print(f"  Min: {min(hard_neg_mrr_values):.4f}")
            print(f"  Max: {max(hard_neg_mrr_values):.4f}")
            print(f"  Mean: {np.mean(hard_neg_mrr_values):.4f}")
            print(f"  Median: {np.median(hard_neg_mrr_values):.4f}")
            
            # Count of perfect MRR (1.0) and zero MRR
            perfect_mrr = sum(1 for mrr in hard_neg_mrr_values if mrr == 1.0)
            zero_mrr = sum(1 for mrr in hard_neg_mrr_values if mrr == 0.0)
            print(f"  Perfect MRR (1.0): {perfect_mrr} ({perfect_mrr/len(hard_neg_mrr_values)*100:.1f}%)")
            print(f"  Zero MRR (0.0): {zero_mrr} ({zero_mrr/len(hard_neg_mrr_values)*100:.1f}%)")
    
    return metrics

def compare_models(results, output_dir='results'):
    """
    Compare metrics across multiple models
    
    Args:
        results: Dictionary mapping model names to their evaluation metrics
        output_dir: Directory to save comparison results
        
    Returns:
        DataFrame with model comparison
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert results to DataFrame
    models = []
    for model_name, metrics in results.items():
        row = {'model': model_name}
        row.update(metrics)
        models.append(row)
    
    df = pd.DataFrame(models)
    
    # Save results to CSV
    df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)
    
    # Create comparison plots
    plt.figure(figsize=(12, 8))
    
    # Compare MRR
    plt.subplot(2, 2, 1)
    plt.bar(df['model'], df['mrr'])
    plt.title('Mean Reciprocal Rank (MRR)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Compare NDCG@10
    plt.subplot(2, 2, 2)
    plt.bar(df['model'], df['ndcg@10'])
    plt.title('NDCG@10')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Compare MAP@10
    plt.subplot(2, 2, 3)
    plt.bar(df['model'], df['map@10'])
    plt.title('MAP@10')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Save the plot
    plt.savefig(os.path.join(output_dir, 'model_comparison.png'))
    
    print(f"Model comparison saved to {output_dir}")
    return df

---
./src/rank_test/ideas.md
---
- pairwise losse
- ranking loss with only hard negatives
- ranking loss with hard negatives + in batch negatives
- info nce loss with hard negatives (documents lower than the positive from the same question)
- info nce loss w/o hard negatives, only taking the top document
- info nce loss taking random document
- info nce loss with multiple positives, all documents
- info nce loss with hard negatives, weighted by the score/rank
- info nce loss but multiple rows per question (one per document), with no hard negatives (this might be bad because it treats other positive documents from the same query as in batch negatives, but we want to experiment)

---
./src/rank_test/losses.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Flexible loss functions that work with various data sampling strategies.
These losses are designed to work with the output of the flexible dataset module.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Tuple, List, Any, Optional
from collections import defaultdict


class BaseLoss(nn.Module):
    """Base class for all loss functions"""
    
    def __init__(self, **kwargs):
        super(BaseLoss, self).__init__()
        self.name = "base_loss"
        
    def forward(self, q_embeddings, a_embeddings, **kwargs):
        """
        Calculate loss
        
        Args:
            q_embeddings: Tensor of question embeddings (batch_size, embed_dim)
            a_embeddings: Tensor of answer embeddings (batch_size, embed_dim)
            **kwargs: Additional loss-specific parameters
            
        Returns:
            loss: Loss value
            metrics: Dictionary of metrics for logging
        """
        raise NotImplementedError("Subclasses must implement forward method")
    
    def get_name(self):
        """Get loss function name for logging"""
        return self.name


class StandardInfoNCELoss(BaseLoss):
    """Standard InfoNCE loss with in-batch negatives"""
    
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.name = f"std_infonce_t{temperature}"
        
    def forward(self, q_embeddings, a_embeddings, **kwargs):
        """
        Calculate standard InfoNCE loss with in-batch negatives
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            
        Returns:
            loss: InfoNCE loss
            metrics: Dictionary with accuracy metrics
        """
        batch_size = q_embeddings.shape[0]
        
        # Calculate similarity matrix
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        
        # Labels are on the diagonal (each question matched with its answer)
        labels = torch.arange(batch_size, device=q_embeddings.device)
        
        # Compute loss (bidirectional)
        loss_q2a = F.cross_entropy(similarity, labels)
        loss_a2q = F.cross_entropy(similarity.T, labels)
        loss = (loss_q2a + loss_a2q) / 2
        
        # Calculate accuracy metrics
        q2a_preds = torch.argmax(similarity, dim=1)
        a2q_preds = torch.argmax(similarity.T, dim=1)
        q2a_acc = (q2a_preds == labels).float().mean()
        a2q_acc = (a2q_preds == labels).float().mean()
        
        metrics = {
            'loss': loss.item(),
            'q2a_acc': q2a_acc.item(),
            'a2q_acc': a2q_acc.item(),
            'avg_acc': (q2a_acc.item() + a2q_acc.item()) / 2
        }
        
        return loss, metrics


class RankInfoNCELoss(BaseLoss):
    """
    InfoNCE loss that leverages rank or score information from the dataset.
    Works with standard batch format or multiple positives format.
    
    Can use either:
    - Ordinal ranks (position in the ranked list, 0 is best)
    - Cardinal scores (actual score values, higher is better)
    """
    
    def __init__(self, temperature=0.1, use_ranks=True, use_scores=False, 
                 rank_weight=0.1, score_weight=0.01):
        """
        Initialize the loss function.
        
        Args:
            temperature: Temperature parameter for scaling similarity scores
            use_ranks: Whether to use ordinal rank information in loss calculation
            use_scores: Whether to use cardinal score information in loss calculation
            rank_weight: Weight for rank-based penalties (higher ranks get lower weight)
            score_weight: Weight for score-based adjustments (higher scores get higher weight)
        """
        super().__init__()
        self.temperature = temperature
        self.use_ranks = use_ranks
        self.use_scores = use_scores
        self.rank_weight = rank_weight
        self.score_weight = score_weight
        
        # Create descriptive name
        ranking_method = []
        if use_ranks:
            ranking_method.append(f"rank{rank_weight}")
        if use_scores:
            ranking_method.append(f"score{score_weight}")
        
        ranking_str = "_".join(ranking_method) if ranking_method else "standard"
        self.name = f"infonce_{ranking_str}_t{temperature}"
        
    def forward(self, q_embeddings, a_embeddings, question_ids=None, ranks=None, scores=None, **kwargs):
        """
        Calculate InfoNCE loss with rank/score-based weighting
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            question_ids: Question IDs to identify answers to the same question
            ranks: Ordinal rank of each answer (0 is highest ranked)
            scores: Cardinal score of each answer (higher is better)
            
        Returns:
            loss: Weighted InfoNCE loss
            metrics: Dictionary with accuracy and other metrics
        """
        batch_size = q_embeddings.shape[0]
        device = q_embeddings.device
        
        # Calculate similarity matrix
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        
        # Standard InfoNCE loss labels
        labels = torch.arange(batch_size, device=device)
        
        # Track whether we've applied any adjustments
        adjusted = False
        sim_weights = torch.ones_like(similarity)
        
        # Group answers by question if needed for adjustments
        if (self.use_ranks or self.use_scores) and question_ids is not None:
            question_groups = defaultdict(list)
            for i, q_id in enumerate(question_ids):
                item = [i]  # Always include index
                
                if self.use_ranks and ranks is not None:
                    item.append(ranks[i].item())  # Add rank
                else:
                    item.append(None)  # Placeholder
                    
                if self.use_scores and scores is not None:
                    item.append(scores[i].item())  # Add score
                else:
                    item.append(None)  # Placeholder
                    
                question_groups[q_id].append(item)
                
            # Apply adjustments for questions with multiple answers
            for q_id, items in question_groups.items():
                if len(items) <= 1:
                    continue  # Skip questions with only one answer
                
                # Apply rank-based adjustments
                if self.use_ranks and ranks is not None:
                    adjusted = True
                    for i, item1 in enumerate(items):
                        idx1, rank1, _ = item1
                        for j, item2 in enumerate(items):
                            if i == j:
                                continue  # Skip self
                                
                            idx2, rank2, _ = item2
                            # Apply penalty based on rank
                            # Higher ranks (worse answers) get lower weight
                            if rank2 is not None:
                                rank_penalty = 1.0 - self.rank_weight * rank2
                                sim_weights[idx1, idx2] *= rank_penalty
                
                # Apply score-based adjustments
                if self.use_scores and scores is not None:
                    adjusted = True
                    
                    # Find max score for this question (for normalization)
                    max_score = max(item[2] for item in items if item[2] is not None)
                    if max_score <= 0:
                        continue  # Skip if no positive scores
                        
                    for i, item1 in enumerate(items):
                        idx1, _, score1 = item1
                        for j, item2 in enumerate(items):
                            if i == j:
                                continue  # Skip self
                                
                            idx2, _, score2 = item2
                            # Apply weight based on normalized score
                            # Higher scores get higher weight
                            if score2 is not None and max_score > 0:
                                normalized_score = score2 / max_score
                                score_weight = 1.0 + self.score_weight * normalized_score
                                sim_weights[idx1, idx2] *= score_weight
        
        # Compute loss
        if adjusted:
            # Apply weights to similarity
            adjusted_similarity = similarity * sim_weights
            
            # Compute loss with adjusted similarity
            loss_q2a = F.cross_entropy(adjusted_similarity, labels)
            loss_a2q = F.cross_entropy(adjusted_similarity.T, labels)
            loss = (loss_q2a + loss_a2q) / 2
        else:
            # Standard InfoNCE without adjustments
            loss_q2a = F.cross_entropy(similarity, labels)
            loss_a2q = F.cross_entropy(similarity.T, labels)
            loss = (loss_q2a + loss_a2q) / 2
        
        # Calculate accuracy metrics
        q2a_preds = torch.argmax(similarity, dim=1)
        a2q_preds = torch.argmax(similarity.T, dim=1)
        q2a_acc = (q2a_preds == labels).float().mean()
        a2q_acc = (a2q_preds == labels).float().mean()
        
        metrics = {
            'loss': loss.item(),
            'q2a_acc': q2a_acc.item(),
            'a2q_acc': a2q_acc.item(),
            'avg_acc': (q2a_acc.item() + a2q_acc.item()) / 2,
            'used_adjustment': adjusted
        }
        
        return loss, metrics


class HardNegativeInfoNCELoss(BaseLoss):
    """
    InfoNCE loss with additional penalty for hard negatives.
    Works with both standard batches and hard negative batches.
    """
    
    def __init__(self, temperature=0.1, hard_negative_weight=1.0):
        """
        Initialize the loss function.
        
        Args:
            temperature: Temperature parameter for scaling similarity scores
            hard_negative_weight: Weight for the hard negative component of the loss
        """
        super().__init__()
        self.temperature = temperature
        self.hard_negative_weight = hard_negative_weight
        self.name = f"hard_neg_infonce_t{temperature}_w{hard_negative_weight}"
        
    def forward(self, q_embeddings, a_embeddings, question_ids=None, **kwargs):
        """
        Calculate InfoNCE loss with hard negative penalty
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            question_ids: Question IDs to identify hard negatives
            
        Returns:
            loss: Combined InfoNCE loss with hard negative penalty
            metrics: Dictionary with accuracy and hard negative metrics
        """
        batch_size = q_embeddings.shape[0]
        device = q_embeddings.device
        
        # Standard InfoNCE loss with in-batch negatives
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        labels = torch.arange(batch_size, device=device)
        base_loss = F.cross_entropy(similarity, labels)
        
        # Additional penalty for hard negatives (answers to the same question)
        hard_negative_loss = torch.tensor(0.0, device=device)
        hard_neg_count = 0
        
        if question_ids is not None:
            # Group by question ID
            question_groups = defaultdict(list)
            for i, q_id in enumerate(question_ids):
                question_groups[q_id].append(i)
            
            # Calculate hard negative loss
            for q_id, indices in question_groups.items():
                if len(indices) <= 1:
                    continue  # Skip questions with only one answer
                
                # For each pair of answers to the same question
                for i, idx1 in enumerate(indices):
                    for j, idx2 in enumerate(indices):
                        if i != j:  # Different answers to same question
                            # Penalize high similarity between different answers to same question
                            sim = similarity[idx1, idx2]
                            hard_negative_loss += torch.exp(sim)
                            hard_neg_count += 1
        
        # Add weighted hard negative loss if found
        total_loss = base_loss
        if hard_neg_count > 0:
            hard_negative_loss = hard_negative_loss / hard_neg_count
            total_loss += self.hard_negative_weight * hard_negative_loss
        
        # Calculate metrics
        preds = torch.argmax(similarity, dim=1)
        accuracy = (preds == labels).float().mean()
        
        metrics = {
            'loss': total_loss.item(),
            'base_loss': base_loss.item(),
            'hard_neg_loss': hard_negative_loss.item() if isinstance(hard_negative_loss, torch.Tensor) else 0,
            'accuracy': accuracy.item(),
            'hard_neg_count': hard_neg_count
        }
        
        return total_loss, metrics


class MultiplePositivesLoss(BaseLoss):
    """
    Loss function that handles multiple positive answers per question.
    Works with the multiple_positives batch transform.
    """
    
    def __init__(self, temperature=0.1, rank_weight=0.1):
        """
        Initialize the loss function.
        
        Args:
            temperature: Temperature parameter for scaling similarity scores
            rank_weight: Weight for rank-based penalties (higher ranks get lower weight)
        """
        super().__init__()
        self.temperature = temperature
        self.rank_weight = rank_weight
        self.name = f"multi_pos_t{temperature}_w{rank_weight}"
        
    def forward(self, q_embeddings, a_embeddings, question_ids, ranks=None, **kwargs):
        """
        Calculate loss with multiple positives per question
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            question_ids: Question IDs to identify answers to the same question
            ranks: Rank of each answer (optional)
            
        Returns:
            loss: Combined loss accounting for multiple positives
            metrics: Dictionary with accuracy and other metrics
        """
        batch_size = q_embeddings.shape[0]
        device = q_embeddings.device
        
        # Calculate similarity matrix
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        
        # Group by question ID
        question_groups = defaultdict(list)
        for i, q_id in enumerate(question_ids):
            question_groups[q_id].append(i)
        
        # Calculate loss
        total_loss = torch.tensor(0.0, device=device)
        correct_count = 0
        total_count = 0
        
        # For each question
        for q_id, indices in question_groups.items():
            if len(indices) <= 0:
                continue
                
            for i, idx in enumerate(indices):
                # Get query embedding
                q_embed = q_embeddings[idx]
                
                # Calculate similarity to all answers
                sims = torch.matmul(q_embed.unsqueeze(0), a_embeddings.T).squeeze(0)
                
                # Create mask for positive samples (answers to same question)
                pos_mask = torch.zeros(batch_size, device=device)
                for pos_idx in indices:
                    weight = 1.0
                    # Apply rank weighting if available
                    if ranks is not None:
                        # Higher ranked answers get higher weight
                        weight = 1.0 - self.rank_weight * ranks[pos_idx].item()
                    pos_mask[pos_idx] = weight
                
                # Create masked loss - encourage high similarity with positives
                neg_mask = 1.0 - (pos_mask > 0).float()
                
                # Compute InfoNCE-style loss for this query
                numerator = torch.sum(pos_mask * torch.exp(sims))
                denominator = torch.sum(torch.exp(sims))
                loss_i = -torch.log(numerator / denominator + 1e-8)
                total_loss += loss_i
                
                # Calculate accuracy
                pred = torch.argmax(sims).item()
                if pos_mask[pred] > 0:
                    correct_count += 1
                total_count += 1
        
        # Average loss
        loss = total_loss / total_count if total_count > 0 else torch.tensor(0.0, device=device)
        
        # Calculate accuracy
        accuracy = correct_count / total_count if total_count > 0 else 0.0
        
        metrics = {
            'loss': loss.item(),
            'accuracy': accuracy,
            'total_samples': total_count
        }
        
        return loss, metrics


class TripletLoss(BaseLoss):
    """
    Triplet loss for query-positive-negative triplets.
    Works with the triplet batch transform.
    """
    
    def __init__(self, margin=0.3):
        """
        Initialize the triplet loss.
        
        Args:
            margin: Margin between positive and negative similarities
        """
        super().__init__()
        self.margin = margin
        self.name = f"triplet_m{margin}"
        
    def forward(self, q_embeddings, a_pos_embeddings, a_neg_embeddings, **kwargs):
        """
        Calculate triplet loss
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_pos_embeddings: Positive answer embeddings (batch_size, embed_dim)
            a_neg_embeddings: Negative answer embeddings (batch_size, embed_dim)
            
        Returns:
            loss: Triplet loss
            metrics: Dictionary with accuracy and similarity metrics
        """
        # Calculate similarity between queries and answers
        pos_sim = torch.sum(q_embeddings * a_pos_embeddings, dim=1)
        neg_sim = torch.sum(q_embeddings * a_neg_embeddings, dim=1)
        
        # Calculate triplet loss: enforce margin between pos_sim and neg_sim
        losses = F.relu(neg_sim - pos_sim + self.margin)
        loss = torch.mean(losses)
        
        # Calculate accuracy (how often pos_sim > neg_sim)
        acc = (pos_sim > neg_sim).float().mean()
        
        metrics = {
            'loss': loss.item(),
            'acc': acc.item(),
            'avg_pos_sim': pos_sim.mean().item(),
            'avg_neg_sim': neg_sim.mean().item(),
            'margin_violations': (losses > 0).float().mean().item()
        }
        
        return loss, metrics


class ListwiseRankingLoss(BaseLoss):
    """
    Listwise ranking loss for learning to rank multiple answers.
    Works with the listwise batch transform.
    """
    
    def __init__(self, temperature=1.0):
        """
        Initialize the listwise ranking loss.
        
        Args:
            temperature: Temperature for scaling similarities
        """
        super().__init__()
        self.temperature = temperature
        self.name = f"listwise_t{temperature}"
        
    def forward(self, q_embeddings, a_list_embeddings, a_list_scores, **kwargs):
        """
        Calculate listwise ranking loss
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
                          or list of embeddings, one per question
            a_list_embeddings: List of answer embeddings tensors
                              [(num_answers, embed_dim), ...]
            a_list_scores: List of scores for each answer per question
                          [(num_answers,), ...]
            
        Returns:
            loss: Listwise ranking loss
            metrics: Dictionary with NDCG and other metrics
        """
        # Determine if we're using a device (for GPU support)
        if isinstance(q_embeddings, list):
            device = q_embeddings[0].device if q_embeddings else torch.device('cpu')
        else:
            device = q_embeddings.device
            
        total_loss = torch.tensor(0.0, device=device)
        total_ndcg = 0.0
        
        # Check if we have a batch or individual questions
        if isinstance(q_embeddings, list):
            q_embeds = q_embeddings
        else:
            # Assume we have a batch with one embedding per question
            q_embeds = [q_embeddings[i].unsqueeze(0) for i in range(q_embeddings.shape[0])]
        
        # Make sure all lists have the same length
        min_length = min(len(q_embeds), len(a_list_embeddings), len(a_list_scores))
        
        # Process each question separately
        for i in range(min_length):
            q_embed = q_embeds[i]
            answers_embed = a_list_embeddings[i]
            scores = a_list_scores[i]
            
            # Calculate similarity between question and all answers
            sim = torch.matmul(q_embed, answers_embed.T).squeeze(0)  # (num_answers,)
            sim = sim / self.temperature
            
            # Convert to probabilities
            sim_probs = F.softmax(sim, dim=0)  # (num_answers,)
            
            # Create target probabilities from normalized scores
            target_probs = F.softmax(scores / self.temperature, dim=0)  # (num_answers,)
            
            # KL divergence loss
            loss_i = F.kl_div(torch.log(sim_probs + 1e-8), target_probs, reduction='sum')
            total_loss += loss_i
            
            # Calculate NDCG
            # Sort predicted and target rankings
            _, pred_indices = torch.sort(sim, descending=True)
            _, ideal_indices = torch.sort(scores, descending=True)
            
            # Calculate DCG
            pred_dcg = self._calculate_dcg(pred_indices, scores)
            ideal_dcg = self._calculate_dcg(ideal_indices, scores)
            
            # Calculate NDCG
            ndcg = pred_dcg / (ideal_dcg + 1e-8)
            total_ndcg += ndcg
        
        # Average loss and metrics
        batch_size = min_length
        loss = total_loss / batch_size if batch_size > 0 else total_loss
        avg_ndcg = total_ndcg / batch_size if batch_size > 0 else 0.0
        
        metrics = {
            'loss': loss.item(),
            'ndcg': avg_ndcg
        }
        
        return loss, metrics
    
    def _calculate_dcg(self, indices, scores):
        """Helper function to calculate DCG"""
        device = indices.device
        ranks = torch.arange(1, len(indices) + 1, dtype=torch.float, device=device)
        gain = scores[indices]
        return torch.sum(gain / torch.log2(ranks + 1))

# Factory function to create a loss by name
def create_loss(loss_name, **kwargs):
    """
    Factory function to create a loss by name
    
    Args:
        loss_name: Name of the loss function
        **kwargs: Additional parameters for the loss
        
    Returns:
        Loss function instance
        
    Raises:
        ValueError: If the loss name is not recognized
    """
    # Basic loss functions
    losses = {
        'infonce': StandardInfoNCELoss,
        'rank_infonce': RankInfoNCELoss,
        'hard_negative': HardNegativeInfoNCELoss,
        'multiple_positives': MultiplePositivesLoss,
        'triplet': TripletLoss,
        'listwise': ListwiseRankingLoss
    }
    
    # Handle specialized versions of InfoNCE
    if loss_name.startswith('infonce_'):
        # Parse options from name
        options = loss_name.split('_')[1:]
        
        # Default parameters
        params = {
            'temperature': kwargs.get('temperature', 0.1),
            'use_ranks': False,
            'use_scores': False,
            'rank_weight': 0.1,
            'score_weight': 0.01
        }
        
        # Update based on options
        for option in options:
            if option.startswith('rank'):
                params['use_ranks'] = True
                try:
                    params['rank_weight'] = float(option[4:])
                except (ValueError, IndexError):
                    pass
            elif option.startswith('score'):
                params['use_scores'] = True
                try:
                    params['score_weight'] = float(option[5:])
                except (ValueError, IndexError):
                    pass
            elif option.startswith('t'):
                try:
                    params['temperature'] = float(option[1:])
                except (ValueError, IndexError):
                    pass
        
        # Update with provided kwargs (override parsed values)
        params.update(kwargs)
        
        # Create RankInfoNCE with parsed parameters
        return RankInfoNCELoss(**params)
    
    # For standard loss names
    if loss_name not in losses:
        raise ValueError(f"Unknown loss function: {loss_name}. Available losses: {list(losses.keys())}")
    
    return losses[loss_name](**kwargs)

---
./src/rank_test/models.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
from transformers import DistilBertModel

class QAEmbeddingModel(nn.Module):
    """
    Model for embedding questions and answers using DistilBERT
    with a projection layer to reduce dimensionality.
    """
    def __init__(self, embed_dim=768, projection_dim=128):
        super(QAEmbeddingModel, self).__init__()
        # Load pretrained DistilBERT
        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')
        # Projection layer to get embeddings of the desired dimension
        self.projection = nn.Linear(embed_dim, projection_dim)
        
    def forward(self, input_ids, attention_mask):
        """
        Generate embeddings for input text
        
        Args:
            input_ids: Token IDs from tokenizer
            attention_mask: Attention mask from tokenizer
            
        Returns:
            normalized_embeddings: L2-normalized embeddings
        """
        # Get BERT embeddings
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Use the [CLS] token embedding (first token)
        embeddings = outputs.last_hidden_state[:, 0, :]
        # Project to lower dimension
        projected = self.projection(embeddings)
        # Normalize embeddings to unit length
        normalized = nn.functional.normalize(projected, p=2, dim=1)
        return normalized

---
./src/rank_test/train.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Training module for the flexible QA ranking dataset.
Supports various dataset strategies and loss functions with Pydantic-based configuration.
"""

import os
import time
import torch
import torch.optim as optim
import wandb
import json
import random
from tqdm import tqdm
from transformers import DistilBertTokenizerFast

from rank_test.config import ExperimentConfig, PREDEFINED_CONFIGS
from rank_test.models import QAEmbeddingModel
from rank_test.dataset import (
    QADataset, 
    get_batch_transform,
    ensure_dataset_exists
)
from rank_test.losses import create_loss
from rank_test.evaluate import evaluate_model


def get_device():
    """Get appropriate device for training"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS (Metal Performance Shaders) on Mac")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("Using CUDA (NVIDIA GPU)")
    else:
        device = torch.device("cpu")
        print("Using CPU (No GPU acceleration available)")
    
    return device


def train_epoch(model, dataloader, loss_fn, optimizer, device, epoch, config, 
               test_dataloader=None, step_offset=0):
    """
    Train model for one epoch
    
    Args:
        model: Model to train
        dataloader: DataLoader with training data
        loss_fn: Loss function
        optimizer: Optimizer
        device: Device to train on
        epoch: Current epoch number
        config: Configuration object
        test_dataloader: DataLoader with test data for periodic evaluation
        step_offset: Global step count offset from previous epochs
        
    Returns:
        Dictionary of training metrics and global step count
    """
    model.train()
    
    # Training metrics
    epoch_loss = 0
    epoch_metrics = {}
    
    # Create progress bar
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
    
    # Track batch time for logging
    batch_times = []
    start_time = time.time()
    
    # Global step counter and doc counter
    global_step = step_offset
    cumulative_docs = 0
    
    for batch_idx, batch_data in enumerate(progress_bar):
        # Extract batch and document count
        if isinstance(batch_data, tuple) and len(batch_data) == 2:
            batch, cumulative_docs = batch_data
        else:
            batch = batch_data
        
        # Update global step
        global_step += 1
        
        # Calculate batch processing time
        batch_start = time.time()
        
        # Process batch based on transform type
        batch_transform = config.batch_transform
        
        if batch_transform == "triplet":
            # Process triplet format
            q_embeddings = model(batch['q_input_ids'].to(device), 
                               batch['q_attention_mask'].to(device))
            a_pos_embeddings = model(batch['a_pos_input_ids'].to(device), 
                                   batch['a_pos_attention_mask'].to(device))
            a_neg_embeddings = model(batch['a_neg_input_ids'].to(device), 
                                   batch['a_neg_attention_mask'].to(device))
            
            # Calculate loss
            loss, batch_metrics = loss_fn(q_embeddings, a_pos_embeddings, a_neg_embeddings)
            
        elif batch_transform == "listwise":
            # Process listwise format - each item in batch has a question and multiple answers
            loss_total = 0
            batch_metrics_sum = {}
            
            for item in batch:
                # Process question
                q_embedding = model(item['q_input_ids'].unsqueeze(0).to(device), 
                                  item['q_attention_mask'].unsqueeze(0).to(device))
                
                # Process all answers for this question
                a_embeddings = model(item['a_input_ids'].to(device), 
                                   item['a_attention_masks'].to(device))
                
                # Calculate loss for this question
                item_loss, item_metrics = loss_fn(
                    [q_embedding], 
                    [a_embeddings], 
                    [item['scores'].to(device)]
                )
                
                # Accumulate loss and metrics
                loss_total += item_loss
                for k, v in item_metrics.items():
                    if k in batch_metrics_sum:
                        batch_metrics_sum[k] += v
                    else:
                        batch_metrics_sum[k] = v
            
            # Average metrics across all items in batch
            batch_count = len(batch)
            if batch_count > 0:
                loss = loss_total / batch_count
                batch_metrics = {k: v / batch_count for k, v in batch_metrics_sum.items()}
            else:
                loss = torch.tensor(0.0, device=device)
                batch_metrics = {'loss': 0.0}
            
        elif batch_transform == "hard_negative":
            # Process hard negative format - each item has a question and multiple answers
            loss_total = 0
            batch_metrics_sum = {}
            total_items = 0
            
            for item in batch:
                # Process question
                q_input_ids = item['q_input_ids'].unsqueeze(0).to(device)
                q_attention_mask = item['q_attention_mask'].unsqueeze(0).to(device)
                q_embedding = model(q_input_ids, q_attention_mask)
                
                # Process answers
                answers = item['answers']
                a_embeddings = []
                
                for answer in answers:
                    a_input_ids = answer['input_ids'].unsqueeze(0).to(device)
                    a_attention_mask = answer['attention_mask'].unsqueeze(0).to(device)
                    a_embedding = model(a_input_ids, a_attention_mask)
                    a_embeddings.append(a_embedding)
                
                a_embeddings = torch.cat(a_embeddings, dim=0)
                
                # Create question_ids for loss function (all same ID)
                question_ids = [item['question_id']] * len(answers)
                
                # Calculate loss
                item_loss, item_metrics = loss_fn(
                    q_embedding.repeat(len(answers), 1), 
                    a_embeddings,
                    question_ids=question_ids
                )
                
                # Accumulate loss and metrics
                loss_total += item_loss
                for k, v in item_metrics.items():
                    if k in batch_metrics_sum:
                        batch_metrics_sum[k] += v
                    else:
                        batch_metrics_sum[k] = v
                
                total_items += 1
            
            # Average metrics
            if total_items > 0:
                loss = loss_total / total_items
                batch_metrics = {k: v / total_items for k, v in batch_metrics_sum.items()}
            else:
                loss = torch.tensor(0.0, device=device)
                batch_metrics = {'loss': 0.0}
            
        else:
            # Standard format (infonce, multiple_positives)
            q_embeddings = model(batch['q_input_ids'].to(device), 
                               batch['q_attention_mask'].to(device))
            a_embeddings = model(batch['a_input_ids'].to(device), 
                               batch['a_attention_mask'].to(device))
            
            # Additional parameters for different loss functions
            kwargs = {}
            if 'question_ids' in batch:
                kwargs['question_ids'] = batch['question_ids']
            if 'ranks' in batch:
                kwargs['ranks'] = batch['ranks'].to(device)
            if 'scores' in batch:
                kwargs['scores'] = batch['scores'].to(device)
            
            # Calculate loss
            loss, batch_metrics = loss_fn(q_embeddings, a_embeddings, **kwargs)
        
        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Update metrics
        epoch_loss += loss.item()
        for k, v in batch_metrics.items():
            if k in epoch_metrics:
                epoch_metrics[k] += v
            else:
                epoch_metrics[k] = v
        
        # Log metrics for this batch
        batch_metrics_log = {f"batch/{k}": v for k, v in batch_metrics.items()}
        batch_metrics_log["batch/loss"] = loss.item()
        batch_metrics_log["global_step"] = global_step
        batch_metrics_log["cumulative_docs_seen"] = cumulative_docs
        batch_metrics_log["docs_per_step"] = cumulative_docs / max(global_step, 1)
        
        # Calculate batch time
        batch_time = time.time() - batch_start
        batch_times.append(batch_time)
        batch_metrics_log["batch/time"] = batch_time
        
        # Update progress bar with latest metrics
        progress_desc = f"Epoch {epoch+1} | Step {global_step} | "
        if 'acc' in batch_metrics:
            progress_desc += f"Acc: {batch_metrics['acc']:.4f} | "
        elif 'accuracy' in batch_metrics:
            progress_desc += f"Acc: {batch_metrics['accuracy']:.4f} | "
        elif 'avg_acc' in batch_metrics:
            progress_desc += f"Acc: {batch_metrics['avg_acc']:.4f} | "
        progress_desc += f"Loss: {loss.item():.4f}"
        progress_bar.set_description(progress_desc)
        
        # Log to wandb
        if wandb.run is not None:
            wandb.log(batch_metrics_log)
        
        # Run evaluation if requested
        if config.eval_steps and test_dataloader and global_step % config.eval_steps == 0:
            # Temporarily switch to eval mode
            model.eval()
            
            print(f"\nRunning evaluation at step {global_step}...")
            try:
                # Run evaluation
                test_metrics = evaluate_model(model, test_dataloader, device)
                
                # Print metrics
                print(f"Step {global_step} evaluation results:")
                for metric_name, metric_value in test_metrics.items():
                    if 'hard_neg' in metric_name:
                        print(f"  {metric_name}: {metric_value:.4f}*") # Mark fixed metrics
                    else:
                        print(f"  {metric_name}: {metric_value:.4f}")
                
                # Log metrics
                if wandb.run is not None:
                    test_metrics_log = {f"step_eval/{k}": v for k, v in test_metrics.items()}
                    test_metrics_log["global_step"] = global_step
                    wandb.log(test_metrics_log)
            
            except Exception as e:
                print(f"Error during evaluation: {e}")
            
            # Switch back to training mode
            model.train()
    
    # Calculate epoch averages
    num_batches = len(dataloader)
    epoch_loss /= max(num_batches, 1)
    for k in epoch_metrics:
        epoch_metrics[k] /= max(num_batches, 1)
    
    # Add loss to metrics
    epoch_metrics['loss'] = epoch_loss
    
    # Calculate epoch time
    epoch_time = time.time() - start_time
    epoch_metrics['time'] = epoch_time
    epoch_metrics['avg_batch_time'] = sum(batch_times) / max(len(batch_times), 1)
    
    # Print epoch summary
    print(f"Epoch {epoch+1} - Loss: {epoch_loss:.4f} | Time: {epoch_time:.2f}s")
    for k, v in epoch_metrics.items():
        if k not in ['loss', 'time', 'avg_batch_time']:
            print(f"  {k}: {v:.4f}")
    
    # Log epoch metrics to wandb
    if wandb.run is not None:
        wandb.log({f"epoch/{k}": v for k, v in epoch_metrics.items()})
        wandb.log({"epoch": epoch+1})
    
    return epoch_metrics, global_step


def train_model(config: ExperimentConfig):
    """
    Train model with the given configuration
    
    Args:
        config: ExperimentConfig object
        
    Returns:
        Trained model and evaluation metrics
    """
    # Initialize training
    print("Using enhanced evaluation with standardized test format")
    
    # Ensure dataset exists
    data_path = config.data_path
    ensure_dataset_exists(
        data_path=data_path,
        data_limit=config.get_limit(),
        force_regenerate=config.force_regenerate
    )
    
    # Get device
    device = get_device()
    
    # Initialize wandb
    if config.log_to_wandb and wandb.run is None:
        run_name = f"{config.loss_type}-{config.batch_transform}-{time.strftime('%Y%m%d-%H%M%S')}"
        if config.name:
            run_name = f"{config.name}-{time.strftime('%Y%m%d-%H%M%S')}"
        
        # Create a clean config for wandb logging
        wandb_config = config.dict()
        
        wandb.init(
            project=config.wandb_project,
            name=run_name,
            config=wandb_config
        )
        
        # Log loss parameters as a table
        loss_kwargs = config.get_loss_kwargs()
        if loss_kwargs:
            loss_params = [[k, str(v)] for k, v in loss_kwargs.items()]
            wandb.log({"loss_parameters": wandb.Table(columns=["Parameter", "Value"], 
                                                    data=loss_params)})
    
    # Create model
    print(f"Creating model with embed_dim={config.embed_dim} and projection_dim={config.projection_dim}")
    model = QAEmbeddingModel(
        embed_dim=config.embed_dim,
        projection_dim=config.projection_dim
    ).to(device)
    
    # Create tokenizer
    print("Creating tokenizer")
    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
    
    # Create dataset based on strategy
    if config.dataset_strategy == "flexible":
        # Get batch transform function
        batch_transform_fn = get_batch_transform(config.batch_transform)
        
        # Load all data
        with open(data_path, 'r') as f:
            all_data = json.load(f)
        
        # Split data based on test size
        num_items = len(all_data)
        test_size = config.test_size
        test_count = int(num_items * test_size)
        
        # Create indices and shuffle
        indices = list(range(num_items))
        random.seed(config.seed)  # Use seed for reproducibility
        random.shuffle(indices)
        
        # Split indices
        test_indices = indices[:test_count]
        train_indices = indices[test_count:]
        
        # Create train and test datasets
        train_data = [all_data[i] for i in train_indices]
        test_data = [all_data[i] for i in test_indices]
        
        print(f"Splitting dataset: {len(train_data)} training samples, {len(test_data)} test samples")
        
        # Create flexible dataset for training
        print("Creating flexible dataset")
        dataset = QADataset(
            data_path=data_path,
            batch_transform_fn=batch_transform_fn,
            batch_size=config.get_batch_size(),
            tokenizer=tokenizer,
            max_length=128,
            **config.get_batch_transform_kwargs()
        )
        # Override raw data with train split
        dataset.raw_data = train_data
        # Recreate batches with train data
        dataset.batches = dataset._create_batches()
        
        # Create dataloader
        print("Creating train dataloader")
        train_loader = QADataset.get_dataloader(dataset, shuffle=True)
        
        # Create standardized test dataset that's consistent across all training methods
        print("Creating standardized test dataset")
        # Always use the standardized test transform regardless of training strategy
        test_transform_fn = get_batch_transform("standardized_test")
        test_batch_size = min(len(test_data), config.get_batch_size() * 4)  # Use larger batches for testing
        
        test_dataset = QADataset(
            data_path=data_path,
            batch_transform_fn=test_transform_fn,
            batch_size=test_batch_size,
            tokenizer=tokenizer,
            max_length=128
        )
        # Override raw data with test split
        test_dataset.raw_data = test_data
        # Recreate batches with test data
        test_dataset.batches = test_dataset._create_batches()
        
        test_loader = QADataset.get_dataloader(test_dataset, shuffle=False)
        
        print(f"Created flexible dataset with {len(dataset)} batches")
        print(f"Test dataset: {len(test_dataset)} batches")
    else:
        # TODO: Use standard dataset approach from original code
        raise NotImplementedError("Standard dataset strategy not implemented in this module")
    
    # Create loss function
    loss_kwargs = config.get_loss_kwargs()
    loss_fn = create_loss(config.loss_type, **loss_kwargs)
    print(f"Using loss function: {loss_fn.get_name()}")
    
    # Create optimizer
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    # Create output directory
    os.makedirs(config.output_dir, exist_ok=True)
    model_dir = os.path.join(config.output_dir, run_name if wandb.run is not None else "model")
    os.makedirs(model_dir, exist_ok=True)
    
    # Save configuration
    config_path = os.path.join(model_dir, "config.json")
    config.save_to_file(config_path)
    print(f"Saved configuration to {config_path}")
    
    # Training loop
    print(f"Starting training for {config.get_epochs()} epochs")
    
    best_metrics = None
    best_loss = float('inf')
    global_step = 0
    
    # Run evaluation at step 0 if requested
    if config.eval_at_zero and test_loader is not None:
        print("\nRunning evaluation at step 0 (initial model)...")
        try:
            model.eval()
            
            # Run evaluation
            initial_metrics = evaluate_model(model, test_loader, device)
            
            # Print metrics
            print("Step 0 evaluation results:")
            for metric_name, metric_value in initial_metrics.items():
                if 'hard_neg' in metric_name:
                    print(f"  {metric_name}: {metric_value:.4f}*")  # Mark fixed metrics
                else:
                    print(f"  {metric_name}: {metric_value:.4f}")
            
            # Log metrics to wandb
            if wandb.run is not None:
                initial_metrics_log = {f"step_eval/{k}": v for k, v in initial_metrics.items()}
                initial_metrics_log["global_step"] = 0
                wandb.log(initial_metrics_log)
            
            # Save initial metrics
            initial_metrics_path = os.path.join(model_dir, "initial_metrics.json")
            with open(initial_metrics_path, 'w') as f:
                json.dump(initial_metrics, f, indent=2)
                
            model.train()
        except Exception as e:
            print(f"Error during initial evaluation: {e}")
            model.train()
    
    for epoch in range(config.get_epochs()):
        # Train one epoch with step-based evaluation
        metrics, global_step = train_epoch(
            model=model,
            dataloader=train_loader,
            loss_fn=loss_fn,
            optimizer=optimizer,
            device=device,
            epoch=epoch,
            config=config,
            test_dataloader=test_loader,
            step_offset=global_step
        )
        
        # Save checkpoint if requested
        if (epoch + 1) % config.checkpoint_interval == 0 or epoch == config.get_epochs() - 1:
            checkpoint_path = os.path.join(model_dir, f"checkpoint_{epoch+1}.pt")
            torch.save(model.state_dict(), checkpoint_path)
            print(f"Saved checkpoint to {checkpoint_path}")
        
        # Update best model if loss improved
        if metrics['loss'] < best_loss:
            best_loss = metrics['loss']
            best_metrics = metrics
            best_model_path = os.path.join(model_dir, "best_model.pt")
            torch.save(model.state_dict(), best_model_path)
            print(f"New best model saved to {best_model_path}")
            
            # Add flag in wandb
            if wandb.run is not None:
                wandb.log({"best_model": True, "global_step": global_step})
    
    # Save final model
    final_model_path = os.path.join(model_dir, "final_model.pt")
    torch.save(model.state_dict(), final_model_path)
    print(f"Final model saved to {final_model_path}")
    
    # Evaluate on test set
    if test_loader is not None:
        print("\nEvaluating on test set...")
        # Run evaluation
        test_metrics = evaluate_model(model, test_loader, device)
        
        # Print test metrics
        print("\nTest metrics:")
        for k, v in test_metrics.items():
            if 'hard_neg' in k:
                print(f"  {k}: {v:.4f}*")  # Mark fixed metrics with an asterisk
            else:
                print(f"  {k}: {v:.4f}")
        
        # Log test metrics to wandb
        if wandb.run is not None:
            wandb.log({f"test/{k}": v for k, v in test_metrics.items()})
        
        # Save test metrics
        test_metrics_path = os.path.join(model_dir, "test_metrics.json")
        with open(test_metrics_path, 'w') as f:
            json.dump(test_metrics, f, indent=2)
    
    # Finish wandb run
    if wandb.run is not None:
        wandb.finish()
    
    return model, best_metrics


def main():
    """Main function to run training from command line"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Train QA model with flexible dataset strategies")
    parser.add_argument("--config", type=str, help="Path to JSON configuration file")
    parser.add_argument("--preset", type=str, choices=list(PREDEFINED_CONFIGS.keys()),
                       help="Use a predefined configuration preset")
    parser.add_argument("--output", type=str, help="Override output directory")
    parser.add_argument("--debug", action="store_true", help="Run in debug mode with minimal data")
    parser.add_argument("--no-wandb", action="store_true", help="Disable wandb logging")
    
    args = parser.parse_args()
    
    # Load configuration
    if args.config:
        print(f"Loading configuration from {args.config}")
        config = ExperimentConfig.from_file(args.config)
    elif args.preset:
        print(f"Using preset configuration: {args.preset}")
        config = PREDEFINED_CONFIGS[args.preset]
    else:
        print("Using default configuration")
        config = ExperimentConfig()
    
    # Override configuration with command line arguments
    if args.output:
        config.output_dir = args.output
    if args.debug:
        config.debug = True
    if args.no_wandb:
        config.log_to_wandb = False
    
    # Train model
    model, metrics = train_model(config)
    
    print("\nTraining complete!")
    print(f"Best metrics: {metrics}")


if __name__ == "__main__":
    main()

---
./src/rank_test/transforms.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Batch transformation strategies for QA ranking tasks.

This module provides various batch transformation strategies for QA ranking tasks.
These transformations prepare data in different formats suitable for different loss functions:

1. infonce - Standard InfoNCE contrastive learning with in-batch negatives
2. multiple_positives - Handles multiple positive answers per question with rank weighting
3. hard_negative - Explicitly incorporates hard negatives (low-ranked answers to same question)
4. triplet - Creates triplets of (query, positive, negative) for triplet loss
5. listwise - Prepares data for listwise ranking losses, handling multiple ranked answers per query
6. standardized_test - Creates a standardized test batch format for consistent evaluation

Each transformation can be used with the QADataset by setting the appropriate batch_transform_fn.
"""

import torch
from typing import List, Dict, Callable, Tuple, Optional, Union
from collections import defaultdict
from tqdm import tqdm
import re

def clean_html(text: str) -> str:
    """Remove HTML tags from text"""
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def infonce_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    take_top: bool = True,
    **kwargs
) -> Tuple[Dict, int]:
    """
    Standard InfoNCE transform with batched tokenization for better performance.
    
    Creates batches where:
    - Each question is paired with its top (or random) answer
    - During training, negatives come from other questions in the same batch
    - Original ranks and scores are preserved for potential weighting
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        take_top: If True, use the highest-ranked answer, otherwise random
        
    Returns:
        Tuple of (batch dictionary with tokenized inputs, document count)
    """
    # Collect all texts first
    questions_to_tokenize = []
    answers_to_tokenize = []
    question_ids = []
    answer_ids = []
    scores = []
    ranks = []
    
    # Document counter
    doc_count = 0
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        
        answers = item['answers']
        if not answers:
            continue
            
        # Sort answers by score (descending) to establish ranks
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
        
        if not take_top and len(sorted_answers) > 1:
            selected_answer = random.choice(sorted_answers)
            rank = sorted_answers.index(selected_answer)
        else:
            selected_answer = sorted_answers[0]
            rank = 0
        
        a_text = clean_html(selected_answer['body'])
        
        # Collect texts and metadata
        questions_to_tokenize.append(q_text)
        answers_to_tokenize.append(a_text)
        question_ids.append(question['id'])
        answer_ids.append(selected_answer['id'])
        scores.append(float(selected_answer['score']))
        ranks.append(rank)
        
        # Count documents (1 question + 1 answer)
        doc_count += 2
    
    if not questions_to_tokenize:
        return None, 0
        
    # Batch tokenize questions
    q_encodings = tokenizer(
        questions_to_tokenize,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # Batch tokenize answers
    a_encodings = tokenizer(
        answers_to_tokenize,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # Create final batch dictionary
    batch = {
        'q_input_ids': q_encodings['input_ids'],
        'q_attention_mask': q_encodings['attention_mask'],
        'a_input_ids': a_encodings['input_ids'],
        'a_attention_mask': a_encodings['attention_mask'],
        'question_ids': question_ids,
        'answer_ids': answer_ids,
        'scores': torch.tensor(scores, dtype=torch.float32),
        'ranks': torch.tensor(ranks, dtype=torch.long)
    }
    
    return batch, doc_count


def multiple_positives_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    pos_count: int = 3,
    **kwargs
) -> Tuple[Dict, int]:
    """
    Multiple positives transform for contrastive learning.
    
    Creates batches where:
    - Each question appears multiple times with different positive answers
    - Each question-answer pair maintains both cardinal scores and ordinal ranks
    - Useful for models that need to learn subtle differences between answers
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        pos_count: Maximum number of positive answers to include per question
        
    Returns:
        Tuple of (batch dictionary with tokenized inputs, document count)
    """
    q_input_ids, q_attention_mask = [], []
    a_input_ids, a_attention_mask = [], []
    question_ids, answer_ids = [], []
    scores, ranks = [], []
    
    # Document counter
    doc_count = 0
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if not answers:
            continue
            
        # Sort answers by score (descending) to establish ranks
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
        
        # Take top K answers as positives (or fewer if not enough answers)
        positives = sorted_answers[:min(pos_count, len(sorted_answers))]
        
        # Count documents (1 question + N answers)
        doc_count += 1 + len(positives)
            
        # For each positive answer
        for rank, answer in enumerate(positives):
            a_text = clean_html(answer['body'])
            
            # Tokenize
            q_encoding = tokenizer(q_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            
            # Add to batch
            q_input_ids.append(q_encoding['input_ids'])
            q_attention_mask.append(q_encoding['attention_mask'])
            a_input_ids.append(a_encoding['input_ids'])
            a_attention_mask.append(a_encoding['attention_mask'])
            question_ids.append(q_id)
            answer_ids.append(answer['id'])
            scores.append(float(answer['score']))  # Cardinal score
            ranks.append(rank)                    # Ordinal rank (position)
    
    if not q_input_ids:
        return None, 0
        
    # Create final batch
    batch = {
        'q_input_ids': torch.cat(q_input_ids, dim=0),
        'q_attention_mask': torch.cat(q_attention_mask, dim=0),
        'a_input_ids': torch.cat(a_input_ids, dim=0),
        'a_attention_mask': torch.cat(a_attention_mask, dim=0),
        'question_ids': question_ids,
        'answer_ids': answer_ids,
        'scores': torch.tensor(scores, dtype=torch.float32),  # Cardinal scores 
        'ranks': torch.tensor(ranks, dtype=torch.long)       # Ordinal ranks
    }
    
    return batch, doc_count


def hard_negative_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    **kwargs
) -> Dict:
    """
    Hard negative transform that explicitly includes lower-ranked 
    answers as hard negatives.
    
    Creates batches where:
    - Each question includes its top answer and lower-ranked answers
    - Each answer maintains both cardinal scores and ordinal ranks
    - Designed for loss functions that need to handle hard negatives explicitly
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        
    Returns:
        Batch dictionary with tokenized inputs for all answers per question
    """
    batch_items = []
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if len(answers) <= 1:
            continue  # Need multiple answers
        
        # Sort answers by score to establish ranks
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
            
        # Tokenize question once
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        
        # Tokenize all answers
        a_encodings = []
        for rank, answer in enumerate(sorted_answers):
            a_text = clean_html(answer['body'])
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            
            # Store answer data
            a_encodings.append({
                'input_ids': a_encoding['input_ids'].squeeze(0),
                'attention_mask': a_encoding['attention_mask'].squeeze(0),
                'id': answer['id'],
                'score': float(answer['score']),    # Cardinal score (actual value)
                'rank': rank                       # Ordinal rank (position)
            })
        
        # Create batch item with question and all its answers
        batch_items.append({
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'answers': a_encodings,
            'question_id': q_id,
            'answer_count': len(a_encodings)
        })
    
    return batch_items


def triplet_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    neg_strategy: str = "hard_negative",
    **kwargs
) -> Dict:
    """
    Triplet transform for triplet loss learning.
    
    Creates batches of triplets where:
    - Each question is paired with a positive and negative answer
    - Different strategies for selecting negatives are supported
    - Specifically designed for triplet loss functions
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        neg_strategy: Strategy for selecting negatives:
                     "hard_negative" - use lower-ranked answer to same question
                     "in_batch" - use answer from different question
                     "mixed" - randomly mix both strategies
        
    Returns:
        Batch dictionary with tokenized triplets
    """
    import random
    
    q_input_ids, q_attention_mask = [], []
    a_pos_input_ids, a_pos_attention_mask = [], []
    a_neg_input_ids, a_neg_attention_mask = [], []
    question_ids, pos_scores, neg_scores = [], [], []
    
    # Group answers by question for sampling
    question_answers = {}
    for item in batch_data:
        q_id = item['question']['id']
        question_answers[q_id] = item
    
    # Question IDs in this batch
    batch_q_ids = list(question_answers.keys())
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if not answers:
            continue
            
        # Get positive (top answer)
        pos_answer = answers[0]
        pos_text = clean_html(pos_answer['body'])
        
        # Get negative based on strategy
        neg_text = None
        neg_score = 0
        
        if neg_strategy == "hard_negative":
            # Use lower ranked answer from same question
            if len(answers) > 1:
                neg_answer = answers[-1]  # Lowest ranked
                neg_text = clean_html(neg_answer['body'])
                neg_score = float(neg_answer['score'])
                
        elif neg_strategy == "in_batch":
            # Use answer from different question
            other_questions = [q for q in batch_q_ids if q != q_id]
            if other_questions:
                other_q_id = random.choice(other_questions)
                other_answers = question_answers[other_q_id]['answers']
                if other_answers:
                    other_answer = other_answers[0]  # Top answer from other question
                    neg_text = clean_html(other_answer['body'])
                    neg_score = float(other_answer['score'])
                    
        elif neg_strategy == "mixed":
            # 50% chance to use each strategy
            if random.random() < 0.5 and len(answers) > 1:
                # Hard negative
                neg_answer = answers[-1]
                neg_text = clean_html(neg_answer['body'])
                neg_score = float(neg_answer['score'])
            else:
                # Other question negative
                other_questions = [q for q in batch_q_ids if q != q_id]
                if other_questions:
                    other_q_id = random.choice(other_questions)
                    other_answers = question_answers[other_q_id]['answers']
                    if other_answers:
                        other_answer = other_answers[0]
                        neg_text = clean_html(other_answer['body'])
                        neg_score = float(other_answer['score'])
        
        # Skip if no negative found
        if not neg_text:
            continue
            
        # Tokenize
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        pos_encoding = tokenizer(pos_text, max_length=max_length, 
                               padding='max_length', truncation=True, return_tensors='pt')
        neg_encoding = tokenizer(neg_text, max_length=max_length, 
                               padding='max_length', truncation=True, return_tensors='pt')
        
        # Add to batch
        q_input_ids.append(q_encoding['input_ids'])
        q_attention_mask.append(q_encoding['attention_mask'])
        a_pos_input_ids.append(pos_encoding['input_ids'])
        a_pos_attention_mask.append(pos_encoding['attention_mask'])
        a_neg_input_ids.append(neg_encoding['input_ids'])
        a_neg_attention_mask.append(neg_encoding['attention_mask'])
        question_ids.append(q_id)
        pos_scores.append(float(pos_answer['score']))
        neg_scores.append(neg_score)
    
    if not q_input_ids:
        return None
        
    # Create final batch
    batch = {
        'q_input_ids': torch.cat(q_input_ids, dim=0),
        'q_attention_mask': torch.cat(q_attention_mask, dim=0),
        'a_pos_input_ids': torch.cat(a_pos_input_ids, dim=0),
        'a_pos_attention_mask': torch.cat(a_pos_attention_mask, dim=0),
        'a_neg_input_ids': torch.cat(a_neg_input_ids, dim=0),
        'a_neg_attention_mask': torch.cat(a_neg_attention_mask, dim=0),
        'question_ids': question_ids,
        'pos_scores': torch.tensor(pos_scores, dtype=torch.float32),
        'neg_scores': torch.tensor(neg_scores, dtype=torch.float32)
    }
    
    return batch


def listwise_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    max_answers: int = 5,
    **kwargs
) -> Dict:
    """
    Listwise transform for listwise ranking losses.
    
    Creates batches where:
    - Each question has multiple answers with scores
    - Answers are ranked by their original scores
    - Designed for listwise ranking loss functions
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        max_answers: Maximum number of answers to include per question
        
    Returns:
        Batch dictionary with tokenized questions and multiple answers
    """
    batch_items = []
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if len(answers) < 2:  # Need at least 2 answers for ranking
            continue
            
        # Limit number of answers
        answers = answers[:min(max_answers, len(answers))]
        
        # Tokenize question
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        
        # Tokenize all answers
        a_input_ids = []
        a_attention_masks = []
        scores = []
        
        for answer in answers:
            a_text = clean_html(answer['body'])
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            
            a_input_ids.append(a_encoding['input_ids'])
            a_attention_masks.append(a_encoding['attention_mask'])
            scores.append(float(answer['score']))
        
        # Stack answer tensors
        a_input_ids = torch.cat(a_input_ids, dim=0)
        a_attention_masks = torch.cat(a_attention_masks, dim=0)
        scores = torch.tensor(scores, dtype=torch.float32)
        
        # Normalize scores to [0, 1]
        if torch.max(scores) > 0:
            scores = scores / torch.max(scores)
        
        # Add to batch
        batch_items.append({
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'a_input_ids': a_input_ids,
            'a_attention_masks': a_attention_masks,
            'scores': scores,
            'question_id': q_id,
            'answer_count': len(scores)
        })
    
    return batch_items


def standardized_test_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int,
    **kwargs
) -> Tuple[Dict, int]:
    """
    Creates a standardized test batch with positive, hard negative, and 
    normal negative samples for each question.
    
    This test transform is strategy-agnostic and provides consistent
    evaluation across all training approaches. The format includes:
    - Original positive answer (highest scored)
    - Hard negative answers (lower-ranked answers to same question)
    - Normal negatives (answers from other questions)
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        
    Returns:
        Tuple of (batch dict for standardized evaluation, document count)
    """
    test_items = []
    
    for item in batch_data:
        question = item['question']
        q_id = question['id']
        q_text = question['title'] + " " + clean_html(question['body'])
        
        # Get all answers for this question, sorted by score
        answers = item['answers']
        if len(answers) < 2:
            continue
            
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
        
        # Tokenize question
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        
        # Get all answers
        all_answers = []
        
        # Top answer is positive
        if sorted_answers:
            pos_answer = sorted_answers[0]
            pos_text = clean_html(pos_answer['body'])
            pos_encoding = tokenizer(pos_text, max_length=max_length, 
                                  padding='max_length', truncation=True, return_tensors='pt')
            
            all_answers.append({
                'input_ids': pos_encoding['input_ids'].squeeze(0),
                'attention_mask': pos_encoding['attention_mask'].squeeze(0),
                'score': float(pos_answer['score']),
                'rank': 0,
                'answer_id': pos_answer['id'],
                'is_positive': True,
                'is_hard_negative': False
            })
        
        # Add hard negatives (lower-ranked answers to same question)
        for i, answer in enumerate(sorted_answers[1:min(6, len(sorted_answers))]):  # Up to 5 hard negatives
            a_text = clean_html(answer['body'])
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                padding='max_length', truncation=True, return_tensors='pt')
            
            all_answers.append({
                'input_ids': a_encoding['input_ids'].squeeze(0),
                'attention_mask': a_encoding['attention_mask'].squeeze(0),
                'score': float(answer['score']),
                'rank': i+1,
                'answer_id': answer['id'],
                'is_positive': False,
                'is_hard_negative': True
            })
        
        # Add test item
        test_items.append({
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'question_id': q_id,
            'answers': all_answers
        })
    
    # For batch-level negative sampling, add answers from other questions
    # as normal negatives to each question's answer pool
    for i, item in enumerate(test_items):
        for j, other_item in enumerate(test_items):
            if i != j:  # Different question
                # Add top answer from other question as a normal negative
                other_answers = other_item['answers']
                if other_answers:
                    other_top = other_answers[0]  # Top answer
                    
                    # Add to this question's answer pool
                    item['answers'].append({
                        'input_ids': other_top['input_ids'],
                        'attention_mask': other_top['attention_mask'],
                        'score': 0.0,  # Lower score for negatives
                        'rank': 999,  # High rank for negatives
                        'answer_id': other_top.get('answer_id', 'unknown'),
                        'is_positive': False,
                        'is_hard_negative': False,
                        'from_question_id': other_item['question_id']
                    })
    
    # Count documents - for eval we don't track this as closely
    doc_count = 0
    for item in test_items:
        # One question + all its answers
        doc_count += 1 + len(item['answers'])
    
    return test_items, doc_count


# Factory function to get the transform function by name
def get_batch_transform(transform_name: str) -> Callable:
    """
    Get a batch transform function by name
    
    Args:
        transform_name: Name of the transform function
        
    Returns:
        Batch transform function
        
    Raises:
        ValueError: If the transform name is not recognized
    """
    transforms = {
        'infonce': infonce_batch_transform,
        'multiple_positives': multiple_positives_batch_transform,
        'hard_negative': hard_negative_batch_transform,
        'triplet': triplet_batch_transform,
        'listwise': listwise_batch_transform,
        'standardized_test': standardized_test_transform
    }
    
    if transform_name not in transforms:
        raise ValueError(f"Unknown transform: {transform_name}. Available transforms: {list(transforms.keys())}")
    
    return transforms[transform_name]

---
