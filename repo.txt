./CLAUDE.md
---
Always run things via uv run $file

Always add packages via uv add $dep

Import things via absolute imports


Design code so you can run things and validate as you go (without requiring too much time to run each file, always add debug mode that runs quickly), and break into as small changes as possible and test them incrementally

Use git! after each change, make git commits etc

IMPORTANT: You must always break changes into the smallest unit possible -- if I ask you to do something, NEVER just go off and start editing files unless it's trivial. Instead, propose a plan, ask for my input, and then make the smallest increment change possible and run it every time to confirm it works

Prioritize simple code that works on one path -- if there's a bug with data shape, find the source of the bug rather than adding defnesive checks. Similarly, when refactoring unless I explicitly ask don't create legacy versions of the code to support both -- just update it so everything works with the new one.

---
./README.md
---
# QA Embeddings Loss Comparison

A system to compare different loss functions for embedding models on question-answer retrieval tasks.

## Overview

This project compares how different loss functions perform when training embedding models for retrieving relevant answers to questions. The project uses StackExchange data to train models on real-world question-answer pairs with upvote-based relevance scores.

## Features

- Flexible dataset with configurable batch transformations:
  - InfoNCE with in-batch negatives
  - Multiple positives per question
  - Hard negative sampling
  - Triplet format
  - Listwise ranking format
- Comprehensive loss function implementations:
  - Standard InfoNCE (with temperature scaling)
  - Rank-weighted InfoNCE
  - Hard negative InfoNCE
  - Multiple positives loss
  - Triplet loss
  - Listwise ranking loss
- Standardized evaluation metrics:
  - MRR (Mean Reciprocal Rank)
  - NDCG@k (Normalized Discounted Cumulative Gain)
  - MAP@k (Mean Average Precision)
  - Hard negative accuracy
- Modular architecture for easy extension and experimentation
- Integration with Weights & Biases for experiment tracking
- Hardware acceleration detection (MPS, CUDA, CPU)

## Getting Started

### Prerequisites

- Python 3.8+
- [uv](https://github.com/astral-sh/uv) (Python package management)
- Kaggle API credentials (for dataset download)

### Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/yourusername/qa-embeddings-loss-comparison.git
cd qa-embeddings-loss-comparison
uv pip install -e .
```

### Running Experiments

To train a model with a predefined configuration:

```bash
uv run rank-test --config configs/test.json
```

To create a custom configuration, you can copy and modify one of the existing configuration files in `configs/`.

## üõ∞Ô∏è  How a Sample Moves Through the System

```mermaid
graph LR
    subgraph Data‚ÄêPreparation
        A[Raw JSON QA<br/>(+ scores)] --> B[Batch Transform<br/>(e.g. infonce)]
    end
    B --> C[PyTorch DataLoader] --> D[Encoder (Model)] --> E[Loss Function] --> F[Metrics & Logging]
```

Shape convention: `N` = questions in the batch  
`M_q` = positives for question `q`  
`d` = embedding dimension (default 128)

| Transform | Returned keys (main) | Shapes |
|-----------|----------------------|--------|
| `infonce` | `q_input_ids`, `a_input_ids` | `(N, L)`, `(N, L)` |
| `multiple_positives` | `q_input_ids`, `a_input_ids`, `ranks`, `scores` | `(~N¬∑M, L)` |
| `hard_negative` | *list* of dicts, each with `answers` list | variable |
| `triplet` | `q_input_ids`, `a_pos_input_ids`, `a_neg_input_ids` | `(N, L)` each |
| `listwise` | *list* of dicts (`a_input_ids` is `(‚â§max_answers, L)`) | variable |
| `standardized_test` | fixed evaluation format (see `transforms.standardized_test_transform`) | variable |

(L = token sequence length after padding/truncation.)

## üîç  Detailed Sampling Strategies

<details>
<summary><strong>1. InfoNCE (`infonce_batch_transform`)</strong></summary>

**Intuition** ‚Äì One positive per question, all other answers in the same mini-batch act as negatives (classic SimCLR / CLIP style).

```python
from rank_test.flexible_dataset import FlexibleQADataset, infonce_batch_transform
dataset = FlexibleQADataset(
    "data/ranked_qa.json",
    batch_transform_fn=infonce_batch_transform,
    batch_size=8,           # provides 7 in-batch negatives per sample
    take_top=True
)
batch = dataset[0]
print(batch.keys())
# dict_keys([... 'q_input_ids', 'a_input_ids', 'ranks', 'scores'])
```

Pros üëâ fastest, no extra memory.  
Cons üëâ treats other good answers from the *same* question as negatives (can be harmful).

Best paired with: `StandardInfoNCELoss`, `RankInfoNCELoss`.
</details>

<details>
<summary><strong>2. Multiple Positives (`multiple_positives_batch_transform`)</strong></summary>

Creates **M duplicates** of each question ‚Äì one for each high-scoring answer.

```
Q           A‚Å∫‚ÇÅ
‚îî‚îÄ‚îÄcopy‚îÄ‚îÄ‚ñ∫  A‚Å∫‚ÇÇ
            ‚Ä¶
```

Returned batch length ‚âà `Œ£_q M_q`.

Useful when you have several "correct" answers and want the model to learn *relative* quality (rank/score).  
Pairs naturally with `RankInfoNCELoss` "rank" or "score" flavours and `MultiplePositivesLoss`.
</details>

<details>
<summary><strong>3. Hard Negative (`hard_negative_batch_transform`)</strong></summary>

Each item contains one question and **all** of its answers (top answer + hard negatives).  
The loss can explicitly penalise high similarity between the question and the *wrong* answers of the same question.

```python
item = batch_items[0]
item.keys()          # ['q_input_ids', 'answers', 'question_id', ...]
len(item['answers']) # e.g. 5
```

Combine with `HardNegativeInfoNCELoss`.
</details>

<details>
<summary><strong>4. Triplet (`triplet_batch_transform`)</strong></summary>

Outputs three tensors per sample (query, positive, negative).  
Negative can be "hard" (same question) or "in-batch" (different question) depending on `neg_strategy`.

Pairs with `TripletLoss (margin)`.
</details>

<details>
<summary><strong>5. Listwise (`listwise_batch_transform`)</strong></summary>

Delivers **a list of answers per query** (‚â§ `max_answers`) plus their normalised scores ‚Üí enables true listwise ranking losses.

Pairs with `ListwiseRankingLoss`.
</details>

<details>
<summary><strong>6. Standardised Test (`standardized_test_transform`)</strong></summary>

Evaluation-only transform that guarantees every question has:
* 1 positive (top answer)  
* up to 5 hard negatives (lower-ranked answers)  
* normal negatives (top answers from other questions)

Used automatically by `evaluate.py`.
</details>

## üî¢  Detailed Loss Functions

| Loss | Formula (sketch) | Typical Inputs | Notes |
|------|------------------|----------------|-------|
| **StandardInfoNCELoss** | \(L=-\frac12\bigl[\log\frac{e^{s_{qq^+}/\tau}}{\sum_a e^{s_{qa}/\tau}}+\log\frac{e^{s_{aa^+}/\tau}}{\sum_q e^{s_{aq}/\tau}}\bigr]\) | `(N,d)` √ó `(N,d)` | Symmetric InfoNCE. |
| **RankInfoNCELoss** | Same as above but multiplies individual similarities by rank / score weights. | needs `question_ids`, optionally `ranks`/`scores`. | `use_ranks` or `use_scores`. |
| **HardNegativeInfoNCELoss** | \(L=L_{InfoNCE}+ \lambda\; \frac1{|H|}\sum_{(q,a^-)} e^{s_{qa^-}/\tau}\) | same as standard + `question_ids` | Penalises hard-neg pairs. |
| **MultiplePositivesLoss** | Normalised log-softmax where *all* answers of a question are positives. | batch from `multiple_positives` | Optional rank weighting. |
| **TripletLoss** | \(\max(0,\; s_{q,a^-}-s_{q,a^+}+m)\) | `(N,d)` triplets | Margin `m` default 0.3. |
| **ListwiseRankingLoss** | KL-divergence between softmax(sim) and softmax(scores) | listwise batch |  Temperature controls softmax sharpness. |

Example:

```python
loss_fn = RankInfoNCELoss(temperature=0.07, use_scores=True, score_weight=0.05)
loss, metrics = loss_fn(
    q_embeddings, a_embeddings,
    question_ids=batch['question_ids'],
    scores=batch['scores']
)
```

## ‚ö° Quick-start Compatibility Matrix

| Transform ‚Üì / Loss ‚Üí | InfoNCE | Rank-InfoNCE | Hard-Neg-InfoNCE | Mult-Pos | Triplet | Listwise |
|----------------------|:-------:|:------------:|:----------------:|:--------:|:-------:|:--------:|
| `infonce` | ‚úî | ‚úî | ‚úñ | ‚úñ | ‚úñ | ‚úñ |
| `multiple_positives` | ‚úî | ‚úî | ‚úñ | ‚úî | ‚úñ | ‚úñ |
| `hard_negative` | ‚úñ | ‚úñ | ‚úî | ‚úñ | ‚úñ | ‚úñ |
| `triplet` | ‚úñ | ‚úñ | ‚úñ | ‚úñ | ‚úî | ‚úñ |
| `listwise` | ‚úñ | ‚úñ | ‚úñ | ‚úñ | ‚úñ | ‚úî |
| `standardized_test` | eval-only | eval-only | eval-only | eval-only | eval-only | eval-only |

## üß≠ Choosing the Right Combo

```mermaid
flowchart TD
    A[Do you have >1 good answer per question?] -->|yes| B[multiple_positives]
    A -->|no| C[Need explicit hard negatives?]
    C -->|yes| D[hard_negative + HardNegInfoNCE]
    C -->|no| E[Small batches? use Triplet] --> F[triplet + TripletLoss]
    D --> G[Otherwise] --> H[infonce + StandardInfoNCE]
```

## üé¨  Minimal End-to-End Example

```python
uv add torch transformers
uv run examples/flexible_dataset_demo.py --data-path data/sample_qa.json
```

Outputs (truncated):

```
InfoNCE Dataset and Loss
Created InfoNCE dataset with 1 batches
Loss: 2.1037
Metrics: {'loss': 2.10, 'q2a_acc': 0.25, ...}
```

Feel free to explore different flags (`--help`) to switch strategies and losses.

## Batch Transformation Strategies

The system uses a flexible data processing approach with various batch transformation strategies:

| Strategy | Description | Best Used With |
|----------|-------------|----------------|
| `infonce` | Standard InfoNCE format with each question paired with a top answer | StandardInfoNCELoss, RankInfoNCELoss |
| `multiple_positives` | Each question appears multiple times with different positive answers | MultiplePositivesLoss |
| `hard_negative` | Explicitly includes lower-ranked answers as hard negatives | HardNegativeInfoNCELoss |
| `triplet` | Creates triplets of (query, positive answer, negative answer) | TripletLoss |
| `listwise` | Prepares multiple ranked answers per question for listwise ranking | ListwiseRankingLoss |
| `standardized_test` | Standard evaluation format for fair comparison | Used automatically for evaluation |

Each transformation prepares data in a specific format suitable for its corresponding loss function. The transforms can be configured with additional parameters:

- `take_top`: Whether to use the highest-ranked answer (True) or a random answer (False)
- `pos_count`: Maximum number of positive answers to include per question
- `neg_strategy`: Strategy for selecting negatives in triplet format ("hard_negative", "in_batch", "mixed")
- `max_answers`: Maximum number of answers to include per question in listwise format

## Loss Functions

### StandardInfoNCELoss

Standard InfoNCE contrastive loss that contrasts positive pairs against in-batch negatives.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)

### RankInfoNCELoss

InfoNCE loss that leverages rank or score information from the dataset.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)
- `use_ranks`: Whether to use ordinal rank information (default: True)
- `use_scores`: Whether to use cardinal score information (default: False)
- `rank_weight`: Weight for rank-based penalties (default: 0.1)
- `score_weight`: Weight for score-based adjustments (default: 0.01)

### HardNegativeInfoNCELoss

InfoNCE loss with additional penalty for hard negatives.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)
- `hard_negative_weight`: Weight for the hard negative component (default: 1.0)

### MultiplePositivesLoss

Loss function that handles multiple positive answers per question.

**Parameters:**
- `temperature`: Temperature parameter for scaling similarity scores (default: 0.1)
- `rank_weight`: Weight for rank-based penalties (default: 0.1)

### TripletLoss

Triplet loss for query-positive-negative triplets.

**Parameters:**
- `margin`: Margin between positive and negative similarities (default: 0.3)

### ListwiseRankingLoss

Listwise ranking loss for learning to rank multiple answers.

**Parameters:**
- `temperature`: Temperature for scaling similarities (default: 1.0)

## Configuration Options

The system uses a strong typing configuration system based on Pydantic. Key configuration options include:

**Dataset and Data Processing:**
- `data_path`: Path to the JSON dataset with ranked QA pairs
- `limit`: Maximum number of questions to process
- `test_size`: Fraction of data to use for testing
- `seed`: Random seed for reproducibility
- `batch_size`: Batch size for training
- `dataset_strategy`: Dataset strategy to use (currently supports "flexible")
- `batch_transform`: Batch transformation strategy to use

**Model Architecture:**
- `embed_dim`: Embedding dimension for the base model
- `projection_dim`: Projection dimension for the final embeddings

**Training Parameters:**
- `epochs`: Number of training epochs
- `learning_rate`: Learning rate for optimization
- `eval_steps`: Number of steps between evaluations
- `eval_at_zero`: Whether to evaluate before training
- `checkpoint_interval`: Number of epochs between checkpoints

**Loss Function Parameters:**
- `loss_type`: Type of loss function to use
- `temperature`: Temperature parameter for scaling similarity scores
- `margin`: Margin for triplet loss
- `use_ranks`: Whether to use rank information
- `use_scores`: Whether to use score information
- `rank_weight`: Weight for rank-based adjustments
- `score_weight`: Weight for score-based adjustments
- `hard_negative_weight`: Weight for hard negative penalty

**Output and Logging:**
- `output_dir`: Directory to save outputs
- `log_to_wandb`: Whether to log to Weights & Biases
- `wandb_project`: Weights & Biases project name

Example configuration:
```json
{
  "name": "Test",
  "data_path": "data/ranked_qa.json",
  "limit": 100,
  "test_size": 0.02,
  "seed": 42,
  "embed_dim": 768,
  "projection_dim": 128,
  "batch_size": 16,
  "epochs": 1,
  "learning_rate": 2e-5,
  "eval_steps": 50,
  "eval_at_zero": true,
  "debug": false,
  "output_dir": "models/flexible",
  "log_to_wandb": false,
  "dataset_strategy": "flexible",
  "batch_transform": "infonce",
  "pos_count": 3,
  "loss_type": "infonce",
  "temperature": 0.1,
  "use_ranks": true,
  "use_scores": false,
  "rank_weight": 0.1
}
```

## Evaluation Metrics

The system provides comprehensive evaluation metrics:

- **MRR**: Mean Reciprocal Rank - measures how highly the first relevant document is ranked
- **Accuracy@k**: Percentage of queries where a relevant document is ranked in the top k
- **NDCG@k**: Normalized Discounted Cumulative Gain - measures ranking quality considering the graded relevance of documents
- **MAP@k**: Mean Average Precision - measures precision at different recall levels
- **Hard Negative Accuracy**: Measures the model's ability to distinguish between positive answers and hard negative answers (lower-ranked answers to the same question)

## Project Structure

- `src/rank_test/`
  - `dataset.py`: Dataset class and data loading utilities
  - `transforms.py`: Batch transformation strategies
  - `models.py`: Model architecture definitions
  - `losses.py`: Loss function implementations
  - `train.py`: Training loop and utilities
  - `evaluate.py`: Evaluation metrics and utilities
  - `config.py`: Experiment configuration
  - `run_experiment.py`: Experiment runner for comparing multiple losses

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---
./test_simplified_train.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Test script for the simplified training module.
"""

import torch
from src.rank_test.config import ExperimentConfig
from src.rank_test.train import train, create_unified_loss

# Configure a minimal test run
config = ExperimentConfig(
    name="Simple Test",
    data_path="data/ranked_qa.json",
    limit=50,  # Use only 50 items
    test_size=0.2,
    seed=42,
    
    embed_dim=768,
    projection_dim=128,
    
    batch_size=4,
    epochs=1,
    learning_rate=2e-5,
    
    eval_steps=10,
    eval_at_zero=True,
    debug=True,
    
    log_to_wandb=False,  # Disable wandb for test
    
    dataset_strategy="flexible",
    batch_transform="infonce",  # Test with simple InfoNCE
    loss_type="infonce",
    temperature=0.1
)

# Run training
if __name__ == "__main__":
    print("Testing simplified training module...")
    model = train(config)
    print("Test complete!")

---
./tests/unit/test_flexible_dataset.py
---


---
./tests/unit/test_flexible_losses.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Unit tests for flexible loss functions.
Tests each loss function with appropriate input, focusing on semantic behavior.
"""

import pytest
import torch
import numpy as np
from collections import defaultdict

# Assuming losses are in src/rank_test/losses.py relative to project root
# Adjust path if necessary
from rank_test.losses import (
    FlexibleInfoNCELoss,
    RankInfoNCELoss,
    RelaxedHardNegativeInfoNCELoss, # New Relaxed HN loss
    TripletLoss,
    ListwiseRankingLoss,
    create_loss # Updated factory function name
)

# Helper to create normalized embeddings
def create_normalized_embeddings(shape, seed=42):
    torch.manual_seed(seed)
    embeddings = torch.randn(shape)
    return embeddings / torch.norm(embeddings, dim=-1, keepdim=True)

@pytest.fixture
def mock_embeddings_basic():
    """Basic (Batch Size, Embed Dim) embeddings for simple tests"""
    q = create_normalized_embeddings((4, 64), seed=1)
    # Make diagonal entries similar (like standard InfoNCE setup)
    a = torch.zeros_like(q)
    a[0] = 0.9 * q[0] + 0.1 * create_normalized_embeddings((1, 64), seed=10)
    a[1] = 0.9 * q[1] + 0.1 * create_normalized_embeddings((1, 64), seed=11)
    a[2] = 0.9 * q[2] + 0.1 * create_normalized_embeddings((1, 64), seed=12)
    a[3] = 0.9 * q[3] + 0.1 * create_normalized_embeddings((1, 64), seed=13)
    a = a / torch.norm(a, dim=-1, keepdim=True)
    return q, a

@pytest.fixture
def mock_data_multi_positive():
    """Data with multiple positives per question for Flex/Rank/Relaxed tests"""
    # Q1: idx 0 (pos), idx 3 (pos/hn)
    # Q2: idx 1 (pos), idx 5 (pos/hn)
    # Q3: idx 2 (pos), idx 7 (pos/hn)
    # Q4: idx 4 (pos)
    # Q5: idx 6 (pos)
    question_ids = ["q1", "q2", "q3", "q1", "q4", "q2", "q5", "q3"]
    ranks = torch.tensor([0, 0, 0, 1, 0, 1, 0, 1], dtype=torch.long) # Lower is better
    scores = torch.tensor([10, 9, 8, 5, 10, 4, 9, 3], dtype=torch.float) # Higher is better
    batch_size = len(question_ids)
    embed_dim = 64

    q_embeddings = create_normalized_embeddings((batch_size, embed_dim), seed=20)
    a_embeddings = torch.zeros_like(q_embeddings)

    # Group indices by question ID
    q_groups = defaultdict(list)
    for i, q_id in enumerate(question_ids):
        q_groups[q_id].append(i)

    # Create answers such that answers for the same question are somewhat similar
    # and answers for different questions are less similar.
    # Make rank 0 answers closer to the query than rank 1 answers.
    noise_level_pos = 0.1 # Noise for the best positive (rank 0)
    noise_level_hn = 0.3  # Noise for the hard negative (rank 1)
    noise_level_neg = 0.8 # Noise relative to unrelated Qs

    for q_id, indices in q_groups.items():
        anchor_q_idx = indices[0] # Use the first query occurrence as anchor for simplicity
        anchor_q_embed = q_embeddings[anchor_q_idx]

        for i in indices:
             rank = ranks[i].item()
             if rank == 0: # Primary positive
                 noise = create_normalized_embeddings((1, embed_dim), seed=30+i)
                 a_embeddings[i] = (1-noise_level_pos) * anchor_q_embed + noise_level_pos * noise
             else: # Hard negative / lower-ranked positive
                 noise = create_normalized_embeddings((1, embed_dim), seed=40+i)
                 a_embeddings[i] = (1-noise_level_hn) * anchor_q_embed + noise_level_hn * noise

    # Add noise relative to *other* questions to simulate negatives
    for i in range(batch_size):
        for j in range(batch_size):
             if question_ids[i] != question_ids[j]: # Unrelated question
                 noise = create_normalized_embeddings((1, embed_dim), seed=50+i*batch_size+j)
                 # Add a small component from an unrelated query's direction
                 # Fix the broadcasting by ensuring consistent shapes
                 noise_component = noise_level_neg * (q_embeddings[j].unsqueeze(0) * 0.1 + noise * 0.9)
                 a_embeddings[i] += noise_component.squeeze(0)

    a_embeddings = a_embeddings / torch.norm(a_embeddings, dim=-1, keepdim=True)

    return q_embeddings, a_embeddings, question_ids, ranks, scores


def test_flexible_infonce_loss_single_positive(mock_embeddings_basic):
    """Test FlexibleInfoNCE behaving like standard InfoNCE (one positive per Q)."""
    q_embeddings, a_embeddings = mock_embeddings_basic
    # Each item has a unique question ID
    question_ids = [f"q{i}" for i in range(q_embeddings.shape[0])]
    
    loss_fn = FlexibleInfoNCELoss(temperature=0.1)
    loss, metrics = loss_fn(q_embeddings, a_embeddings, question_ids=question_ids)
    
    assert isinstance(loss, torch.Tensor) and loss.dim() == 0
    assert loss.item() > 0
    assert 'loss' in metrics and 'acc' in metrics and 'groups' in metrics
    assert metrics['groups'] == q_embeddings.shape[0] # Each qid is its own group
    # Accuracy should be high because diagonal elements were made similar
    assert metrics['acc'] > 0.5


def test_flexible_infonce_loss_multi_positive(mock_data_multi_positive):
    """Test FlexibleInfoNCE with multiple positives per question."""
    q_embeddings, a_embeddings, question_ids, _, _ = mock_data_multi_positive
    
    loss_fn = FlexibleInfoNCELoss(temperature=0.1)
    loss, metrics = loss_fn(q_embeddings, a_embeddings, question_ids=question_ids)
    
    assert isinstance(loss, torch.Tensor) and loss.dim() == 0
    assert loss.item() > 0
    assert 'loss' in metrics and 'acc' in metrics and 'groups' in metrics
    
    # Count unique question IDs
    unique_qids = len(set(question_ids))
    assert metrics['groups'] == unique_qids
    
    # Accuracy check: Since rank 0 answers were made closer, expect decent accuracy
    # where accuracy means predicting *any* answer from the same group.
    assert metrics['acc'] > 0 # Should get some right


def test_rank_infonce_loss_weights(mock_data_multi_positive):
    """Test RankInfoNCE applies weighting based on ranks/scores."""
    q_embeddings, a_embeddings, question_ids, ranks, scores = mock_data_multi_positive
    
    # --- Test with Rank Weighting ---
    loss_fn_rank = RankInfoNCELoss(temperature=0.1, use_ranks=True, use_scores=False, rank_weight=0.5)
    loss_rank, metrics_rank = loss_fn_rank(q_embeddings, a_embeddings, question_ids=question_ids, ranks=ranks)

    # --- Test with Score Weighting ---
    loss_fn_score = RankInfoNCELoss(temperature=0.1, use_ranks=False, use_scores=True, score_weight=0.1)
    loss_score, metrics_score = loss_fn_score(q_embeddings, a_embeddings, question_ids=question_ids, scores=scores)

    # --- Test without Weighting (using FlexibleInfoNCE for comparison) ---
    loss_fn_base = FlexibleInfoNCELoss(temperature=0.1)
    loss_base, metrics_base = loss_fn_base(q_embeddings, a_embeddings, question_ids=question_ids)

    # --- Assertions ---
    assert isinstance(loss_rank, torch.Tensor) and loss_rank.dim() == 0
    assert isinstance(loss_score, torch.Tensor) and loss_score.dim() == 0
    assert loss_rank.item() > 0
    assert loss_score.item() > 0
    
    # Semantic Check: Weighted losses should differ from the base flexible loss
    # because ranks/scores vary within groups. The exact difference depends on data,
    # but they shouldn't be identical unless all ranks/scores within groups are the same.
    assert not np.isclose(loss_rank.item(), loss_base.item()), "Rank weighting should change loss"
    assert not np.isclose(loss_score.item(), loss_base.item()), "Score weighting should change loss"
    
    # Check metrics structure
    assert 'acc' in metrics_rank and 'groups' in metrics_rank
    assert 'acc' in metrics_score and 'groups' in metrics_score
    # Accuracy definition is the same as FlexibleInfoNCE (predicting any item in group)
    assert np.isclose(metrics_rank['acc'], metrics_base['acc'])
    assert np.isclose(metrics_score['acc'], metrics_base['acc'])


def test_relaxed_hard_negative_loss():
    """Test RelaxedHardNegative loss penalizes HN less than FlexibleInfoNCE."""
    # Create specialized test data that will highlight the difference
    # between the two accuracy definitions
    batch_size = 6
    embed_dim = 64
    
    # Setup: 3 questions, 2 answers each
    # q1: idx 0,1
    # q2: idx 2,3
    # q3: idx 4,5
    question_ids = ["q1", "q1", "q2", "q2", "q3", "q3"]
    
    # Create embeddings with specific similarity patterns
    q_embeddings = torch.zeros((batch_size, embed_dim))
    a_embeddings = torch.zeros((batch_size, embed_dim))
    
    # Initialize with random normalized vectors
    torch.manual_seed(42)
    for i in range(batch_size):
        q_embeddings[i] = torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
        a_embeddings[i] = torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Now create a specific pattern where:
    # - For indices 0,2,4: highest similarity is with another answer from same question
    # - For indices 1,3,5: highest similarity is with itself
    
    # Make answer 0's highest similarity be with q1 (itself) but also high with a1
    a_embeddings[0] = 0.7 * q_embeddings[0] + 0.3 * torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Make answer 1's highest similarity be with q0 (not itself)
    a_embeddings[1] = 0.8 * q_embeddings[0] + 0.2 * torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Make answer 2's highest similarity be with q3 (not itself)
    a_embeddings[2] = 0.8 * q_embeddings[3] + 0.2 * torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Make answer 3's highest similarity be with q2 (itself)
    a_embeddings[3] = 0.7 * q_embeddings[3] + 0.3 * torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Make answer 4's highest similarity be with q5 (not itself)
    a_embeddings[4] = 0.8 * q_embeddings[5] + 0.2 * torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Make answer 5's highest similarity be with q4 (itself)
    a_embeddings[5] = 0.7 * q_embeddings[5] + 0.3 * torch.nn.functional.normalize(torch.randn(embed_dim), dim=0)
    
    # Normalize all embeddings
    q_embeddings = torch.nn.functional.normalize(q_embeddings, dim=1)
    a_embeddings = torch.nn.functional.normalize(a_embeddings, dim=1)
    
    # Set up the losses
    temp = 0.1
    hard_neg_prob = 0.2 # Assign 20% target probability to hard negatives
    
    # --- Relaxed Hard Negative Loss ---
    loss_fn_relaxed = RelaxedHardNegativeInfoNCELoss(temperature=temp, hard_neg_target_prob=hard_neg_prob)
    loss_relaxed, metrics_relaxed = loss_fn_relaxed(q_embeddings, a_embeddings, question_ids=question_ids)

    # --- Flexible InfoNCE Loss (for comparison) ---
    loss_fn_flex = FlexibleInfoNCELoss(temperature=temp)
    loss_flex, metrics_flex = loss_fn_flex(q_embeddings, a_embeddings, question_ids=question_ids)
    
    # --- Assertions ---
    assert isinstance(loss_relaxed, torch.Tensor) and loss_relaxed.dim() == 0
    assert loss_relaxed.item() >= 0 # KL divergence loss >= 0
    assert 'acc' in metrics_relaxed and 'groups' in metrics_relaxed
    
    # Semantic Check: Loss is lower for relaxed version because it doesn't
    # penalize hard negatives as strongly
    assert loss_relaxed.item() < loss_flex.item(), \
        "RelaxedHN loss should be lower than FlexibleInfoNCE when hard negatives are somewhat similar"

    # Check accuracy definitions are different:
    # - FlexibleInfoNCE counts correct if highest similarity is with ANY item in same group
    # - RelaxedHardNegative counts correct only if highest similarity is with ITSELF
    # With our constructed test data, these should give different results
    assert 'acc' in metrics_relaxed and 'acc' in metrics_flex
    assert 0 <= metrics_relaxed['acc'] <= 1 and 0 <= metrics_flex['acc'] <= 1
    
    # FlexibleInfoNCE should have higher accuracy as its definition is more lenient
    assert metrics_flex['acc'] > metrics_relaxed['acc'], \
        f"FlexibleInfoNCE acc ({metrics_flex['acc']}) should be higher than RelaxedHN acc ({metrics_relaxed['acc']})"


# --- Tests for Triplet and Listwise Losses (mostly unchanged, maybe slightly more robust) ---

@pytest.fixture
def mock_triplet_data():
    """Create mock data for triplet loss with clearer separation"""
    batch_size = 10
    embed_dim = 64
    q = create_normalized_embeddings((batch_size, embed_dim), seed=60)
    a_pos = torch.zeros_like(q)
    a_neg = torch.zeros_like(q)

    # Make positives clearly similar, negatives clearly dissimilar
    for i in range(batch_size):
        noise_pos = create_normalized_embeddings((1, embed_dim), seed=70+i)
        noise_neg = create_normalized_embeddings((1, embed_dim), seed=80+i)
        # Ensure positive is aligned, negative is less aligned
        a_pos[i] = 0.95 * q[i] + 0.05 * noise_pos
        a_neg[i] = 0.1 * q[i] + 0.9 * noise_neg # Make negative mostly noise

    a_pos = a_pos / torch.norm(a_pos, dim=-1, keepdim=True)
    a_neg = a_neg / torch.norm(a_neg, dim=-1, keepdim=True)

    return q, a_pos, a_neg

def test_triplet_loss(mock_triplet_data):
    """Test triplet loss ensures margin"""
    q_embeddings, a_pos_embeddings, a_neg_embeddings = mock_triplet_data
    margin = 0.3
    loss_fn = TripletLoss(margin=margin)
    loss, metrics = loss_fn(q_embeddings, a_pos_embeddings, a_neg_embeddings)

    assert isinstance(loss, torch.Tensor) and loss.dim() == 0
    assert loss.item() >= 0
    assert 'loss' in metrics and 'acc' in metrics and 'avg_pos_sim' in metrics
    assert 'avg_neg_sim' in metrics and 'margin_violations' in metrics
    
    # Semantic checks
    assert metrics['acc'] > 0.8, "Accuracy should be high with clear separation"
    assert metrics['avg_pos_sim'] > metrics['avg_neg_sim'] + margin * 0.5, \
        "Avg pos sim should be significantly > avg neg sim"
    # Some violations might occur if noise pushes neg closer occasionally
    assert 0 <= metrics['margin_violations'] <= 1


@pytest.fixture
def mock_listwise_data():
    """Create mock data for listwise ranking loss with varying list lengths"""
    q_embeddings = []
    a_list_embeddings = []
    a_list_scores = []
    embed_dim = 64

    # Question 1: 3 answers
    q1 = create_normalized_embeddings((1, embed_dim), seed=90)
    a1 = create_normalized_embeddings((3, embed_dim), seed=91)
    s1 = torch.tensor([5.0, 2.0, 0.0]) # Clear score separation
    q_embeddings.append(q1.squeeze(0)) # Store as (embed_dim,)
    a_list_embeddings.append(a1)
    a_list_scores.append(s1)

    # Question 2: 5 answers
    q2 = create_normalized_embeddings((1, embed_dim), seed=100)
    a2 = create_normalized_embeddings((5, embed_dim), seed=101)
    s2 = torch.tensor([10.0, 8.0, 8.0, 3.0, 1.0]) # Include ties
    q_embeddings.append(q2.squeeze(0))
    a_list_embeddings.append(a2)
    a_list_scores.append(s2)
    
    # Question 3: 2 answers
    q3 = create_normalized_embeddings((1, embed_dim), seed=110)
    a3 = create_normalized_embeddings((2, embed_dim), seed=111)
    s3 = torch.tensor([7.0, 6.0]) 
    q_embeddings.append(q3.squeeze(0))
    a_list_embeddings.append(a3)
    a_list_scores.append(s3)

    # Structure expected by loss: list of tensors for Q, list of lists for A/Scores
    return q_embeddings, a_list_embeddings, a_list_scores


def test_listwise_ranking_loss(mock_listwise_data):
    """Test listwise ranking loss computes KL divergence and NDCG."""
    q_embeddings, a_list_embeddings, a_list_scores = mock_listwise_data
    loss_fn = ListwiseRankingLoss(temperature=1.0) # Use temp=1 for direct score comparison
    
    # --- Calculate Loss ---
    # Note: Listwise loss expects a list of Q embeddings if A/Scores are lists
    loss, metrics = loss_fn(q_embeddings, a_list_embeddings, a_list_scores)

    # --- Assertions ---
    assert isinstance(loss, torch.Tensor) and loss.dim() == 0
    assert loss.item() >= 0 # KL divergence >= 0
    assert 'loss' in metrics and 'ndcg' in metrics
    
    # Semantic checks
    # Since embeddings are random, NDCG will likely be low, but should be between 0 and 1
    assert 0 <= metrics['ndcg'] <= 1, "NDCG must be in [0, 1]"
    # We could craft embeddings to test NDCG=1, but random check is simpler


# --- Test Factory Function ---

def test_create_loss_factory():
    """Test the updated loss factory function."""
    # Test creating new base/derived losses
    loss_flex = create_loss('infonce', temperature=0.5)
    assert isinstance(loss_flex, FlexibleInfoNCELoss)
    assert loss_flex.temperature == 0.5

    loss_rank = create_loss('rank_infonce', use_ranks=True, rank_weight=0.2)
    assert isinstance(loss_rank, RankInfoNCELoss)
    assert loss_rank.use_ranks is True and loss_rank.rank_weight == 0.2

    loss_relaxed = create_loss('relaxed_hard_neg', hard_neg_target_prob=0.15)
    assert isinstance(loss_relaxed, RelaxedHardNegativeInfoNCELoss)
    assert loss_relaxed.hard_neg_target_prob == 0.15

    loss_triplet = create_loss('triplet', margin=0.5)
    assert isinstance(loss_triplet, TripletLoss)
    assert loss_triplet.margin == 0.5

    loss_listwise = create_loss('listwise', temperature=2.0)
    assert isinstance(loss_listwise, ListwiseRankingLoss)
    assert loss_listwise.temperature == 2.0

    # Test creating named variants
    loss_rank_named = create_loss('rank_infonce_rank0.1_score0.05_t0.2')
    assert isinstance(loss_rank_named, RankInfoNCELoss)
    assert loss_rank_named.use_ranks is True and loss_rank_named.rank_weight == 0.1
    assert loss_rank_named.use_scores is True and loss_rank_named.score_weight == 0.05
    assert loss_rank_named.temperature == 0.2

    loss_relaxed_named = create_loss('relaxed_hard_neg_p0.25_t0.05')
    assert isinstance(loss_relaxed_named, RelaxedHardNegativeInfoNCELoss)
    assert loss_relaxed_named.hard_neg_target_prob == 0.25
    assert loss_relaxed_named.temperature == 0.05
    
    # Test overriding named params with kwargs
    loss_override = create_loss('rank_infonce_rank0.1_t0.2', temperature=0.9, rank_weight=0.99)
    assert loss_override.temperature == 0.9
    assert loss_override.rank_weight == 0.99


    # Test invalid name
    with pytest.raises(ValueError):
        create_loss('nonexistent_loss_type')

---
./tests/integration/test_flexible_pipeline.py
---
# #!/usr/bin/env python
# # -*- coding: utf-8 -*-

# """
# Integration tests for the flexible dataset and loss function pipeline.
# Tests that the different components work well together.
# """

# import os
# import pytest
# import torch
# import json
# import tempfile
# from torch.utils.data import DataLoader

# from rank_test.flexible_dataset import (
#     FlexibleQADataset,
#     infonce_batch_transform,
#     multiple_positives_batch_transform,
#     hard_negative_batch_transform,
#     triplet_batch_transform,
#     listwise_batch_transform
# )

# from rank_test.flexible_losses import (
#     StandardInfoNCELoss,
#     RankInfoNCELoss,
#     HardNegativeInfoNCELoss,
#     MultiplePositivesLoss,
#     TripletLoss,
#     ListwiseRankingLoss
# )

# from rank_test.models import QAEmbeddingModel

# # Create a mock dataset for testing
# @pytest.fixture
# def mock_qa_data():
#     """Create a mock dataset for testing"""
#     data = [
#         {
#             "question": {
#                 "id": "q1",
#                 "title": "Question 1",
#                 "body": "What is the best way to test code?"
#             },
#             "answers": [
#                 {
#                     "id": "a1",
#                     "body": "Use pytest for testing",
#                     "score": 95  # Cardinal score (e.g., user rating or upvotes)
#                 },
#                 {
#                     "id": "a2",
#                     "body": "Use unittest for testing",
#                     "score": 82
#                 },
#                 {
#                     "id": "a3",
#                     "body": "Use pytest with coverage for testing",
#                     "score": 78
#                 },
#                 {
#                     "id": "a4",
#                     "body": "Use a combination of pytest and unittest",
#                     "score": 65
#                 },
#                 {
#                     "id": "a5",
#                     "body": "Manual testing works too",
#                     "score": 35
#                 }
#             ]
#         },
#         {
#             "question": {
#                 "id": "q2",
#                 "title": "Question 2",
#                 "body": "How to implement a dataset in PyTorch?"
#             },
#             "answers": [
#                 {
#                     "id": "a6",
#                     "body": "Inherit from torch.utils.data.Dataset",
#                     "score": 92
#                 },
#                 {
#                     "id": "a7",
#                     "body": "Override __getitem__ and __len__",
#                     "score": 88
#                 },
#                 {
#                     "id": "a8",
#                     "body": "Create a class with __getitem__, __len__, and collate_fn",
#                     "score": 75
#                 },
#                 {
#                     "id": "a9",
#                     "body": "Use DataLoader with your custom Dataset class",
#                     "score": 62
#                 }
#             ]
#         },
#         {
#             "question": {
#                 "id": "q3",
#                 "title": "Question 3",
#                 "body": "What is a loss function?"
#             },
#             "answers": [
#                 {
#                     "id": "a10",
#                     "body": "A function that measures model error",
#                     "score": 90
#                 },
#                 {
#                     "id": "a11",
#                     "body": "A metric that quantifies the difference between predicted and actual values",
#                     "score": 85
#                 },
#                 {
#                     "id": "a12",
#                     "body": "A mathematical function that evaluates how well a model performs",
#                     "score": 70
#                 }
#             ]
#         }
#     ]
    
#     # Create a temporary file with the data
#     with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as f:
#         json.dump(data, f)
#         temp_path = f.name
    
#     yield temp_path
    
#     # Cleanup
#     os.unlink(temp_path)

# @pytest.fixture
# def mock_tokenizer():
#     """Return a mock tokenizer for testing"""
#     # For testing, we can use a simple tokenizer
#     class MockTokenizer:
#         def __call__(self, text, max_length=10, padding=None, truncation=None, return_tensors=None):
#             # Just return a fixed tensor for testing
#             return {
#                 'input_ids': torch.ones(1, max_length, dtype=torch.long),
#                 'attention_mask': torch.ones(1, max_length, dtype=torch.long)
#             }
    
#     return MockTokenizer()

# @pytest.fixture
# def mock_model():
#     """Return a mock model for testing"""
#     model = QAEmbeddingModel(embed_dim=768, projection_dim=128)
    
#     # Replace forward method with a simple mock
#     def mock_forward(input_ids, attention_mask=None):
#         batch_size = input_ids.shape[0]
#         # Return normalized random embeddings
#         embeddings = torch.randn(batch_size, 128)
#         return embeddings / torch.norm(embeddings, dim=1, keepdim=True)
    
#     model.forward = mock_forward
#     return model


# def test_infonce_pipeline(mock_qa_data, mock_tokenizer, mock_model):
#     """Test InfoNCE pipeline with standard dataset"""
#     # Create dataset
#     dataset = FlexibleQADataset(
#         mock_qa_data,
#         batch_transform_fn=infonce_batch_transform,
#         batch_size=3,
#         tokenizer=mock_tokenizer,
#         max_length=10
#     )
    
#     # Create dataloader
#     dataloader = FlexibleQADataset.get_dataloader(dataset)
    
#     # Create loss function
#     loss_fn = StandardInfoNCELoss(temperature=0.1)
    
#     # Test full pipeline
#     for batch in dataloader:
#         # Get embeddings
#         q_embeddings = mock_model(batch['q_input_ids'])
#         a_embeddings = mock_model(batch['a_input_ids'])
        
#         # Calculate loss
#         loss, metrics = loss_fn(q_embeddings, a_embeddings)
        
#         # Basic validation
#         assert isinstance(loss, torch.Tensor)
#         assert loss.dim() == 0  # Scalar tensor
#         assert loss.item() > 0  # Loss should be positive
        
#         # Check metrics
#         assert 'loss' in metrics
#         assert 'q2a_acc' in metrics
#         assert 'a2q_acc' in metrics
        
#         # Only need to test one batch
#         break


# def test_rank_infonce_pipeline(mock_qa_data, mock_tokenizer, mock_model):
#     """Test rank-aware InfoNCE pipeline"""
#     # Create dataset with multiple positives
#     dataset = FlexibleQADataset(
#         mock_qa_data,
#         batch_transform_fn=multiple_positives_batch_transform,
#         batch_size=3,
#         tokenizer=mock_tokenizer,
#         max_length=10,
#         pos_count=2
#     )
    
#     # Create dataloader
#     dataloader = FlexibleQADataset.get_dataloader(dataset)
    
#     # Create loss function
#     loss_fn = RankInfoNCELoss(temperature=0.1, use_ranks=True, rank_weight=0.1)
    
#     # Test full pipeline
#     for batch in dataloader:
#         # Get embeddings
#         q_embeddings = mock_model(batch['q_input_ids'])
#         a_embeddings = mock_model(batch['a_input_ids'])
        
#         # Calculate loss
#         loss, metrics = loss_fn(
#             q_embeddings, 
#             a_embeddings,
#             question_ids=batch['question_ids'],
#             ranks=batch['ranks']
#         )
        
#         # Basic validation
#         assert isinstance(loss, torch.Tensor)
#         assert loss.dim() == 0  # Scalar tensor
#         assert loss.item() > 0  # Loss should be positive
        
#         # Only need to test one batch
#         break


# def test_hard_negative_pipeline(mock_qa_data, mock_tokenizer, mock_model):
#     """Test hard negative pipeline"""
#     # Create dataset
#     dataset = FlexibleQADataset(
#         mock_qa_data,
#         batch_transform_fn=hard_negative_batch_transform,
#         batch_size=3,
#         tokenizer=mock_tokenizer,
#         max_length=10
#     )
    
#     # Create dataloader
#     dataloader = FlexibleQADataset.get_dataloader(dataset)
    
#     # Create loss function
#     loss_fn = HardNegativeInfoNCELoss(temperature=0.1, hard_negative_weight=1.0)
    
#     # Test full pipeline
#     for batch_items in dataloader:
#         for item in batch_items:
#             # Process each question with its answers
#             q_embedding = mock_model(item['q_input_ids'].unsqueeze(0))
            
#             # Get embeddings for all answers
#             a_embeddings = []
#             a_ids = []
#             for answer in item['answers']:
#                 a_embedding = mock_model(answer['input_ids'].unsqueeze(0))
#                 a_embeddings.append(a_embedding)
#                 a_ids.append(answer['id'])
            
#             a_embeddings = torch.cat(a_embeddings, dim=0)
            
#             # Create question_ids list (all same ID for this question's answers)
#             question_ids = [item['question_id']] * len(a_ids)
            
#             # Calculate loss
#             loss, metrics = loss_fn(
#                 q_embedding.repeat(len(a_ids), 1),  # Repeat question embedding
#                 a_embeddings,
#                 question_ids=question_ids
#             )
            
#             # Basic validation
#             assert isinstance(loss, torch.Tensor)
#             assert loss.dim() == 0  # Scalar tensor
#             assert loss.item() > 0  # Loss should be positive
            
#             # Check metrics
#             assert 'loss' in metrics
#             assert 'hard_neg_count' in metrics
            
#             # Only need to test one item
#             break
#         break


# def test_triplet_pipeline(mock_qa_data, mock_tokenizer, mock_model):
#     """Test triplet loss pipeline"""
#     # Create dataset
#     dataset = FlexibleQADataset(
#         mock_qa_data,
#         batch_transform_fn=triplet_batch_transform,
#         batch_size=3,
#         tokenizer=mock_tokenizer,
#         max_length=10,
#         neg_strategy="hard_negative"
#     )
    
#     # Create dataloader
#     dataloader = FlexibleQADataset.get_dataloader(dataset)
    
#     # Create loss function
#     loss_fn = TripletLoss(margin=0.3)
    
#     # Test full pipeline
#     for batch in dataloader:
#         # Get embeddings
#         q_embeddings = mock_model(batch['q_input_ids'])
#         a_pos_embeddings = mock_model(batch['a_pos_input_ids'])
#         a_neg_embeddings = mock_model(batch['a_neg_input_ids'])
        
#         # Calculate loss
#         loss, metrics = loss_fn(q_embeddings, a_pos_embeddings, a_neg_embeddings)
        
#         # Basic validation
#         assert isinstance(loss, torch.Tensor)
#         assert loss.dim() == 0  # Scalar tensor
#         assert loss.item() >= 0  # Loss should be non-negative
        
#         # Check metrics
#         assert 'loss' in metrics
#         assert 'acc' in metrics
#         assert 'avg_pos_sim' in metrics
#         assert 'avg_neg_sim' in metrics
        
#         # Only need to test one batch
#         break


# def test_listwise_pipeline(mock_qa_data, mock_tokenizer, mock_model):
#     """Test listwise ranking pipeline"""
#     # Create dataset
#     dataset = FlexibleQADataset(
#         mock_qa_data,
#         batch_transform_fn=listwise_batch_transform,
#         batch_size=3,
#         tokenizer=mock_tokenizer,
#         max_length=10,
#         max_answers=3
#     )
    
#     # Create dataloader
#     dataloader = FlexibleQADataset.get_dataloader(dataset)
    
#     # Create loss function
#     loss_fn = ListwiseRankingLoss(temperature=1.0)
    
#     # Test full pipeline
#     for batch_items in dataloader:
#         # Prepare inputs for listwise loss
#         q_embeddings = []
#         a_list_embeddings = []
#         a_list_scores = []
        
#         for item in batch_items:
#             # Get question embedding
#             q_embedding = mock_model(item['q_input_ids'].unsqueeze(0))
            
#             # Get embeddings for all answers
#             a_embeddings = mock_model(item['a_input_ids'])
            
#             q_embeddings.append(q_embedding)
#             a_list_embeddings.append(a_embeddings)
#             a_list_scores.append(item['scores'])
        
#         # Calculate loss
#         loss, metrics = loss_fn(q_embeddings, a_list_embeddings, a_list_scores)
        
#         # Basic validation
#         assert isinstance(loss, torch.Tensor)
#         assert loss.dim() == 0  # Scalar tensor
#         assert loss.item() > 0  # Loss should be positive
        
#         # Check metrics
#         assert 'loss' in metrics
#         assert 'ndcg' in metrics
#         assert 0 <= metrics['ndcg'] <= 1
        
#         # Only need to test one batch
#         break


# def test_pipeline_combinations(mock_qa_data, mock_tokenizer, mock_model):
#     """Test different combinations of dataset transformations and loss functions"""
#     # Define combinations to test
#     combinations = [
#         (infonce_batch_transform, StandardInfoNCELoss(temperature=0.1)),
#         (multiple_positives_batch_transform, RankInfoNCELoss(temperature=0.1, use_ranks=True)),
#         (hard_negative_batch_transform, HardNegativeInfoNCELoss(temperature=0.1)),
#         (triplet_batch_transform, TripletLoss(margin=0.3)),
#         (listwise_batch_transform, ListwiseRankingLoss(temperature=1.0))
#     ]
    
#     for transform_fn, loss_fn in combinations:
#         # Create dataset
#         dataset = FlexibleQADataset(
#             mock_qa_data,
#             batch_transform_fn=transform_fn,
#             batch_size=3,
#             tokenizer=mock_tokenizer,
#             max_length=10
#         )
        
#         # Verify dataset was created
#         assert len(dataset) > 0

---
./src/rank_test/__init__.py
---


---
./src/rank_test/config.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Configuration module using Pydantic for strongly typed configuration.
Provides a simple, flat structure for experiment configuration.
"""

from typing import Dict, Any, Optional, Union, Literal
from pathlib import Path
import json
from pydantic import BaseModel, Field
from argdantic.sources import from_file, JsonFileLoader
# Type definitions using literals instead of enums
DatasetStrategyType = Literal["standard", "flexible"]
BatchTransformType = Literal["infonce", "multiple_positives", "hard_negative", "triplet", "listwise"]
NegativeStrategyType = Literal["hard_negative", "in_batch", "mixed"]
LossType = Literal["infonce", "rank_infonce", "relaxed_hard_neg", "triplet", "listwise", "mse"]

@from_file(loader=JsonFileLoader)
class ExperimentConfig(BaseModel):
    """Complete experiment configuration with flat structure"""
    # Experiment metadata
    name: Optional[str] = Field(default=None, description="Name of the experiment")
    
    # Data settings
    data_path: str = Field(default="data/ranked_qa.json", description="Path to the dataset JSON file")
    limit: Optional[int] = Field(default=None, description="Limit the number of samples to process")
    test_size: float = Field(default=0.2, description="Proportion of data to use for testing")
    seed: int = Field(default=42, description="Random seed for reproducibility")
    force_regenerate: bool = Field(default=False, description="Force dataset regeneration")
    
    # Model settings
    embed_dim: int = Field(default=768, description="Embedding dimension from BERT")
    projection_dim: int = Field(default=128, description="Dimension for embeddings projection")
    
    # Training settings
    batch_size: int = Field(default=64, description="Training batch size")
    epochs: int = Field(default=5, description="Number of training epochs")
    learning_rate: float = Field(default=2e-5, description="Learning rate")
    checkpoint_interval: int = Field(default=1, description="Save checkpoint every N epochs")
    eval_steps: Optional[int] = Field(default=None, description="Evaluate every N steps")
    eval_at_zero: bool = Field(default=False, description="Evaluate before training")
    debug: bool = Field(default=False, description="Debug mode with minimal data")
    
    # Output settings
    output_dir: str = Field(default="models", description="Directory to save models and results")
    log_to_wandb: bool = Field(default=True, description="Whether to log metrics to W&B")
    wandb_project: str = Field(default="qa-embeddings-comparison", description="W&B project name")
    use_fixed_evaluation: bool = Field(default=True, description="Use the fixed evaluation implementation")
    
    # Dataset strategy settings
    dataset_strategy: DatasetStrategyType = Field(default="standard", description="Dataset strategy to use")
    batch_transform: BatchTransformType = Field(default="infonce", description="Batch transformation strategy")
    
    # InfoNCE transform options
    take_top: bool = Field(default=True, description="For InfoNCE: use top-ranked answer (True) or random (False)")
    
    # Multiple positives options
    pos_count: int = Field(default=3, description="For multiple_positives: number of positives per question")
    
    # Triplet options
    neg_strategy: NegativeStrategyType = Field(default="hard_negative", description="For triplet: negative sampling strategy")
    
    # Listwise options
    max_answers: int = Field(default=5, description="For listwise: maximum answers per question")
    
    # Loss settings
    loss_type: LossType = Field(default="infonce", description="Loss function to use")
    temperature: float = Field(default=0.1, description="Temperature for InfoNCE and listwise losses")
    margin: float = Field(default=0.2, description="Margin for triplet loss")
    normalize: bool = Field(default=True, description="For MSE: normalize scores")
    
    # Loss weighting options
    use_ranks: bool = Field(default=True, description="Use ordinal ranks in loss calculation")
    use_scores: bool = Field(default=False, description="Use cardinal scores in loss calculation")
    rank_weight: float = Field(default=0.1, description="Weight for rank-based penalties")
    score_weight: float = Field(default=0.05, description="Weight for score-based adjustments")
    hard_negative_weight: float = Field(default=1.0, description="Weight for hard negative penalty")
    
    def auto_select_loss_type(cls, values):
        """Automatically select loss type based on batch transform if not explicitly set"""
        if 'loss_type' not in values or not values['loss_type']:
            batch_transform = values.get('batch_transform')
            if batch_transform:
                if batch_transform == "infonce":
                    values['loss_type'] = "infonce"
                elif batch_transform == "multiple_positives":
                    values['loss_type'] = "rank_infonce"
                elif batch_transform == "hard_negative":
                    values['loss_type'] = "relaxed_hard_neg"
                elif batch_transform == "triplet":
                    values['loss_type'] = "triplet"
                elif batch_transform == "listwise":
                    values['loss_type'] = "listwise"
        return values
    
    def validate_debug_settings(cls, values):
        """Reduce settings in debug mode"""
        if values.get('debug', False):
            # Reduce batch size and epochs in debug mode
            values['batch_size'] = min(values.get('batch_size', 16), 4)
            values['epochs'] = min(values.get('epochs', 5), 2)
            
            # Limit data
            if values.get('limit') is None or values.get('limit', 0) > 50:
                values['limit'] = 50
                
        return values
    
    def get_loss_kwargs(self) -> Dict[str, Any]:
        """Get keyword arguments for the loss function"""
        kwargs = {}
        
        loss_type = self.loss_type
        
        # Add loss-specific parameters
        if loss_type in ["infonce", "rank_infonce", "relaxed_hard_neg", "listwise"]:
            kwargs['temperature'] = self.temperature
            
        if loss_type == "rank_infonce":
            kwargs['use_ranks'] = self.use_ranks
            kwargs['use_scores'] = self.use_scores
            kwargs['rank_weight'] = self.rank_weight
            kwargs['score_weight'] = self.score_weight
            
        elif loss_type == "relaxed_hard_neg":
            kwargs['hard_neg_target_prob'] = self.hard_negative_weight
            
        elif loss_type == "triplet":
            kwargs['margin'] = self.margin
            
        elif loss_type == "mse":
            kwargs['normalize'] = self.normalize
            
        return kwargs
    
    def get_batch_transform_kwargs(self) -> Dict[str, Any]:
        """Get keyword arguments for the batch transformation function"""
        kwargs = {}
        
        transform = self.batch_transform
        
        if transform == "infonce":
            kwargs['take_top'] = self.take_top
        elif transform == "multiple_positives":
            kwargs['pos_count'] = self.pos_count
        elif transform == "triplet":
            kwargs['neg_strategy'] = self.neg_strategy
        elif transform == "listwise":
            kwargs['max_answers'] = self.max_answers
            
        return kwargs
    
    def get_limit(self) -> Optional[int]:
        """Get effective data limit, accounting for debug mode"""
        if self.debug:
            return min(self.limit or 50, 50)
        return self.limit
    
    def get_batch_size(self) -> int:
        """Get effective batch size, accounting for debug mode"""
        if self.debug:
            return min(self.batch_size, 4)
        return self.batch_size
    
    def get_epochs(self) -> int:
        """Get effective number of epochs, accounting for debug mode"""
        if self.debug:
            return min(self.epochs, 2)
        return self.epochs
    
    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> 'ExperimentConfig':
        """Load configuration from a JSON file"""
        with open(file_path, 'r') as f:
            config_data = json.load(f)
        return cls.parse_obj(config_data)
    
    def save_to_file(self, file_path: Union[str, Path]) -> None:
        """Save configuration to a JSON file"""
        with open(file_path, 'w') as f:
            json.dump(self.dict(), f, indent=2)


# Predefined experiment configurations
PREDEFINED_CONFIGS = {
    # Standard InfoNCE with in-batch negatives
    'infonce_standard': ExperimentConfig(
        name="InfoNCE Standard",
        dataset_strategy="flexible",
        batch_transform="infonce",
        take_top=True,
        loss_type="infonce",
        temperature=0.1
    ),
    
    # InfoNCE with random answer selection
    'infonce_random': ExperimentConfig(
        name="InfoNCE Random",
        dataset_strategy="flexible",
        batch_transform="infonce",
        take_top=False,  # Choose random answer instead of top
        loss_type="infonce",
        temperature=0.1
    ),
    
    # Multiple positives with rank weighting
    'multiple_positives_rank': ExperimentConfig(
        name="Multiple Positives with Ranks",
        dataset_strategy="flexible",
        batch_transform="multiple_positives",
        pos_count=3,
        use_ranks=True,
        use_scores=False,
        rank_weight=0.1,
        loss_type="rank_infonce",
        temperature=0.1
    ),
    
    # Multiple positives with score weighting
    'multiple_positives_score': ExperimentConfig(
        name="Multiple Positives with Scores",
        dataset_strategy="flexible",
        batch_transform="multiple_positives",
        pos_count=3,
        use_ranks=False,
        use_scores=True,
        score_weight=0.05,
        loss_type="rank_infonce",
        temperature=0.1
    ),
    
    # Hard negative approach
    'hard_negative': ExperimentConfig(
        name="Relaxed Hard Negative InfoNCE",
        dataset_strategy="flexible",
        batch_transform="hard_negative",
        hard_negative_weight=0.1,  # Target probability for hard negatives
        loss_type="relaxed_hard_neg",
        temperature=0.1
    ),
    
    # Triplet loss with hard negatives
    'triplet_hard_neg': ExperimentConfig(
        name="Triplet with Hard Negatives",
        dataset_strategy="flexible",
        batch_transform="triplet",
        neg_strategy="hard_negative",
        loss_type="triplet",
        margin=0.2
    ),
    
    # Listwise ranking
    'listwise': ExperimentConfig(
        name="Listwise Ranking",
        dataset_strategy="flexible",
        batch_transform="listwise",
        max_answers=5,
        loss_type="listwise",
        temperature=1.0
    )
}

---
./src/rank_test/dataset.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Flexible dataset module that supports various sampling strategies
for QA ranking tasks. This module provides configurable approaches
for creating training data from ranked QA pairs.
"""

import json
import os
import random
import time
import csv
import sys
from collections import defaultdict
from typing import Callable
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm, trange
from transformers import DistilBertTokenizerFast


class QADataset(Dataset):
    """
    Flexible QA dataset that supports various batch transformation strategies.
    This dataset provides configurable approaches for creating training data
    from ranked QA pairs for different loss functions.
    """
    
    def __init__(
        self, 
        data_path: str, 
        batch_transform_fn: Callable = None, 
        batch_size: int = 16, 
        tokenizer = None,
        max_length: int = 128,
        shuffle: bool = True,
        **kwargs
    ):
        """
        Initialize the dataset with a specific batch transformation strategy.
        
        Args:
            data_path: Path to JSON dataset with ranked QA pairs
            batch_transform_fn: Function that transforms raw data to model-ready batches
            batch_size: Batch size for pre-processing
            tokenizer: Tokenizer to use (will create DistilBERT tokenizer if None)
            max_length: Maximum sequence length for tokenization
            shuffle: Whether to shuffle data during batch creation
            **kwargs: Additional parameters for the batch transform function
        """
        # Load raw data
        with open(data_path, 'r') as f:
            self.raw_data = json.load(f)
            
        # Store tokenizer and other parameters
        self.tokenizer = tokenizer or DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
        self.max_length = max_length
        
        # Use provided transform or default
        self.batch_transform_fn = batch_transform_fn or infonce_batch_transform
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.kwargs = kwargs
        
        # Pre-process into batches
        self.batches = self._create_batches()
        
    def _create_batches(self):
        """Transform raw data into batches based on the strategy"""
        # Create mini-batches of raw data
        num_items = len(self.raw_data)
        indices = list(range(num_items))
        
        if self.shuffle:
            random.shuffle(indices)
        
        batches = []
        print(f"Creating {len(indices)} batches of size {self.batch_size}")
        print(f"Using tokenizer: {self.tokenizer}")
        
        for i in trange(0, num_items, self.batch_size, desc="Creating batches"):
            batch_indices = indices[i:i+self.batch_size]
            batch_data = [self.raw_data[idx] for idx in batch_indices]
            
            # Apply the transform to create model-ready batch
            result = self.batch_transform_fn(
                batch_data, 
                self.tokenizer, 
                self.max_length, 
                **self.kwargs
            )
            
            # Handle return value (either just batch or batch with doc count)
            if isinstance(result, tuple) and len(result) == 2:
                processed_batch, batch_docs = result
            else:
                processed_batch = result
                # Estimate docs if not provided (backward compatibility)
                batch_docs = len(batch_data) * 2  # Rough estimate: 1 question + 1 answer per item
            
            if processed_batch:  # Skip empty batches
                # Store batch with its individual doc count (not cumulative)
                batches.append((processed_batch, batch_docs))
                
        return batches
    
    def __len__(self):
        """Return number of batches"""
        return len(self.batches)
        
    def __getitem__(self, idx):
        """Return a pre-processed batch with document count"""
        return self.batches[idx]
    
    @staticmethod
    def get_dataloader(dataset, shuffle=False):
        """
        Create a DataLoader for this dataset
        
        Since the dataset already returns batches, use batch_size=1
        """
        return DataLoader(
            dataset,
            batch_size=1,
            shuffle=shuffle,
            collate_fn=lambda x: x[0]  # Extract single batch from list
        )

def parse_from_json(data_path: str, limit: int = None) -> list:
    """
    Load QA pairs from a JSON file
    
    Args:
        data_path: Path to the JSON file
        limit: Maximum number of items to load
        
    Returns:
        List of QA pairs
    """
    with open(data_path, 'r') as f:
        data = json.load(f)
        
    if limit is not None:
        data = data[:limit]
        
    return data


def export_to_json(data: list, output_path: str = "data/ranked_qa.json") -> None:
    """
    Export QA pairs to a JSON file
    
    Args:
        data: List of QA pairs
        output_path: Path to save the JSON file
    """
    print(f"Exporting data to {output_path}...")
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"Exported {len(data)} question-answer sets to {output_path}")


def download_dataset():
    """
    Download the StackExchange dataset from Kaggle.
    Uses Kaggle API to download and extract the dataset.
    """
    # Create a directory for the dataset if it doesn't exist
    os.makedirs("data", exist_ok=True)
    
    # Check if files already exist to avoid re-downloading
    if os.path.exists("data/Questions.csv") and os.path.exists("data/Answers.csv"):
        q_size = os.path.getsize("data/Questions.csv")
        a_size = os.path.getsize("data/Answers.csv")
        # If files are reasonable size, assume they're valid
        if q_size > 10000 and a_size > 10000:
            print("Found existing dataset files:")
            print(f"  Questions.csv: {q_size/1_000_000:.1f} MB")
            print(f"  Answers.csv: {a_size/1_000_000:.1f} MB")
            return "data"
    
    print("Downloading dataset from Kaggle...")
    
    try:
        # Import the Kaggle API
        from kaggle.api.kaggle_api_extended import KaggleApi
        api = KaggleApi()
        api.authenticate()
        
        # Download the dataset
        dataset_name = 'stackoverflow/statsquestions'
        print(f"Downloading dataset {dataset_name}...")
        api.dataset_download_files(
            dataset_name, 
            path='data',
            unzip=True
        )
        print("Dataset downloaded and extracted successfully.")
        
        # Verify files were downloaded and are not empty
        if os.path.exists("data/Questions.csv") and os.path.exists("data/Answers.csv"):
            q_size = os.path.getsize("data/Questions.csv")
            a_size = os.path.getsize("data/Answers.csv")
            print("Dataset files:")
            print(f"  Questions.csv: {q_size/1_000_000:.1f} MB")
            print(f"  Answers.csv: {a_size/1_000_000:.1f} MB")
            return "data"
        else:
            raise FileNotFoundError("Dataset files not found after download")
            
    except (ImportError, ModuleNotFoundError):
        print("Error: Kaggle API not found. Installing kaggle package...")
        os.system("uv add kaggle")
        print("Kaggle package installed. Please run the script again.")
        sys.exit(1)
        
    except Exception as e:
        print(f"Error downloading dataset: {e}")
        
        # Fall back to using sample data for testing
        print("Falling back to sample data for testing")
        return "data"

def parse_posts(data_dir, limit=None):
    """
    Parse questions and answers from CSV files.
    
    Args:
        data_dir: Path to the directory containing Questions.csv and Answers.csv
        limit: Optional limit on the number of questions to process
        
    Returns:
        questions, answers: Dictionaries containing questions and their answers
    """
    questions_file = os.path.join(data_dir, "Questions.csv")
    answers_file = os.path.join(data_dir, "Answers.csv")
    
    # If files don't exist, return sample data
    if not os.path.exists(questions_file) or not os.path.exists(answers_file):
        print("CSV files not found. Using sample data instead.")
        return _create_sample_data()
    
    print(f"Parsing data from {data_dir}...")
    
    questions = {}  # Dictionary to store questions by ID
    answers = defaultdict(list)  # Dictionary to store answers by parent ID
    
    try:
        # First pass: collect all answers
        answer_count = 0
        start_time = time.time()
        last_update_time = start_time
        
        print("First pass: collecting answers...")
        
        with open(answers_file, 'r', encoding='latin-1') as f:
            # Skip header
            header = next(f)
            
            # Count lines for progress bar (subtract 1 for header)
            total_lines = sum(1 for _ in f) 
            f.seek(0)
            next(f)  # Skip header again
            
            # Create CSV reader
            reader = csv.reader(f)
            
            # Create progress bar
            for row in tqdm(reader, total=total_lines, desc="Parsing answers"):
                # Skip empty rows
                if not row:
                    continue
                
                answer_id, owner_id, creation_date, parent_id, score, body = row[:6]
                
                # Store answer data
                answers[parent_id].append({
                    "id": answer_id,
                    "body": body,
                    "score": int(score),
                    "is_accepted": False  # Will be set later
                })
                answer_count += 1
                
                # Print progress every 1000 answers or every 5 seconds
                current_time = time.time()
                if answer_count % 1000 == 0 or (current_time - last_update_time >= 5 and answer_count % 100 == 0):
                    elapsed = current_time - start_time
                    rate = answer_count / elapsed if elapsed > 0 else 0
                    print(f"  Processed {answer_count} answers... ({rate:.1f} answers/sec)")
                    last_update_time = current_time
        
        elapsed = time.time() - start_time
        print(f"Collected {answer_count} answers for {len(answers)} questions in {elapsed:.1f} seconds.")
        
        # Second pass: collect questions that have answers
        print("Second pass: collecting questions with answers...")
        start_time = time.time()
        last_update_time = start_time
        question_count = 0
        
        with open(questions_file, 'r', encoding='latin-1') as f:
            # Skip header
            header = next(f)
            
            # Count lines for progress bar (subtract 1 for header)
            total_lines = sum(1 for _ in f) 
            f.seek(0)
            next(f)  # Skip header again
            
            # Create CSV reader
            reader = csv.reader(f)
            
            # Create progress bar
            for row in tqdm(reader, total=total_lines, desc="Parsing questions"):
                # Skip empty rows
                if not row:
                    continue
                
                # CSV may not have all columns for every row
                row_data = row[:6] if len(row) >= 6 else row + [''] * (6 - len(row))
                question_id, owner_id, creation_date, score, title, body = row_data
                
                # Only process questions that have answers
                if question_id in answers:
                    questions[question_id] = {
                        "id": question_id,
                        "title": title,
                        "body": body,
                        "score": int(score),
                        "view_count": 0,  # Not available in this dataset
                        "tags": "",       # Not available in this dataset
                        "accepted_answer_id": None  # Will try to determine later
                    }
                    
                    question_count += 1
                    
                    # Print progress every 100 questions or every 5 seconds
                    current_time = time.time()
                    if question_count % 100 == 0 or (current_time - last_update_time >= 5 and question_count % 10 == 0):
                        elapsed = current_time - start_time
                        rate = question_count / elapsed if elapsed > 0 else 0
                        print(f"  Processed {question_count} questions with answers... ({rate:.1f} questions/sec)")
                        last_update_time = current_time
                    
                    # Apply limit if specified
                    if limit and question_count >= limit:
                        print(f"Reached limit of {limit} questions.")
                        break
        
        elapsed = time.time() - start_time
        print(f"Collected {question_count} questions with answers in {elapsed:.1f} seconds.")
        
        # Since we don't have accepted answers in this dataset, 
        # we'll consider the highest scored answer as the accepted one
        print("Ranking answers by score...")
        start_time = time.time()
        
        # Rank answers by score for each question
        for q_id in answers:
            # Sort answers by score (highest first)
            answers[q_id].sort(key=lambda x: x["score"], reverse=True)
            
            # If the question exists in our collection and has answers
            if q_id in questions and answers[q_id]:
                # Set the top-scored answer as the accepted answer
                top_answer = answers[q_id][0]
                top_answer["is_accepted"] = True
                questions[q_id]["accepted_answer_id"] = top_answer["id"]
        
        elapsed = time.time() - start_time
        print(f"Ranked all answers and marked highest-scored answers as accepted in {elapsed:.1f} seconds.")
        
        # If no questions were collected or limit is 0, use sample data
        if not questions or (limit is not None and limit <= 0):
            print("No questions collected. Using sample data instead.")
            return _create_sample_data()
        
        return questions, answers
    
    except Exception as e:
        print(f"Error parsing posts: {e}")
        import traceback
        traceback.print_exc()
        
        # Return sample data in case of error
        print("Falling back to sample data due to error.")
        return _create_sample_data()

def _create_sample_data():
    """Create sample QA data for testing"""
    questions = {}
    answers = {}
    
    # Add a few sample Q&A pairs for testing
    sample_q = {
        "id": "q1",
        "title": "Sample Question",
        "body": "This is a sample question for testing"
    }
    
    sample_a1 = {
        "id": "a1",
        "body": "This is a sample answer",
        "score": 5
    }
    
    sample_a2 = {
        "id": "a2",
        "body": "This is another sample answer",
        "score": 3
    }
    
    questions["q1"] = sample_q
    answers["q1"] = [sample_a1, sample_a2]
    
    return questions, answers

def ensure_dataset_exists(data_path: str = 'data/ranked_qa.json', 
                           data_limit: int = None, 
                           force_regenerate: bool = False) -> None:
    """
    Ensure the dataset exists, generating it if necessary.
    
    Args:
        data_path: Path where the JSON dataset should be stored
        data_limit: Limit the number of questions to process
        force_regenerate: Force regeneration even if file exists
    """
    # Check current limit if file exists
    current_limit = None
    regenerate_needed = force_regenerate
    
    if os.path.exists(data_path) and not force_regenerate:
        # Try to determine the current limit from the dataset
        try:
            with open(data_path, 'r') as f:
                data = json.load(f)
                current_limit = len(data)
                print(f"Found existing dataset at {data_path} with {current_limit} items")
                
                # If data_limit is specified and different from current, regenerate
                if data_limit is not None and data_limit != current_limit:
                    print(f"Requested limit ({data_limit}) differs from current dataset size ({current_limit})")
                    regenerate_needed = True
                else:
                    return  # Dataset exists with correct limit
        except Exception as e:
            print(f"Error reading existing dataset: {e}")
            regenerate_needed = True  # Regenerate if there's an issue with the file
    else:
        regenerate_needed = True
    
    if regenerate_needed:
        if os.path.exists(data_path):
            action = "Regenerating" if force_regenerate else "Updating"
            print(f"{action} dataset at {data_path} with limit={data_limit}")
        else:
            print(f"Dataset not found at {data_path}. Generating it with limit={data_limit}")
        
        # Get path for data directory
        data_dir = download_dataset()
            
        # Parse posts from CSV files
        print(f"Using CSV files in {data_dir}")
        questions, answers = parse_posts(data_dir, limit=data_limit)
        
        # Convert to the format expected by our JSON dataset
        data = []
        for q_id, question in questions.items():
            if q_id in answers:
                item = {
                    "question": question,
                    "answers": answers[q_id]
                }
                data.append(item)
        
        # Export to JSON
        export_to_json(data, data_path)

---
./src/rank_test/evaluate.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Fixed implementation of the evaluation code to correctly calculate hard negative accuracy metrics.
"""

import os
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict

def ndcg_at_k(relevances, k):
    """
    Calculate Normalized Discounted Cumulative Gain (NDCG) at k
    
    Args:
        relevances: List of relevance scores in rank order
        k: Cutoff for relevance calculation
        
    Returns:
        NDCG@k score
    """
    relevances = np.array(relevances)
    if len(relevances) < k:
        # Pad with zeros if not enough relevances
        relevances = np.pad(relevances, (0, k - len(relevances)))
    relevances = relevances[:k]
    
    # Calculate DCG
    dcg = np.sum(relevances / np.log2(np.arange(2, k + 2)))
    
    # Calculate ideal DCG
    ideal_relevances = np.sort(relevances)[::-1]
    idcg = np.sum(ideal_relevances / np.log2(np.arange(2, k + 2)))
    
    # Return NDCG
    return dcg / idcg if idcg > 0 else 0.0

def map_at_k(relevances, k):
    """
    Calculate Mean Average Precision (MAP) at k
    
    Args:
        relevances: List of relevance scores in rank order
        k: Cutoff for MAP calculation
        
    Returns:
        MAP@k score
    """
    relevances = np.array(relevances)
    if len(relevances) < k:
        # Pad with zeros if not enough relevances
        relevances = np.pad(relevances, (0, k - len(relevances)))
    relevances = relevances[:k]
    
    # Calculate precision at each position
    precisions = np.cumsum(relevances) / np.arange(1, k + 1)
    
    # Calculate average precision
    ap = np.sum(precisions * relevances) / np.sum(relevances) if np.sum(relevances) > 0 else 0.0
    
    return ap

def mrr(relevances):
    """
    Calculate Reciprocal Rank (RR) for a single query
    
    Args:
        relevances: List of relevance scores in rank order
                   (sorted by similarity/relevance in descending order)
        
    Returns:
        Reciprocal Rank score (1/rank of first relevant item)
    """
    relevances = np.array(relevances)
    # Find index of first relevant result
    indices = np.where(relevances > 0)[0]
    if len(indices) > 0:
        rank = indices[0] + 1  # +1 because indices are 0-based
        return 1.0 / rank
    else:
        return 0.0

def evaluate_standardized_test(model, test_dataloader, device, k_values=[1, 5, 10], debug_output=False):
    """
    Evaluate model with standardized test format that includes positive, hard negative,
    and normal negative answers for each question.
    
    Args:
        model: QAEmbeddingModel
        test_dataloader: DataLoader with standardized test data
        device: Device to run evaluation on
        k_values: List of k values for @k metrics
        debug_output: Whether to print detailed debug information
        
    Returns:
        Dictionary of evaluation metrics
    """
    model.eval()
    
    # Initialize metrics
    metrics = {
        # Overall metrics (all answers)
        'mrr': 0.0,
        'accuracy@1': 0.0,
        
        # In-batch negative metrics (ignoring hard negatives)
        'mrr_in_batch': 0.0,
        'accuracy@1_in_batch': 0.0,
        
        # Hard negative metrics (only answers for same question)
        'mrr_hard_neg': 0.0,
        'accuracy@1_hard_neg': 0.0,
    }
    
    for k in k_values:
        metrics[f'ndcg@{k}'] = 0.0
        metrics[f'map@{k}'] = 0.0
    
    # Track per-query metrics for calculating std. dev.
    per_query_metrics = {
        'mrr': [],
        'mrr_in_batch': [],
        'mrr_hard_neg': [],
    }
    
    for k in k_values:
        per_query_metrics[f'ndcg@{k}'] = []
        per_query_metrics[f'map@{k}'] = []
    
    total_questions = 0
    hard_neg_questions = 0
    
    # Process each batch
    with torch.no_grad():
        for batch_data in tqdm(test_dataloader, desc="Calculating embeddings"):
            if isinstance(batch_data, tuple):
                batch, _ = batch_data
            else:
                batch = batch_data
                
            for item in batch:
                # Skip items without answers
                if 'answers' not in item or not item['answers']:
                    continue
                
                total_questions += 1
                
                # Calculate question embedding
                q_embedding = model(
                    item['q_input_ids'].unsqueeze(0).to(device),
                    item['q_attention_mask'].unsqueeze(0).to(device)
                )
                
                # Process all answers for this question
                answers = item['answers']
                all_similarities = []
                has_hard_negative = False
                
                # Get embeddings and metadata for each answer
                for answer in answers:
                    # Calculate answer embedding
                    a_embedding = model(
                        answer['input_ids'].unsqueeze(0).to(device),
                        answer['attention_mask'].unsqueeze(0).to(device)
                    )
                    
                    # Calculate similarity
                    similarity = torch.matmul(q_embedding, a_embedding.t()).item()
                    
                    # Store similarity with metadata
                    all_similarities.append({
                        'similarity': similarity,
                        'is_positive': answer['is_positive'],
                        'is_hard_negative': answer.get('is_hard_negative', False),
                        'score': answer['score'],
                        'rank': answer['rank']
                    })
                    
                    # Check if this is a hard negative
                    if answer.get('is_hard_negative', False):
                        has_hard_negative = True
                
                if has_hard_negative:
                    hard_neg_questions += 1
                
                # Sort by similarity (descending)
                all_similarities.sort(key=lambda x: x['similarity'], reverse=True)
                
                # Create binary relevance array (1 for positive, 0 for negative)
                relevance = [1 if a['is_positive'] else 0 for a in all_similarities]
                
                # 1. Overall evaluation (all answers)
                # Calculate MRR
                query_mrr = mrr(relevance)
                metrics['mrr'] += query_mrr
                per_query_metrics['mrr'].append(query_mrr)
                
                # Calculate accuracy@1
                metrics['accuracy@1'] += 1.0 if relevance[0] == 1 else 0.0
                
                # Calculate NDCG@k and MAP@k
                for k in k_values:
                    query_ndcg = ndcg_at_k(relevance, k)
                    query_map = map_at_k(relevance, k)
                    
                    metrics[f'ndcg@{k}'] += query_ndcg
                    metrics[f'map@{k}'] += query_map
                    
                    per_query_metrics[f'ndcg@{k}'].append(query_ndcg)
                    per_query_metrics[f'map@{k}'].append(query_map)
                
                # 2. In-batch evaluation (exclude hard negatives)
                in_batch_sims = [a for a in all_similarities if not a.get('is_hard_negative', False)]
                in_batch_relevance = [1 if a['is_positive'] else 0 for a in in_batch_sims]
                
                # Calculate MRR for in-batch
                query_mrr_in_batch = mrr(in_batch_relevance)
                metrics['mrr_in_batch'] += query_mrr_in_batch
                per_query_metrics['mrr_in_batch'].append(query_mrr_in_batch)
                
                # Calculate accuracy@1 for in-batch
                metrics['accuracy@1_in_batch'] += 1.0 if in_batch_relevance and in_batch_relevance[0] == 1 else 0.0
                
                # 3. Hard negative evaluation (only for questions with hard negatives)
                if has_hard_negative:
                    # Include only the positive and hard negative answers
                    hard_neg_sims = [a for a in all_similarities if a['is_positive'] or a.get('is_hard_negative', False)]
                    hard_neg_relevance = [1 if a['is_positive'] else 0 for a in hard_neg_sims]
                    
                    # Calculate MRR for hard negatives
                    query_mrr_hard_neg = mrr(hard_neg_relevance)
                    metrics['mrr_hard_neg'] += query_mrr_hard_neg
                    per_query_metrics['mrr_hard_neg'].append(query_mrr_hard_neg)
                    
                    # Calculate accuracy@1 for hard negatives
                    metrics['accuracy@1_hard_neg'] += 1.0 if hard_neg_relevance and hard_neg_relevance[0] == 1 else 0.0
    
    # Average metrics
    if total_questions > 0:
        # Average overall and in-batch metrics
        for key in metrics:
            if key not in ['mrr_hard_neg', 'accuracy@1_hard_neg']:
                metrics[key] /= total_questions
    
    # Average hard negative metrics
    if hard_neg_questions > 0:
        metrics['mrr_hard_neg'] /= hard_neg_questions
        metrics['accuracy@1_hard_neg'] /= hard_neg_questions
    

    # Print debug info if requested
    if debug_output:
        print(f"Total questions evaluated: {total_questions}")
        print(f"Questions with hard negatives: {hard_neg_questions}")
        
        print("\nMetrics summary:")
        print(f"Overall MRR: {metrics['mrr']:.4f}")
        print(f"In-batch MRR: {metrics['mrr_in_batch']:.4f}")
        print(f"Hard negative MRR: {metrics['mrr_hard_neg']:.4f}")
        
        print(f"Overall accuracy@1: {metrics['accuracy@1']:.4f}")
        print(f"In-batch accuracy@1: {metrics['accuracy@1_in_batch']:.4f}")
        print(f"Hard negative accuracy@1: {metrics['accuracy@1_hard_neg']:.4f}")
    
    return metrics


def evaluate_model(model, test_dataloader, device, k_values=[1, 5, 10], debug_output=False):
    """
    Smart evaluation function that detects the test dataloader format and 
    uses the appropriate evaluation strategy.
    
    Args:
        model: QAEmbeddingModel
        test_dataloader: DataLoader with test data
        device: Device to run evaluation on
        k_values: List of k values for @k metrics
        debug_output: Whether to print detailed debug information
        
    Returns:
        Dictionary of evaluation metrics
    """
    # Check the first batch to determine the format
    for batch_data in test_dataloader:
        # Extract batch if it comes with document count
        if isinstance(batch_data, tuple) and len(batch_data) == 2:
            batch = batch_data[0]
        else:
            batch = batch_data
            
        # Check if this is a standardized test batch
        if isinstance(batch, list) and 'answers' in batch[0] and isinstance(batch[0]['answers'], list):
            return evaluate_standardized_test(model, test_dataloader, device, k_values, debug_output)
            
        break  # Only need to check first batch
        
    # Default to original implementation for backward compatibility
    model.eval()
    
    # Collect all embeddings, relevance scores, and question IDs
    all_q_embeddings = []
    all_a_embeddings = []
    all_question_ids = []  # To track which answers belong to which questions
    
    # Extract data to track which answers belong to which questions
    question_data = defaultdict(list)  # Map question ID to list of answer indices
    
    # Calculate embeddings for each item
    with torch.no_grad():
        start_idx = 0  # Track the absolute index in the final concatenated tensor
        
        for batch_idx, batch_data in enumerate(tqdm(test_dataloader, desc="Calculating embeddings")):
            # Extract batch if it comes with document count
            if isinstance(batch_data, tuple) and len(batch_data) == 2:
                batch = batch_data[0]
            else:
                batch = batch_data
                
            # Get embeddings
            q_embeddings = model(batch['q_input_ids'].to(device), 
                                batch['q_attention_mask'].to(device))
            a_embeddings = model(batch['a_input_ids'].to(device), 
                                batch['a_attention_mask'].to(device))
            
            # Store embeddings
            all_q_embeddings.append(q_embeddings.cpu())
            all_a_embeddings.append(a_embeddings.cpu())
            
            # Track question IDs from batch
            batch_size = q_embeddings.shape[0]
            
            # Use question_id from batch, or create dummy IDs if not available
            if 'question_id' in batch:
                q_ids = batch['question_id']
            elif 'question_ids' in batch:
                q_ids = batch['question_ids']
            else:
                q_ids = [f"batch{batch_idx}_item{i}" for i in range(batch_size)]
                
            all_question_ids.extend(q_ids)
            
            # Store which answers belong to which questions
            for i in range(batch_size):
                idx = start_idx + i  # Absolute index in the concatenated tensor
                q_id = q_ids[i]
                question_data[q_id].append(idx)
            
            # Update the start index for the next batch
            start_idx += batch_size
    
    # Concatenate all embeddings
    all_q_embeddings = torch.cat(all_q_embeddings, dim=0)
    all_a_embeddings = torch.cat(all_a_embeddings, dim=0)
    
    # Calculate similarity matrix for all question-answer pairs
    similarity = torch.matmul(all_q_embeddings, all_a_embeddings.T)
    
    # Initialize metrics
    metrics = {
        # Overall metrics (all answers)
        'mrr': 0.0,
        'accuracy@1': 0.0,
        
        # In-batch negative metrics (ignoring hard negatives)
        'mrr_in_batch': 0.0,
        'accuracy@1_in_batch': 0.0,
        
        # Hard negative metrics (only answers for same question)
        'mrr_hard_neg': 0.0,
        'accuracy@1_hard_neg': 0.0,
    }
    
    for k in k_values:
        metrics[f'ndcg@{k}'] = 0.0
        metrics[f'map@{k}'] = 0.0
    
    # Dictionary to track metrics for each query
    per_query_metrics = {
        'mrr': [],
        'mrr_in_batch': [],
        'mrr_hard_neg': [],
    }
    
    for k in k_values:
        per_query_metrics[f'ndcg@{k}'] = []
        per_query_metrics[f'map@{k}'] = []
        
    # IMPORTANT: How the three types of accuracy work:
    # 1. Overall accuracy - Ranks across ALL answers in the test set
    #    - For each question, how well its correct answer ranks among all possible answers
    # 2. In-batch negative accuracy - Only considers answers NOT from the same question
    #    - For each question, how well its answer ranks among answers to other questions
    # 3. Hard negative accuracy - Only considers answers to the same question
    #    - For each question, how well its correct answer ranks among all answers for that question
    
    # Calculate metrics for each query
    n_queries = similarity.shape[0]
    for i in range(n_queries):
        # Get similarities for this query
        query_similarities = similarity[i].numpy()
        
        # Create binary relevance (1 for matches, 0 for non-matches)
        relevance = np.zeros_like(query_similarities)
        relevance[i] = 1  # Correct answer is at index i
        
        # Get question ID for this query
        q_id = all_question_ids[i]
        
        # Get indices of all answers to this question (hard negatives)
        hard_negative_indices = question_data[q_id]
        
        # 1. Overall ranking (all answers)
        sorted_indices = np.argsort(-query_similarities)
        sorted_relevances = relevance[sorted_indices]
        
        # Calculate overall MRR and metrics
        query_mrr = mrr(sorted_relevances)
        metrics['mrr'] += query_mrr
        per_query_metrics['mrr'].append(query_mrr)
        
        # Add accuracy@1 - whether the correct answer is ranked first
        metrics['accuracy@1'] += 1.0 if sorted_indices[0] == i else 0.0
        
        # Calculate NDCG@k and MAP@k for overall ranking
        for k in k_values:
            query_ndcg = ndcg_at_k(sorted_relevances, k)
            query_map = map_at_k(sorted_relevances, k)
            
            metrics[f'ndcg@{k}'] += query_ndcg
            metrics[f'map@{k}'] += query_map
            
            per_query_metrics[f'ndcg@{k}'].append(query_ndcg)
            per_query_metrics[f'map@{k}'].append(query_map)
        
        # 2. In-batch negative ranking (excluding hard negatives)
        # Create mask that excludes hard negatives
        in_batch_mask = np.ones_like(query_similarities, dtype=bool)
        for idx in hard_negative_indices:
            if idx != i:  # Keep the correct answer
                in_batch_mask[idx] = False
        
        # Only consider in-batch negatives
        in_batch_similarities = query_similarities[in_batch_mask]
        in_batch_relevance = relevance[in_batch_mask]
        
        # Get correct answer index in this reduced set
        correct_idx = np.where(in_batch_mask)[0].tolist().index(i)
        
        # Sort by similarity (descending)
        in_batch_sorted_indices = np.argsort(-in_batch_similarities)
        in_batch_sorted_relevances = in_batch_relevance[in_batch_sorted_indices]
        
        # Calculate MRR for in-batch negatives
        query_mrr_in_batch = mrr(in_batch_sorted_relevances)
        metrics['mrr_in_batch'] += query_mrr_in_batch
        per_query_metrics['mrr_in_batch'].append(query_mrr_in_batch)
        
        # Add accuracy@1 for in-batch negatives
        metrics['accuracy@1_in_batch'] += 1.0 if in_batch_sorted_indices[0] == correct_idx else 0.0
    
    # 3. Hard negative ranking (only answers to same question)
    # We need a more robust way to track questions with multiple answers
    # Create dict mapping question_id to indices of its answers
    question_to_indices = defaultdict(list)
    for i, q_id in enumerate(all_question_ids):
        # Convert tensor to scalar if needed
        if isinstance(q_id, torch.Tensor):
            q_id = q_id.item()
        # Convert string tensor to string if needed
        elif hasattr(q_id, 'decode'):
            q_id = q_id.decode()
            
        question_to_indices[q_id].append(i)
    
    # Track metrics for each query
    hard_neg_mrr_values = []
    hard_neg_accuracy_values = []
    
    # Process only questions that have multiple answers (for hard negative comparison)
    questions_with_multiple_answers = []
    for q_id, indices in question_to_indices.items():
        if len(indices) <= 1:
            continue
            
        questions_with_multiple_answers.append(q_id)
        
        # For each query in this question
        for query_idx in indices:
            # Get similarities between this query and all answers
            query_similarities = similarity[query_idx].cpu().numpy()
            
            # Create a mask that selects only answers to this question
            mask = np.zeros(len(query_similarities), dtype=bool)
            for idx in indices:
                mask[idx] = True
                
            # Get similarities only for answers to this question
            hard_neg_similarities = query_similarities[mask]
            
            # Create binary relevance scores (1 for the correct answer, 0 for others)
            hard_neg_relevance = np.zeros(len(indices))
            # Find position of query_idx in indices
            query_position = indices.index(query_idx)
            hard_neg_relevance[query_position] = 1
            
            # Sort by similarity (descending)
            hard_neg_sorted_indices = np.argsort(-hard_neg_similarities)
            hard_neg_sorted_relevances = hard_neg_relevance[hard_neg_sorted_indices]
            
            # Calculate MRR
            query_mrr = mrr(hard_neg_sorted_relevances)
            hard_neg_mrr_values.append(query_mrr)
            
            # Calculate accuracy@1 (whether the top result is the correct answer)
            accuracy = 1.0 if hard_neg_sorted_indices[0] == query_position else 0.0
            hard_neg_accuracy_values.append(accuracy)
    
    # Calculate total MRR for hard negatives
    if hard_neg_mrr_values:
        metrics['mrr_hard_neg'] = np.mean(hard_neg_mrr_values)
        per_query_metrics['mrr_hard_neg'] = hard_neg_mrr_values
        metrics['mrr_hard_neg_std'] = np.std(hard_neg_mrr_values)
    else:
        metrics['mrr_hard_neg'] = 0.0
        metrics['mrr_hard_neg_std'] = 0.0
    
    # Calculate accuracy for hard negatives
    if hard_neg_accuracy_values:
        metrics['accuracy@1_hard_neg'] = np.mean(hard_neg_accuracy_values)
    else:
        metrics['accuracy@1_hard_neg'] = 0.0
        
    if debug_output:
        print(f"Questions with multiple answers: {len(questions_with_multiple_answers)}")
        print(f"Total evaluations for hard negatives: {len(hard_neg_mrr_values)}")
        if hard_neg_mrr_values:
            mrr_values = np.array(hard_neg_mrr_values)
            print(f"Hard negative MRR: mean={np.mean(mrr_values):.4f}, "
                  f"min={np.min(mrr_values):.4f}, max={np.max(mrr_values):.4f}")
            print(f"Hard negative accuracy@1: {metrics['accuracy@1_hard_neg']:.4f}")
    
    # Average metrics across all queries for overall and in-batch metrics
    for key in metrics:
        if key not in ['mrr_hard_neg', 'mrr_hard_neg_std', 'accuracy@1_hard_neg']:
            metrics[key] /= n_queries
    
    # Add std dev for regular metrics
    for key in per_query_metrics:
        if key != 'mrr_hard_neg' and per_query_metrics[key]:
            metrics[f'{key}_std'] = np.std(per_query_metrics[key])
    
    # Print debug info if requested
    if debug_output:
        print(f"Total questions: {len(question_to_indices)}")
        print(f"Questions with multiple answers: {len(questions_with_multiple_answers)}")
        print(f"Total evaluations for hard negatives: {len(hard_neg_mrr_values)}")
        
        if hard_neg_mrr_values:
            print("\nHard negative MRR distribution:")
            print(f"  Min: {min(hard_neg_mrr_values):.4f}")
            print(f"  Max: {max(hard_neg_mrr_values):.4f}")
            print(f"  Mean: {np.mean(hard_neg_mrr_values):.4f}")
            print(f"  Median: {np.median(hard_neg_mrr_values):.4f}")
            
            # Count of perfect MRR (1.0) and zero MRR
            perfect_mrr = sum(1 for mrr in hard_neg_mrr_values if mrr == 1.0)
            zero_mrr = sum(1 for mrr in hard_neg_mrr_values if mrr == 0.0)
            print(f"  Perfect MRR (1.0): {perfect_mrr} ({perfect_mrr/len(hard_neg_mrr_values)*100:.1f}%)")
            print(f"  Zero MRR (0.0): {zero_mrr} ({zero_mrr/len(hard_neg_mrr_values)*100:.1f}%)")
    
    return metrics

def compare_models(results, output_dir='results'):
    """
    Compare metrics across multiple models
    
    Args:
        results: Dictionary mapping model names to their evaluation metrics
        output_dir: Directory to save comparison results
        
    Returns:
        DataFrame with model comparison
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert results to DataFrame
    models = []
    for model_name, metrics in results.items():
        row = {'model': model_name}
        row.update(metrics)
        models.append(row)
    
    df = pd.DataFrame(models)
    
    # Save results to CSV
    df.to_csv(os.path.join(output_dir, 'model_comparison.csv'), index=False)
    
    # Create comparison plots
    plt.figure(figsize=(12, 8))
    
    # Compare MRR
    plt.subplot(2, 2, 1)
    plt.bar(df['model'], df['mrr'])
    plt.title('Mean Reciprocal Rank (MRR)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Compare NDCG@10
    plt.subplot(2, 2, 2)
    plt.bar(df['model'], df['ndcg@10'])
    plt.title('NDCG@10')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Compare MAP@10
    plt.subplot(2, 2, 3)
    plt.bar(df['model'], df['map@10'])
    plt.title('MAP@10')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    # Save the plot
    plt.savefig(os.path.join(output_dir, 'model_comparison.png'))
    
    print(f"Model comparison saved to {output_dir}")
    return df

---
./src/rank_test/ideas.md
---
- pairwise losse
- ranking loss with only hard negatives
- ranking loss with hard negatives + in batch negatives
- info nce loss with hard negatives (documents lower than the positive from the same question)
- info nce loss w/o hard negatives, only taking the top document
- info nce loss taking random document
- info nce loss with multiple positives, all documents
- info nce loss with hard negatives, weighted by the score/rank
- info nce loss but multiple rows per question (one per document), with no hard negatives (this might be bad because it treats other positive documents from the same query as in batch negatives, but we want to experiment)

---
./src/rank_test/losses.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Unified loss functions that work with batch transforms directly.
Each loss function handles its specific batch format and model forward passes.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import defaultdict
from typing import Dict, List, Tuple, Union, Any, Optional

# Define base loss classes

class BaseLoss(nn.Module):
    """Base class for all loss functions"""
    
    def __init__(self, **kwargs):
        super(BaseLoss, self).__init__()
        self.name = "base_loss"
        
    def forward(self, **kwargs):
        """
        Calculate loss
            
        Returns:
            loss: Loss value
            metrics: Dictionary of metrics for logging
        """
        raise NotImplementedError("Subclasses must implement forward method")
    
    def get_name(self):
        """Get loss function name for logging"""
        return self.name


class FlexibleInfoNCELoss(BaseLoss):
    """
    Flexible InfoNCE loss that handles single or multiple positives per query.
    Relies on question_ids to identify positive pairs.
    Assumes batch is structured such that for a given question_id, all its 
    corresponding items (q_embeddings and a_embeddings) are positives for each other.
    """
    
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.name = f"flex_infonce_t{temperature}"
        
    def forward(self, q_embeddings, a_embeddings, question_ids, **kwargs):
        """
        Calculate flexible InfoNCE loss.
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            question_ids: List or Tensor of question IDs corresponding to each q/a pair.
                          Items with the same question_id are considered positives for each other.
            
        Returns:
            loss: InfoNCE loss value
            metrics: Dictionary with accuracy metrics
        """
        batch_size = q_embeddings.shape[0]
        device = q_embeddings.device
        
        if batch_size <= 1:
             return torch.tensor(0.0, device=device, requires_grad=True), {'loss': 0.0, 'acc': 0.0, 'groups': 0}

        # Calculate similarity matrix (Q x A)
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        
        # Group indices by question ID
        question_groups = defaultdict(list)
        for i, q_id in enumerate(question_ids):
            # Ensure q_id is hashable (e.g., convert tensor to int/str if needed)
            hashable_q_id = q_id.item() if isinstance(q_id, torch.Tensor) else q_id
            question_groups[hashable_q_id].append(i)

        total_loss = torch.tensor(0.0, device=device)
        correct_count = 0
        group_count = 0

        # Calculate loss per query
        for q_id, indices in question_groups.items():
            if len(indices) == 0:
                continue
            
            group_count += 1
            
            # Loss for each item in the group (treating others in the group as positive)
            for i in indices:
                # Log-softmax scores for the current query q_i against all answers
                log_probs = F.log_softmax(similarity[i], dim=0)
                
                # Positive indices for this query are the *other* items in the same group
                # If only one item for this q_id, it's contrasted against all others (standard InfoNCE)
                positive_indices = indices # All items from the same question are positives
                
                if len(positive_indices) > 0:
                    # Average the negative log probabilities of positive targets
                    loss_i = -log_probs[positive_indices].mean()
                    total_loss += loss_i

                # Accuracy: Check if the highest scoring item is one of the positives
                # Note: This definition might need adjustment based on specific eval needs
                # For simplicity, we check if the argmax is *any* of the items from the same group.
                if len(indices) > 0: # Only calculate accuracy if positives exist
                    pred_idx = torch.argmax(similarity[i]).item()
                    if pred_idx in indices:
                         correct_count += 1 # Count prediction for this item

        # Average loss over all items in the batch
        loss = total_loss / batch_size if batch_size > 0 else torch.tensor(0.0, device=device)
        
        # Average accuracy over all items
        accuracy = correct_count / batch_size if batch_size > 0 else 0.0
        
        metrics = {
            'loss': loss.item(),
            'acc': accuracy,
            'groups': group_count # Number of unique questions processed
        }
        
        return loss, metrics


class RankInfoNCELoss(FlexibleInfoNCELoss):
    """
    InfoNCE loss that weights positive examples based on rank or score.
    Inherits from FlexibleInfoNCELoss and modifies the positive target weighting.
    Lower ranks (better) or higher scores (better) receive higher weight.
    """
    
    def __init__(self, temperature=0.1, use_ranks=True, use_scores=False, 
                 rank_weight=0.1, score_weight=0.01):
        """
        Initialize the loss function.
        
        Args:
            temperature: Temperature parameter for scaling similarity scores
            use_ranks: Whether to use ordinal rank information (lower is better)
            use_scores: Whether to use cardinal score information (higher is better)
            rank_weight: Controls how much weight decreases for higher ranks.
                         Weight = exp(-rank_weight * rank)
            score_weight: Controls how much weight increases for higher scores.
                          Weight = exp(score_weight * normalized_score)
        """
        super().__init__(temperature=temperature)
        self.use_ranks = use_ranks
        self.use_scores = use_scores
        self.rank_weight = rank_weight
        self.score_weight = score_weight
        
        # Create descriptive name
        ranking_method = []
        if use_ranks:
            ranking_method.append(f"rank{rank_weight}")
        if use_scores:
            ranking_method.append(f"score{score_weight}")
        
        ranking_str = "_".join(ranking_method) if ranking_method else "standard"
        self.name = f"rank_infonce_{ranking_str}_t{temperature}"
        
    def forward(self, q_embeddings, a_embeddings, question_ids, ranks=None, scores=None, **kwargs):
        """
        Calculate rank/score-weighted InfoNCE loss.
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            question_ids: List or Tensor of question IDs.
            ranks: Ordinal rank of each answer (0 is highest ranked). Tensor expected.
            scores: Cardinal score of each answer (higher is better). Tensor expected.
            
        Returns:
            loss: Weighted InfoNCE loss
            metrics: Dictionary with accuracy and other metrics
        """
        batch_size = q_embeddings.shape[0]
        device = q_embeddings.device
        
        if batch_size <= 1:
             return torch.tensor(0.0, device=device, requires_grad=True), {'loss': 0.0, 'acc': 0.0, 'groups': 0}

        # Calculate similarity matrix (Q x A)
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        
        # Group indices by question ID
        question_groups = defaultdict(list)
        group_data = defaultdict(lambda: {'indices': [], 'ranks': [], 'scores': []})
        
        for i, q_id in enumerate(question_ids):
            hashable_q_id = q_id.item() if isinstance(q_id, torch.Tensor) else q_id
            group_data[hashable_q_id]['indices'].append(i)
            if self.use_ranks and ranks is not None:
                group_data[hashable_q_id]['ranks'].append(ranks[i].item())
            if self.use_scores and scores is not None:
                group_data[hashable_q_id]['scores'].append(scores[i].item())

        total_loss = torch.tensor(0.0, device=device)
        correct_count = 0
        group_count = 0

        # Calculate loss per query
        for q_id, data in group_data.items():
            indices = data['indices']
            if len(indices) == 0:
                continue
                
            group_count += 1
            
            group_ranks = torch.tensor(data['ranks'], device=device) if self.use_ranks and data['ranks'] else None
            group_scores = torch.tensor(data['scores'], device=device) if self.use_scores and data['scores'] else None
            
            # Calculate weights for positives within this group
            pos_weights = torch.ones(len(indices), device=device)
            
            if self.use_ranks and group_ranks is not None and len(group_ranks) == len(indices):
                # Exponential decay for ranks: lower rank (better) -> higher weight
                pos_weights *= torch.exp(-self.rank_weight * group_ranks)
                
            if self.use_scores and group_scores is not None and len(group_scores) == len(indices):
                # Normalize scores within the group (optional, but helps scale weight)
                max_score = torch.max(group_scores)
                min_score = torch.min(group_scores)
                if max_score > min_score:
                     norm_scores = (group_scores - min_score) / (max_score - min_score)
                else:
                     norm_scores = torch.zeros_like(group_scores) # Handle case where all scores are the same

                # Exponential increase for scores: higher score -> higher weight
                pos_weights *= torch.exp(self.score_weight * norm_scores)

            # Normalize weights to sum to 1 (important for weighted average)
            if torch.sum(pos_weights) > 0:
                 pos_weights /= torch.sum(pos_weights)
            else:
                 pos_weights = torch.ones_like(pos_weights) / len(pos_weights) # Equal weight if sum is zero

            # Loss for each item in the group
            for i_idx, i in enumerate(indices): # i_idx is the index within the group, i is the global index
                log_probs = F.log_softmax(similarity[i], dim=0)
                
                # Positive indices for this query are all items in the same group
                positive_indices = indices 
                
                if len(positive_indices) > 0:
                    # Weighted average of negative log probabilities of positive targets
                    # We use the weights calculated for the *targets* (positives)
                    loss_i = -torch.sum(pos_weights * log_probs[positive_indices])
                    total_loss += loss_i

                # Accuracy: Check if the highest scoring item is one of the positives
                if len(indices) > 0:
                    pred_idx = torch.argmax(similarity[i]).item()
                    if pred_idx in indices:
                         correct_count += 1

        # Average loss over all items
        loss = total_loss / batch_size if batch_size > 0 else torch.tensor(0.0, device=device)
        accuracy = correct_count / batch_size if batch_size > 0 else 0.0
        
        metrics = {
            'loss': loss.item(),
            'acc': accuracy,
            'groups': group_count
        }
        
        return loss, metrics


class RelaxedHardNegativeInfoNCELoss(FlexibleInfoNCELoss):
    """
    InfoNCE loss variant that reduces the penalty for confusing positives
    with hard negatives (other answers from the same question).
    Uses KL divergence with a soft target distribution where hard negatives
    receive a small target probability.
    """
    
    def __init__(self, temperature=0.1, hard_neg_target_prob=0.1):
        """
        Initialize the loss function.
        
        Args:
            temperature: Temperature parameter for scaling similarity scores.
            hard_neg_target_prob: Target probability mass to distribute among 
                                  hard negatives for a given query. Should be < 1.0.
        """
        super().__init__(temperature=temperature)
        if not (0.0 <= hard_neg_target_prob < 1.0):
             raise ValueError("hard_neg_target_prob must be between 0.0 and 1.0 (exclusive of 1.0)")
        self.hard_neg_target_prob = hard_neg_target_prob
        self.name = f"relaxed_hn_infonce_t{temperature}_p{hard_neg_target_prob}"
        
    def forward(self, q_embeddings, a_embeddings, question_ids, **kwargs):
        """
        Calculate relaxed hard negative InfoNCE loss using KL divergence.
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_embeddings: Answer embeddings (batch_size, embed_dim)
            question_ids: List or Tensor of question IDs.
            
        Returns:
            loss: Relaxed hard negative loss value
            metrics: Dictionary with accuracy metrics
        """
        batch_size = q_embeddings.shape[0]
        device = q_embeddings.device
        
        if batch_size <= 1:
             return torch.tensor(0.0, device=device, requires_grad=True), {'loss': 0.0, 'acc': 0.0, 'groups': 0}

        # Calculate similarity matrix (Q x A)
        similarity = torch.matmul(q_embeddings, a_embeddings.T) / self.temperature
        
        # Group indices by question ID
        question_groups = defaultdict(list)
        for i, q_id in enumerate(question_ids):
            hashable_q_id = q_id.item() if isinstance(q_id, torch.Tensor) else q_id
            question_groups[hashable_q_id].append(i)

        total_loss = torch.tensor(0.0, device=device)
        correct_count = 0
        group_count = 0

        # Calculate loss per query using KL divergence
        for q_id, indices in question_groups.items():
            if len(indices) == 0:
                continue
            
            group_count += 1
            
            # Loss for each item `i` in the group
            for i in indices:
                # Identify positive and hard negative indices for query i
                # In this setup, all items from the same group are treated symmetrically.
                # We consider 'i' as the anchor/positive for this specific calculation,
                # and the *other* items in the group as hard negatives.
                positive_indices = [i] # Anchor item is the positive target
                hard_negative_indices = [j for j in indices if j != i]
                
                # Create the soft target distribution for query i
                target_probs = torch.zeros(batch_size, device=device)
                
                # Assign probability to the positive anchor
                positive_mass = 1.0 - self.hard_neg_target_prob
                target_probs[positive_indices] = positive_mass / len(positive_indices) # Should always be len 1 here

                # Distribute remaining probability mass among hard negatives
                if hard_negative_indices and self.hard_neg_target_prob > 0:
                     target_probs[hard_negative_indices] = self.hard_neg_target_prob / len(hard_negative_indices)
                
                # Ensure target sums to 1 (handles edge case of no hard negs)
                if not torch.isclose(torch.sum(target_probs), torch.tensor(1.0, device=device)):
                    # Fallback if no hard negs, put all mass on positive
                     target_probs = torch.zeros(batch_size, device=device)
                     target_probs[positive_indices] = 1.0 / len(positive_indices)

                # Calculate KL divergence loss for query i
                # log_softmax of similarities vs target distribution
                log_probs = F.log_softmax(similarity[i], dim=0)
                loss_i = F.kl_div(log_probs, target_probs, reduction='sum') # kl_div expects log input
                total_loss += loss_i

                # Accuracy: Check if the highest scoring item is the anchor 'i'
                # (Stricter than FlexibleInfoNCE, checks if the *intended* positive wins)
                if len(indices) > 0:
                    pred_idx = torch.argmax(similarity[i]).item()
                    if pred_idx == i: # Check if prediction matches the anchor
                         correct_count += 1

        # Average loss over all items in the batch
        loss = total_loss / batch_size if batch_size > 0 else torch.tensor(0.0, device=device)
        
        # Average accuracy over all items
        # Note: Accuracy definition here is stricter (predicting the anchor 'i')
        accuracy = correct_count / batch_size if batch_size > 0 else 0.0
        
        metrics = {
            'loss': loss.item(),
            'acc': accuracy, # Accuracy of predicting the specific anchor 'i'
            'groups': group_count
        }
        
        return loss, metrics


class TripletLoss(BaseLoss):
    """
    Triplet loss for query-positive-negative triplets.
    Works with the triplet batch transform.
    """
    
    def __init__(self, margin=0.3):
        """
        Initialize the triplet loss.
        
        Args:
            margin: Margin between positive and negative similarities
        """
        super().__init__()
        self.margin = margin
        self.name = f"triplet_m{margin}"
        
    def forward(self, q_embeddings, a_pos_embeddings, a_neg_embeddings, **kwargs):
        """
        Calculate triplet loss
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
            a_pos_embeddings: Positive answer embeddings (batch_size, embed_dim)
            a_neg_embeddings: Negative answer embeddings (batch_size, embed_dim)
            
        Returns:
            loss: Triplet loss
            metrics: Dictionary with accuracy and similarity metrics
        """
        # Calculate similarity between queries and answers
        pos_sim = torch.sum(q_embeddings * a_pos_embeddings, dim=1)
        neg_sim = torch.sum(q_embeddings * a_neg_embeddings, dim=1)
        
        # Calculate triplet loss: enforce margin between pos_sim and neg_sim
        losses = F.relu(neg_sim - pos_sim + self.margin)
        loss = torch.mean(losses)
        
        # Calculate accuracy (how often pos_sim > neg_sim)
        acc = (pos_sim > neg_sim).float().mean()
        
        metrics = {
            'loss': loss.item(),
            'acc': acc.item(),
            'avg_pos_sim': pos_sim.mean().item(),
            'avg_neg_sim': neg_sim.mean().item(),
            'margin_violations': (losses > 0).float().mean().item()
        }
        
        return loss, metrics


class ListwiseRankingLoss(BaseLoss):
    """
    Listwise ranking loss for learning to rank multiple answers.
    Works with the listwise batch transform.
    """
    
    def __init__(self, temperature=1.0):
        """
        Initialize the listwise ranking loss.
        
        Args:
            temperature: Temperature for scaling similarities
        """
        super().__init__()
        self.temperature = temperature
        self.name = f"listwise_t{temperature}"
        
    def forward(self, q_embeddings, a_list_embeddings, a_list_scores, **kwargs):
        """
        Calculate listwise ranking loss
        
        Args:
            q_embeddings: Question embeddings (batch_size, embed_dim)
                          or list of embeddings, one per question
            a_list_embeddings: List of answer embeddings tensors
                              [(num_answers, embed_dim), ...]
            a_list_scores: List of scores for each answer per question
                          [(num_answers,), ...]
            
        Returns:
            loss: Listwise ranking loss
            metrics: Dictionary with NDCG and other metrics
        """
         # Determine if we're using a device (for GPU support)
        if isinstance(q_embeddings, list):
            # Handle case where input is already per-question lists
            if not q_embeddings: return torch.tensor(0.0), {'loss': 0.0, 'ndcg': 0.0}
            device = q_embeddings[0].device
            q_embeds = q_embeddings
            batch_size_eff = len(q_embeddings)
        elif isinstance(q_embeddings, torch.Tensor):
            # Handle standard batch tensor input (assuming one query embedding per list entry)
            if q_embeddings.nelement() == 0: return torch.tensor(0.0), {'loss': 0.0, 'ndcg': 0.0}
            device = q_embeddings.device
            # This assumes the batch dimension of q_embeddings corresponds to the lists
            q_embeds = [q_embeddings[i] for i in range(q_embeddings.shape[0])]
            batch_size_eff = q_embeddings.shape[0]
        else:
            raise TypeError("q_embeddings must be a list of tensors or a single tensor.")

        if not isinstance(a_list_embeddings, list) or not isinstance(a_list_scores, list):
             raise TypeError("a_list_embeddings and a_list_scores must be lists.")
        
        # Ensure consistent lengths
        min_len = min(len(q_embeds), len(a_list_embeddings), len(a_list_scores))
        if min_len == 0: return torch.tensor(0.0, device=device), {'loss': 0.0, 'ndcg': 0.0}
        
        total_loss = torch.tensor(0.0, device=device)
        total_ndcg = 0.0
        valid_items = 0

        for i in range(min_len):
            q_embed = q_embeds[i] # Shape might be (embed_dim,) or (1, embed_dim)
            answers_embed = a_list_embeddings[i] # Shape (num_answers, embed_dim)
            scores = a_list_scores[i] # Shape (num_answers,)

            if answers_embed.nelement() == 0 or scores.nelement() == 0 or q_embed.nelement() == 0:
                continue
            
            # Ensure q_embed is 2D for matmul: (1, embed_dim)
            if q_embed.dim() == 1:
                q_embed = q_embed.unsqueeze(0)
                
            # Ensure answers_embed and scores are on the correct device
            answers_embed = answers_embed.to(device)
            scores = scores.to(device)

            num_answers = answers_embed.shape[0]
            if num_answers < 1: # Need at least one answer
                 continue

            # Calculate similarity: (1, embed_dim) @ (embed_dim, num_answers) -> (1, num_answers)
            sim = torch.matmul(q_embed, answers_embed.T).squeeze(0)  # Result shape: (num_answers,)
            sim = sim / self.temperature
            
            # Convert similarities to probabilities
            sim_probs = F.softmax(sim, dim=0)
            
            # Create target probabilities from normalized scores (normalize to avoid large values)
            # Temperature scaling for targets as well
            norm_scores = scores / self.temperature
            target_probs = F.softmax(norm_scores, dim=0)
            
            # KL divergence loss: KL(target || predicted) = sum(target * log(target / predicted))
            # Note: F.kl_div expects input=log(predicted), target=target
            # Use reduction='batchmean' or 'sum' and average later
            loss_i = F.kl_div(torch.log(sim_probs + 1e-9), target_probs, reduction='sum')
            
             # Avoid NaN loss if target_probs has zeros where sim_probs is non-zero due to temp scaling
            if torch.isnan(loss_i):
                # Fallback or alternative handling needed, e.g., skip or use different loss
                print(f"Warning: NaN loss encountered for item {i}. Skipping.")
                continue
                
            total_loss += loss_i
            valid_items += 1
            
            # Calculate NDCG
            if num_answers > 1:
                try:
                    # Sort predicted and target rankings
                    _, pred_indices = torch.sort(sim, descending=True)
                    _, ideal_indices = torch.sort(scores, descending=True) # Use original scores for ideal ranking
                    
                    # Calculate DCG
                    pred_dcg = self._calculate_dcg(pred_indices, scores) # Use original scores for gain
                    ideal_dcg = self._calculate_dcg(ideal_indices, scores)
                    
                    # Calculate NDCG
                    ndcg = pred_dcg / (ideal_dcg + 1e-8) if ideal_dcg > 0 else 0.0
                    total_ndcg += ndcg.item() # Add scalar value
                except Exception as e:
                    print(f"Warning: Error calculating NDCG for item {i}: {e}")
                    # NDCG calculation might fail if scores are constant, etc.


        # Average loss and metrics over valid items processed
        loss = total_loss / valid_items if valid_items > 0 else torch.tensor(0.0, device=device)
        avg_ndcg = total_ndcg / valid_items if valid_items > 0 else 0.0
        
        metrics = {
            'loss': loss.item(),
            'ndcg': avg_ndcg
        }
        
        return loss, metrics
    
    def _calculate_dcg(self, indices, scores):
        """Helper function to calculate DCG"""
        device = indices.device
        # Ranks are 1-based: 1, 2, 3, ...
        ranks = torch.arange(1, len(indices) + 1, dtype=torch.float, device=device)
        # Gain is the score of the item at that rank
        gain = scores[indices] # Use original scores
        # DCG = sum(gain_i / log2(rank_i + 1))
        return torch.sum(gain / torch.log2(ranks + 1))


class UnifiedLoss(nn.Module):
    """Base class for unified loss functions"""
    
    def __init__(self, **kwargs):
        super(UnifiedLoss, self).__init__()
        self.name = "unified_base_loss"
        
    def forward(self, model, batch, device):
        """
        Process batch and calculate loss based on model outputs
        
        Args:
            model: Model to use for forward pass
            batch: Raw batch from dataloader 
                  (format depends on batch transform)
            device: Device to run on
            
        Returns:
            loss: Loss tensor with gradient
            metrics: Dictionary of metrics for logging
        """
        raise NotImplementedError("Subclasses must implement forward method")
    
    def get_name(self):
        """Get loss function name for logging"""
        return self.name


class UnifiedInfoNCELoss(UnifiedLoss):
    """
    InfoNCE loss adapter for standard and multiple positives formats.
    Works with:
    - infonce_batch_transform: Standard InfoNCE with in-batch negatives
      Format: {'q_input_ids', 'a_input_ids', 'question_ids', 'ranks', 'scores'}
    
    - multiple_positives_batch_transform: Multiple positives per query
      Format: {'q_input_ids', 'a_input_ids', 'question_ids', 'ranks', 'scores'}
    """
    
    def __init__(self, temperature=0.1, **kwargs):
        super().__init__()
        self.temperature = temperature
        self.name = f"unified_infonce_t{temperature}"
        # Create wrapped loss function
        self.core_loss = FlexibleInfoNCELoss(temperature=temperature)
        
    def forward(self, model, batch, device):
        """
        Process batch and calculate InfoNCE loss
        
        Args:
            model: QA Embedding model
            batch: Batch from infonce or multiple_positives transform
            device: Device to run on
            
        Returns:
            loss: Loss tensor with gradient
            metrics: Dictionary with accuracy metrics
        """
        # Move tensors to device
        q_input_ids = batch['q_input_ids'].to(device)
        q_attention_mask = batch['q_attention_mask'].to(device)
        a_input_ids = batch['a_input_ids'].to(device)
        a_attention_mask = batch['a_attention_mask'].to(device)
        question_ids = batch['question_ids']
        
        # Get embeddings
        q_embeddings = model(q_input_ids, q_attention_mask)
        a_embeddings = model(a_input_ids, a_attention_mask)
        
        # Calculate loss using core loss function
        loss, metrics = self.core_loss(
            q_embeddings, a_embeddings, question_ids=question_ids
        )
        
        return loss, metrics


class UnifiedRankInfoNCELoss(UnifiedLoss):
    """
    Rank-weighted InfoNCE loss adapter for infonce and multiple_positives formats.
    Uses rank/score information to weight the positive examples.
    Works with:
    - infonce_batch_transform: Standard InfoNCE with in-batch negatives
      Format: {'q_input_ids', 'a_input_ids', 'question_ids', 'ranks', 'scores'}
    
    - multiple_positives_batch_transform: Multiple positives per query
      Format: {'q_input_ids', 'a_input_ids', 'question_ids', 'ranks', 'scores'}
    """
    
    def __init__(self, temperature=0.1, use_ranks=True, use_scores=False,
                 rank_weight=0.1, score_weight=0.01, **kwargs):
        super().__init__()
        self.temperature = temperature
        self.use_ranks = use_ranks
        self.use_scores = use_scores
        self.rank_weight = rank_weight
        self.score_weight = score_weight
        
        # Create descriptive name
        ranking_method = []
        if use_ranks:
            ranking_method.append(f"rank{rank_weight}")
        if use_scores:
            ranking_method.append(f"score{score_weight}")
        
        ranking_str = "_".join(ranking_method) if ranking_method else "standard"
        self.name = f"unified_rank_infonce_{ranking_str}_t{temperature}"
        
        # Create wrapped loss function
        self.core_loss = RankInfoNCELoss(
            temperature=temperature,
            use_ranks=use_ranks,
            use_scores=use_scores,
            rank_weight=rank_weight,
            score_weight=score_weight
        )
        
    def forward(self, model, batch, device):
        """
        Process batch and calculate rank-weighted InfoNCE loss
        
        Args:
            model: QA Embedding model
            batch: Batch from infonce or multiple_positives transform
            device: Device to run on
            
        Returns:
            loss: Loss tensor with gradient
            metrics: Dictionary with accuracy metrics
        """
        # Move tensors to device
        q_input_ids = batch['q_input_ids'].to(device)
        q_attention_mask = batch['q_attention_mask'].to(device)
        a_input_ids = batch['a_input_ids'].to(device)
        a_attention_mask = batch['a_attention_mask'].to(device)
        question_ids = batch['question_ids']
        
        # Get embeddings
        q_embeddings = model(q_input_ids, q_attention_mask)
        a_embeddings = model(a_input_ids, a_attention_mask)
        
        # Extract rank/score information if present and needed
        kwargs = {}
        if self.use_ranks and 'ranks' in batch:
            kwargs['ranks'] = batch['ranks'].to(device)
        if self.use_scores and 'scores' in batch:
            kwargs['scores'] = batch['scores'].to(device)
        
        # Calculate loss using core loss function
        loss, metrics = self.core_loss(
            q_embeddings, a_embeddings, 
            question_ids=question_ids,
            **kwargs
        )
        
        return loss, metrics


class UnifiedRelaxedHardNegativeLoss(UnifiedLoss):
    """
    Relaxed hard negative InfoNCE loss adapter.
    Works with:
    - hard_negative_batch_transform: Each question with multiple answers
      Format: List of dicts, each with {'q_input_ids', 'answers', 'question_id'}
      where answers is a list of {'input_ids', 'attention_mask', 'id', 'score', 'rank'}
    """
    
    def __init__(self, temperature=0.1, hard_neg_target_prob=0.1, **kwargs):
        super().__init__()
        self.temperature = temperature
        self.hard_neg_target_prob = hard_neg_target_prob
        self.name = f"unified_relaxed_hn_t{temperature}_p{hard_neg_target_prob}"
        
        # Create wrapped loss function
        self.core_loss = RelaxedHardNegativeInfoNCELoss(
            temperature=temperature,
            hard_neg_target_prob=hard_neg_target_prob
        )
        
    def forward(self, model, batch_items, device):
        """
        Process hard negative batch and calculate loss
        
        Args:
            model: QA Embedding model
            batch_items: List of items from hard_negative_batch_transform
            device: Device to run on
            
        Returns:
            loss: Loss tensor with gradient
            metrics: Dictionary with accuracy metrics
        """
        # Initialize loss and metrics accumulators
        total_loss = torch.tensor(0.0, device=device, requires_grad=True)
        total_metrics = {'loss': 0.0, 'acc': 0.0, 'groups': 0}
        total_items = 0
        
        # Process each question with its answers
        for item in batch_items:
            # Get question embedding
            q_input_ids = item['q_input_ids'].unsqueeze(0).to(device)
            q_attention_mask = item['q_attention_mask'].unsqueeze(0).to(device)
            q_embedding = model(q_input_ids, q_attention_mask)
            
            # Process all answers for this question
            a_embeddings = []
            question_ids = []
            
            for answer in item['answers']:
                a_input_ids = answer['input_ids'].unsqueeze(0).to(device)
                a_attention_mask = answer['attention_mask'].unsqueeze(0).to(device)
                a_embedding = model(a_input_ids, a_attention_mask)
                a_embeddings.append(a_embedding)
                question_ids.append(item['question_id'])
            
            # Stack answer embeddings
            if a_embeddings:
                a_embeddings = torch.cat(a_embeddings, dim=0)
                
                # Create repeated q_embedding to match a_embeddings
                q_repeated = q_embedding.repeat(len(a_embeddings), 1)
                
                # Calculate loss for this item
                item_loss, item_metrics = self.core_loss(
                    q_repeated, a_embeddings, question_ids=question_ids
                )
                
                # Accumulate loss and metrics
                if total_loss.item() == 0 and not total_loss.requires_grad:
                    # First item, replace the tensor
                    total_loss = item_loss
                else:
                    # Add to the existing tensor
                    total_loss = total_loss + item_loss
                
                for k, v in item_metrics.items():
                    total_metrics[k] += v
                
                total_items += 1
        
        # Average metrics
        if total_items > 0:
            loss = total_loss / total_items
            for k in total_metrics:
                total_metrics[k] /= total_items
        else:
            loss = torch.tensor(0.0, device=device, requires_grad=True)
        
        # Ensure loss is in metrics dict
        total_metrics['loss'] = loss.item()
        
        return loss, total_metrics


class UnifiedTripletLoss(UnifiedLoss):
    """
    Triplet loss adapter.
    Works with:
    - triplet_batch_transform: Query, positive, negative triplets
      Format: {'q_input_ids', 'a_pos_input_ids', 'a_neg_input_ids', 
              'q_attention_mask', 'a_pos_attention_mask', 'a_neg_attention_mask'}
    """
    
    def __init__(self, margin=0.3, **kwargs):
        super().__init__()
        self.margin = margin
        self.name = f"unified_triplet_m{margin}"
        
        # Create wrapped loss function
        self.core_loss = TripletLoss(margin=margin)
        
    def forward(self, model, batch, device):
        """
        Process triplet batch and calculate loss
        
        Args:
            model: QA Embedding model
            batch: Batch from triplet_batch_transform
            device: Device to run on
            
        Returns:
            loss: Loss tensor with gradient
            metrics: Dictionary with accuracy and similarity metrics
        """
        # Move tensors to device
        q_input_ids = batch['q_input_ids'].to(device)
        q_attention_mask = batch['q_attention_mask'].to(device)
        a_pos_input_ids = batch['a_pos_input_ids'].to(device)
        a_pos_attention_mask = batch['a_pos_attention_mask'].to(device)
        a_neg_input_ids = batch['a_neg_input_ids'].to(device)
        a_neg_attention_mask = batch['a_neg_attention_mask'].to(device)
        
        # Get embeddings
        q_embeddings = model(q_input_ids, q_attention_mask)
        a_pos_embeddings = model(a_pos_input_ids, a_pos_attention_mask)
        a_neg_embeddings = model(a_neg_input_ids, a_neg_attention_mask)
        
        # Calculate loss using core loss function
        loss, metrics = self.core_loss(
            q_embeddings, a_pos_embeddings, a_neg_embeddings
        )
        
        return loss, metrics


class UnifiedListwiseRankingLoss(UnifiedLoss):
    """
    Listwise ranking loss adapter.
    Works with:
    - listwise_batch_transform: Each question with multiple scored answers
      Format: List of dicts, each with {'q_input_ids', 'a_input_ids', 
              'q_attention_mask', 'a_attention_masks', 'scores', 'question_id'}
    """
    
    def __init__(self, temperature=1.0, **kwargs):
        super().__init__()
        self.temperature = temperature
        self.name = f"unified_listwise_t{temperature}"
        
        # Create wrapped loss function
        self.core_loss = ListwiseRankingLoss(temperature=temperature)
        
    def forward(self, model, batch_items, device):
        """
        Process listwise batch and calculate loss
        
        Args:
            model: QA Embedding model
            batch_items: List of items from listwise_batch_transform
            device: Device to run on
            
        Returns:
            loss: Loss tensor with gradient
            metrics: Dictionary with NDCG and other metrics
        """
        # Prepare data for listwise loss
        q_embeddings = []
        a_list_embeddings = []
        a_list_scores = []
        
        # Process each item
        for item in batch_items:
            # Get question embedding
            q_input_ids = item['q_input_ids'].unsqueeze(0).to(device)
            q_attention_mask = item['q_attention_mask'].unsqueeze(0).to(device)
            q_embedding = model(q_input_ids, q_attention_mask)
            
            # Get answer embeddings
            a_input_ids = item['a_input_ids'].to(device)
            a_attention_masks = item['a_attention_masks'].to(device)
            a_embeddings = model(a_input_ids, a_attention_masks)
            
            # Add to lists
            q_embeddings.append(q_embedding)
            a_list_embeddings.append(a_embeddings)
            a_list_scores.append(item['scores'].to(device))
        
        # Skip empty batch
        if not q_embeddings:
            return torch.tensor(0.0, device=device, requires_grad=True), {'loss': 0.0, 'ndcg': 0.0}
        
        # Calculate loss using core loss function
        loss, metrics = self.core_loss(
            q_embeddings, a_list_embeddings, a_list_scores
        )
        
        return loss, metrics


def create_unified_loss(loss_name, **kwargs):
    """
    Factory function to create a unified loss by name
    
    Args:
        loss_name: Name of the loss function
        **kwargs: Additional parameters for the loss
        
    Returns:
        UnifiedLoss: Loss function instance
        
    Raises:
        ValueError: If the loss name is not recognized
    """
    # Define mappings from loss names to unified loss classes
    losses = {
        'infonce': UnifiedInfoNCELoss,
        'rank_infonce': UnifiedRankInfoNCELoss,
        'relaxed_hard_neg': UnifiedRelaxedHardNegativeLoss,
        'triplet': UnifiedTripletLoss,
        'listwise': UnifiedListwiseRankingLoss
    }
    
    # Handle named variants with parameters embedded in the name
    # rank_infonce_rank0.1_score0.05_t0.2
    if loss_name.startswith('rank_infonce_'):
        options = loss_name.split('_')[2:] # Skip 'rank_infonce'
        params = {
            'temperature': kwargs.get('temperature', 0.1),
            'use_ranks': False,
            'use_scores': False,
            'rank_weight': 0.1,
            'score_weight': 0.01
        }
        
        for option in options:
            if option.startswith('rank'):
                params['use_ranks'] = True
                try: params['rank_weight'] = float(option[4:])
                except (ValueError, IndexError): pass
            elif option.startswith('score'):
                params['use_scores'] = True
                try: params['score_weight'] = float(option[5:])
                except (ValueError, IndexError): pass
            elif option.startswith('t'):
                try: params['temperature'] = float(option[1:])
                except (ValueError, IndexError): pass
        
        params.update(kwargs) # Allow explicit kwargs to override parsed ones
        return UnifiedRankInfoNCELoss(**params)
    
    # relaxed_hard_neg_p0.2_t0.1
    if loss_name.startswith('relaxed_hard_neg_'):
        options = loss_name.split('_')[3:] # Skip 'relaxed_hard_neg'
        params = {
            'temperature': kwargs.get('temperature', 0.1),
            'hard_neg_target_prob': kwargs.get('hard_neg_target_prob', 0.1)
        }
        
        for option in options:
            if option.startswith('t'):
                try: params['temperature'] = float(option[1:])
                except (ValueError, IndexError): pass
            elif option.startswith('p'):
                try: params['hard_neg_target_prob'] = float(option[1:])
                except (ValueError, IndexError): pass
        
        params.update(kwargs)
        return UnifiedRelaxedHardNegativeLoss(**params)
    
    # Try simple lookup
    if loss_name in losses:
        return losses[loss_name](**kwargs)
    
    # If no match found
    available = list(losses.keys()) + ["rank_infonce_...", "relaxed_hard_neg_..."]
    raise ValueError(f"Unknown loss function: {loss_name}. Available losses: {available}")

---
./src/rank_test/models.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
from transformers import DistilBertModel

class QAEmbeddingModel(nn.Module):
    """
    Model for embedding questions and answers using DistilBERT
    with a projection layer to reduce dimensionality.
    """
    def __init__(self, embed_dim=768, projection_dim=128):
        super(QAEmbeddingModel, self).__init__()
        # Load pretrained DistilBERT
        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')
        # Projection layer to get embeddings of the desired dimension
        self.projection = nn.Linear(embed_dim, projection_dim)
        
    def forward(self, input_ids, attention_mask):
        """
        Generate embeddings for input text
        
        Args:
            input_ids: Token IDs from tokenizer
            attention_mask: Attention mask from tokenizer
            
        Returns:
            normalized_embeddings: L2-normalized embeddings
        """
        # Get BERT embeddings
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        # Use the [CLS] token embedding (first token)
        embeddings = outputs.last_hidden_state[:, 0, :]
        # Project to lower dimension
        projected = self.projection(embeddings)
        # Normalize embeddings to unit length
        normalized = nn.functional.normalize(projected, p=2, dim=1)
        return normalized

---
./src/rank_test/train.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Simplified training module for QA ranking models.
Provides a streamlined training process with unified loss functions.
"""

import time
import torch
import torch.optim as optim
import wandb
import json
import random
from tqdm import tqdm
from transformers import DistilBertTokenizerFast

from rank_test.config import ExperimentConfig
from rank_test.models import QAEmbeddingModel
from rank_test.dataset import (
    QADataset, 
    ensure_dataset_exists
)
from rank_test.transforms import get_batch_transform
from rank_test.losses import create_unified_loss
from rank_test.evaluate import evaluate_model
from argdantic import ArgParser

def get_device():
    """Get appropriate device for training"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS (Metal Performance Shaders) on Mac")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("Using CUDA (NVIDIA GPU)")
    else:
        device = torch.device("cpu")
        print("Using CPU (No GPU acceleration available)")
    
    return device


def create_dataloaders(config: ExperimentConfig):
    """
    Create train and test dataloaders based on configuration
    
    Args:
        config: ExperimentConfig object
        
    Returns:
        Tuple of (train_loader, test_loader)
    """
    # Ensure dataset exists
    data_path = config.data_path
    ensure_dataset_exists(
        data_path=data_path,
        data_limit=config.get_limit(),
        force_regenerate=config.force_regenerate
    )
    
    # Create tokenizer
    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
    
    # Get batch transform function
    batch_transform_fn = get_batch_transform(config.batch_transform)
    
    # Load all data
    with open(data_path, 'r') as f:
        all_data = json.load(f)
    
    # Split data based on test size
    num_items = len(all_data)
    test_size = config.test_size
    test_count = int(num_items * test_size)
    
    # Create indices and shuffle
    indices = list(range(num_items))
    random.seed(config.seed)  # Use seed for reproducibility
    random.shuffle(indices)
    
    # Split indices
    test_indices = indices[:test_count]
    train_indices = indices[test_count:]
    
    # Create train and test datasets
    train_data = [all_data[i] for i in train_indices]
    test_data = [all_data[i] for i in test_indices]
    
    print(f"Splitting dataset: {len(train_data)} training samples, {len(test_data)} test samples")
    
    # Create training dataset
    print("Creating training dataset")
    train_dataset = QADataset(
        data_path=data_path,
        batch_transform_fn=batch_transform_fn,
        batch_size=config.get_batch_size(),
        tokenizer=tokenizer,
        max_length=128,
        **config.get_batch_transform_kwargs()
    )
    # Override raw data with train split
    train_dataset.raw_data = train_data
    # Recreate batches with train data
    train_dataset.batches = train_dataset._create_batches()
    
    # Create dataloader
    train_loader = QADataset.get_dataloader(train_dataset, shuffle=True)
    
    # Create standardized test dataset
    print("Creating standardized test dataset")
    # Always use the standardized test transform regardless of training strategy
    test_transform_fn = get_batch_transform("standardized_test")
    test_batch_size = min(len(test_data), config.get_batch_size() * 4)  # Use larger batches for testing
    
    test_dataset = QADataset(
        data_path=data_path,
        batch_transform_fn=test_transform_fn,
        batch_size=test_batch_size,
        tokenizer=tokenizer,
        max_length=128
    )
    # Override raw data with test split
    test_dataset.raw_data = test_data
    # Recreate batches with test data
    test_dataset.batches = test_dataset._create_batches()
    
    test_loader = QADataset.get_dataloader(test_dataset, shuffle=False)
    
    print(f"Created training dataset with {len(train_dataset)} batches")
    print(f"Test dataset: {len(test_dataset)} batches")
    
    return train_loader, test_loader

cli = ArgParser()
@cli.command()
def train(config: ExperimentConfig):
    """
    Simplified training function that uses unified loss functions
    
    Args:
        config: ExperimentConfig object
        
    Returns:
        Trained model
    """
    # Setup device, dataset, model
    device = get_device()
    
    # Initialize wandb if enabled
    if config.log_to_wandb and wandb.run is None:
        run_name = f"{config.loss_type}-{config.batch_transform}-{time.strftime('%Y%m%d-%H%M%S')}"
        if config.name:
            run_name = f"{config.name}-{time.strftime('%Y%m%d-%H%M%S')}"
        
        # Create a clean config for wandb logging
        wandb_config = config.dict()
        
        wandb.init(
            project=config.wandb_project,
            name=run_name,
            config=wandb_config
        )
        
        # Log loss parameters as a table
        loss_kwargs = config.get_loss_kwargs()
        if loss_kwargs:
            loss_params = [[k, str(v)] for k, v in loss_kwargs.items()]
            wandb.log({"loss_parameters": wandb.Table(columns=["Parameter", "Value"], 
                                                    data=loss_params)})
    
    # Create model
    print(f"Creating model with embed_dim={config.embed_dim} and projection_dim={config.projection_dim}")
    model = QAEmbeddingModel(
        embed_dim=config.embed_dim,
        projection_dim=config.projection_dim
    ).to(device)
    
    # Create dataset and dataloader
    train_loader, test_loader = create_dataloaders(config)
    
    # Create loss function with unified interface
    loss_fn = create_unified_loss(config.loss_type, **config.get_loss_kwargs())
    print(f"Using loss function: {loss_fn.get_name()}")
    
    # Create optimizer
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    # Initial evaluation (if enabled)
    if config.eval_at_zero and test_loader is not None:
        print("\nRunning evaluation at step 0 (initial model)...")
        try:
            model.eval()
            
            # Run evaluation
            initial_metrics = evaluate_model(model, test_loader, device)
            
            # Print metrics
            print("Step 0 evaluation results:")
            for metric_name, metric_value in initial_metrics.items():
                if 'hard_neg' in metric_name:
                    print(f"  {metric_name}: {metric_value:.4f}*")  # Mark fixed metrics
                else:
                    print(f"  {metric_name}: {metric_value:.4f}")
            
            # Log metrics to wandb
            if wandb.run is not None:
                initial_metrics_log = {f"eval/{k}": v for k, v in initial_metrics.items()}
                initial_metrics_log["step"] = 0
                wandb.log(initial_metrics_log)
                
            model.train()
        except Exception as e:
            print(f"Error during initial evaluation: {e}")
            model.train()
    
    # Training loop
    global_step = 0
    cumulative_docs = 0
    
    # Main training loop - epoch based with step counter
    for epoch in range(config.get_epochs()):
        for batch_data in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            # Extract batch and document count
            batch_doc_count = 0
            if isinstance(batch_data, tuple) and len(batch_data) == 2:
                batch, batch_doc_count = batch_data
            else:
                batch = batch_data
                batch_doc_count = len(batch) * 2  # Estimate
            
            # Add batch doc count to running total
            cumulative_docs += batch_doc_count
            
            # Process batch
            model.train()
            batch_start = time.time()
            
            # The key simplification: Unified interface for all loss functions
            # Loss function handles different batch formats internally
            loss, batch_metrics = loss_fn(model, batch, device)
            
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Update global step
            global_step += 1
            
            # Calculate batch time
            batch_time = time.time() - batch_start
            
            # Log metrics for this batch
            batch_metrics_log = {f"batch/{k}": v for k, v in batch_metrics.items()}
            batch_metrics_log["batch/loss"] = loss.item()
            batch_metrics_log["step"] = global_step
            batch_metrics_log["epoch"] = epoch + 1
            batch_metrics_log["cumulative_docs_seen"] = cumulative_docs
            batch_metrics_log["docs_per_step"] = cumulative_docs / max(global_step, 1)
            batch_metrics_log["batch/time"] = batch_time
            
            # Update progress bar with latest metrics
            progress_desc = f"Epoch {epoch+1} | Step {global_step} | "
            if 'acc' in batch_metrics:
                progress_desc += f"Acc: {batch_metrics['acc']:.4f} | "
            elif 'accuracy' in batch_metrics:
                progress_desc += f"Acc: {batch_metrics['accuracy']:.4f} | "
            elif 'ndcg' in batch_metrics:
                progress_desc += f"NDCG: {batch_metrics['ndcg']:.4f} | "
            progress_desc += f"Loss: {loss.item():.4f}"
            
            # Log to wandb
            if wandb.run is not None:
                wandb.log(batch_metrics_log)
            
            # Run evaluation if requested
            if config.eval_steps and test_loader and global_step % config.eval_steps == 0:
                # Temporarily switch to eval mode
                model.eval()
                
                print(f"\nRunning evaluation at step {global_step}...")
                try:
                    # Run evaluation
                    test_metrics = evaluate_model(model, test_loader, device)
                    
                    # Print metrics
                    print(f"Step {global_step} evaluation results:")
                    for metric_name, metric_value in test_metrics.items():
                        if 'hard_neg' in metric_name:
                            print(f"  {metric_name}: {metric_value:.4f}*") # Mark fixed metrics
                        else:
                            print(f"  {metric_name}: {metric_value:.4f}")
                    
                    # Log metrics
                    if wandb.run is not None:
                        test_metrics_log = {f"eval/{k}": v for k, v in test_metrics.items()}
                        test_metrics_log["step"] = global_step
                        wandb.log(test_metrics_log)
                
                except Exception as e:
                    print(f"Error during evaluation: {e}")
                
                # Switch back to training mode
                model.train()
    
    # Final evaluation
    if test_loader is not None:
        print("\nRunning final evaluation...")
        model.eval()
        final_metrics = evaluate_model(model, test_loader, device)
        
        # Print final metrics
        print("\nFinal metrics:")
        for k, v in final_metrics.items():
            if 'hard_neg' in k:
                print(f"  {k}: {v:.4f}*")  # Mark fixed metrics with an asterisk
            else:
                print(f"  {k}: {v:.4f}")
        
        # Log final metrics to wandb
        if wandb.run is not None:
            wandb.log({f"final/{k}": v for k, v in final_metrics.items()})
            wandb.log({"step": global_step})
    
    # Finish wandb run
    if wandb.run is not None:
        wandb.finish()
    
    return model


if __name__ == "__main__":
    cli()

---
./src/rank_test/transforms.py
---
#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Batch transformation strategies for QA ranking tasks.

This module provides various batch transformation strategies for QA ranking tasks.
These transformations prepare data in different formats suitable for different loss functions:

1. infonce - Standard InfoNCE contrastive learning with in-batch negatives
2. multiple_positives - Handles multiple positive answers per question with rank weighting
3. hard_negative - Explicitly incorporates hard negatives (low-ranked answers to same question)
4. triplet - Creates triplets of (query, positive, negative) for triplet loss
5. listwise - Prepares data for listwise ranking losses, handling multiple ranked answers per query
6. standardized_test - Creates a standardized test batch format for consistent evaluation

Each transformation can be used with the QADataset by setting the appropriate batch_transform_fn.
"""

import torch
from typing import List, Dict, Callable, Tuple, Optional, Union
from collections import defaultdict
from tqdm import tqdm
import re

def clean_html(text: str) -> str:
    """Remove HTML tags from text"""
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def infonce_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    take_top: bool = True,
    **kwargs
) -> Tuple[Dict, int]:
    """
    Standard InfoNCE transform with batched tokenization for better performance.
    
    Creates batches where:
    - Each question is paired with its top (or random) answer
    - During training, negatives come from other questions in the same batch
    - Original ranks and scores are preserved for potential weighting
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        take_top: If True, use the highest-ranked answer, otherwise random
        
    Returns:
        Tuple of (batch dictionary with tokenized inputs, document count)
    """
    # Collect all texts first
    questions_to_tokenize = []
    answers_to_tokenize = []
    question_ids = []
    answer_ids = []
    scores = []
    ranks = []
    
    # Document counter
    doc_count = 0
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        
        answers = item['answers']
        if not answers:
            continue
            
        # Sort answers by score (descending) to establish ranks
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
        
        if not take_top and len(sorted_answers) > 1:
            selected_answer = random.choice(sorted_answers)
            rank = sorted_answers.index(selected_answer)
        else:
            selected_answer = sorted_answers[0]
            rank = 0
        
        a_text = clean_html(selected_answer['body'])
        
        # Collect texts and metadata
        questions_to_tokenize.append(q_text)
        answers_to_tokenize.append(a_text)
        question_ids.append(question['id'])
        answer_ids.append(selected_answer['id'])
        scores.append(float(selected_answer['score']))
        ranks.append(rank)
        
        # Count documents (1 question + 1 answer)
        doc_count += 2
    
    if not questions_to_tokenize:
        return None, 0
        
    # Batch tokenize questions
    q_encodings = tokenizer(
        questions_to_tokenize,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # Batch tokenize answers
    a_encodings = tokenizer(
        answers_to_tokenize,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # Create final batch dictionary
    batch = {
        'q_input_ids': q_encodings['input_ids'],
        'q_attention_mask': q_encodings['attention_mask'],
        'a_input_ids': a_encodings['input_ids'],
        'a_attention_mask': a_encodings['attention_mask'],
        'question_ids': question_ids,
        'answer_ids': answer_ids,
        'scores': torch.tensor(scores, dtype=torch.float32),
        'ranks': torch.tensor(ranks, dtype=torch.long)
    }
    
    return batch, doc_count


def multiple_positives_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    pos_count: int = 3,
    **kwargs
) -> Tuple[Dict, int]:
    """
    Multiple positives transform for contrastive learning.
    
    Creates batches where:
    - Each question appears multiple times with different positive answers
    - Each question-answer pair maintains both cardinal scores and ordinal ranks
    - Useful for models that need to learn subtle differences between answers
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        pos_count: Maximum number of positive answers to include per question
        
    Returns:
        Tuple of (batch dictionary with tokenized inputs, document count)
    """
    q_input_ids, q_attention_mask = [], []
    a_input_ids, a_attention_mask = [], []
    question_ids, answer_ids = [], []
    scores, ranks = [], []
    
    # Document counter
    doc_count = 0
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if not answers:
            continue
            
        # Sort answers by score (descending) to establish ranks
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
        
        # Take top K answers as positives (or fewer if not enough answers)
        positives = sorted_answers[:min(pos_count, len(sorted_answers))]
        
        # Count documents (1 question + N answers)
        doc_count += 1 + len(positives)
            
        # For each positive answer
        for rank, answer in enumerate(positives):
            a_text = clean_html(answer['body'])
            
            # Tokenize
            q_encoding = tokenizer(q_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            
            # Add to batch
            q_input_ids.append(q_encoding['input_ids'])
            q_attention_mask.append(q_encoding['attention_mask'])
            a_input_ids.append(a_encoding['input_ids'])
            a_attention_mask.append(a_encoding['attention_mask'])
            question_ids.append(q_id)
            answer_ids.append(answer['id'])
            scores.append(float(answer['score']))  # Cardinal score
            ranks.append(rank)                    # Ordinal rank (position)
    
    if not q_input_ids:
        return None, 0
        
    # Create final batch
    batch = {
        'q_input_ids': torch.cat(q_input_ids, dim=0),
        'q_attention_mask': torch.cat(q_attention_mask, dim=0),
        'a_input_ids': torch.cat(a_input_ids, dim=0),
        'a_attention_mask': torch.cat(a_attention_mask, dim=0),
        'question_ids': question_ids,
        'answer_ids': answer_ids,
        'scores': torch.tensor(scores, dtype=torch.float32),  # Cardinal scores 
        'ranks': torch.tensor(ranks, dtype=torch.long)       # Ordinal ranks
    }
    
    return batch, doc_count


def hard_negative_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    **kwargs
) -> Dict:
    """
    Hard negative transform that explicitly includes lower-ranked 
    answers as hard negatives.
    
    Creates batches where:
    - Each question includes its top answer and lower-ranked answers
    - Each answer maintains both cardinal scores and ordinal ranks
    - Designed for loss functions that need to handle hard negatives explicitly
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        
    Returns:
        Batch dictionary with tokenized inputs for all answers per question
    """
    batch_items = []
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if len(answers) <= 1:
            continue  # Need multiple answers
        
        # Sort answers by score to establish ranks
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
            
        # Tokenize question once
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        
        # Tokenize all answers
        a_encodings = []
        for rank, answer in enumerate(sorted_answers):
            a_text = clean_html(answer['body'])
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            
            # Store answer data
            a_encodings.append({
                'input_ids': a_encoding['input_ids'].squeeze(0),
                'attention_mask': a_encoding['attention_mask'].squeeze(0),
                'id': answer['id'],
                'score': float(answer['score']),    # Cardinal score (actual value)
                'rank': rank                       # Ordinal rank (position)
            })
        
        # Create batch item with question and all its answers
        batch_items.append({
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'answers': a_encodings,
            'question_id': q_id,
            'answer_count': len(a_encodings)
        })
    
    return batch_items


def triplet_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    neg_strategy: str = "hard_negative",
    **kwargs
) -> Dict:
    """
    Triplet transform for triplet loss learning.
    
    Creates batches of triplets where:
    - Each question is paired with a positive and negative answer
    - Different strategies for selecting negatives are supported
    - Specifically designed for triplet loss functions
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        neg_strategy: Strategy for selecting negatives:
                     "hard_negative" - use lower-ranked answer to same question
                     "in_batch" - use answer from different question
                     "mixed" - randomly mix both strategies
        
    Returns:
        Batch dictionary with tokenized triplets
    """
    import random
    
    q_input_ids, q_attention_mask = [], []
    a_pos_input_ids, a_pos_attention_mask = [], []
    a_neg_input_ids, a_neg_attention_mask = [], []
    question_ids, pos_scores, neg_scores = [], [], []
    
    # Group answers by question for sampling
    question_answers = {}
    for item in batch_data:
        q_id = item['question']['id']
        question_answers[q_id] = item
    
    # Question IDs in this batch
    batch_q_ids = list(question_answers.keys())
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if not answers:
            continue
            
        # Get positive (top answer)
        pos_answer = answers[0]
        pos_text = clean_html(pos_answer['body'])
        
        # Get negative based on strategy
        neg_text = None
        neg_score = 0
        
        if neg_strategy == "hard_negative":
            # Use lower ranked answer from same question
            if len(answers) > 1:
                neg_answer = answers[-1]  # Lowest ranked
                neg_text = clean_html(neg_answer['body'])
                neg_score = float(neg_answer['score'])
                
        elif neg_strategy == "in_batch":
            # Use answer from different question
            other_questions = [q for q in batch_q_ids if q != q_id]
            if other_questions:
                other_q_id = random.choice(other_questions)
                other_answers = question_answers[other_q_id]['answers']
                if other_answers:
                    other_answer = other_answers[0]  # Top answer from other question
                    neg_text = clean_html(other_answer['body'])
                    neg_score = float(other_answer['score'])
                    
        elif neg_strategy == "mixed":
            # 50% chance to use each strategy
            if random.random() < 0.5 and len(answers) > 1:
                # Hard negative
                neg_answer = answers[-1]
                neg_text = clean_html(neg_answer['body'])
                neg_score = float(neg_answer['score'])
            else:
                # Other question negative
                other_questions = [q for q in batch_q_ids if q != q_id]
                if other_questions:
                    other_q_id = random.choice(other_questions)
                    other_answers = question_answers[other_q_id]['answers']
                    if other_answers:
                        other_answer = other_answers[0]
                        neg_text = clean_html(other_answer['body'])
                        neg_score = float(other_answer['score'])
        
        # Skip if no negative found
        if not neg_text:
            continue
            
        # Tokenize
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        pos_encoding = tokenizer(pos_text, max_length=max_length, 
                               padding='max_length', truncation=True, return_tensors='pt')
        neg_encoding = tokenizer(neg_text, max_length=max_length, 
                               padding='max_length', truncation=True, return_tensors='pt')
        
        # Add to batch
        q_input_ids.append(q_encoding['input_ids'])
        q_attention_mask.append(q_encoding['attention_mask'])
        a_pos_input_ids.append(pos_encoding['input_ids'])
        a_pos_attention_mask.append(pos_encoding['attention_mask'])
        a_neg_input_ids.append(neg_encoding['input_ids'])
        a_neg_attention_mask.append(neg_encoding['attention_mask'])
        question_ids.append(q_id)
        pos_scores.append(float(pos_answer['score']))
        neg_scores.append(neg_score)
    
    if not q_input_ids:
        return None
        
    # Create final batch
    batch = {
        'q_input_ids': torch.cat(q_input_ids, dim=0),
        'q_attention_mask': torch.cat(q_attention_mask, dim=0),
        'a_pos_input_ids': torch.cat(a_pos_input_ids, dim=0),
        'a_pos_attention_mask': torch.cat(a_pos_attention_mask, dim=0),
        'a_neg_input_ids': torch.cat(a_neg_input_ids, dim=0),
        'a_neg_attention_mask': torch.cat(a_neg_attention_mask, dim=0),
        'question_ids': question_ids,
        'pos_scores': torch.tensor(pos_scores, dtype=torch.float32),
        'neg_scores': torch.tensor(neg_scores, dtype=torch.float32)
    }
    
    return batch


def listwise_batch_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int, 
    max_answers: int = 5,
    **kwargs
) -> Dict:
    """
    Listwise transform for listwise ranking losses.
    
    Creates batches where:
    - Each question has multiple answers with scores
    - Answers are ranked by their original scores
    - Designed for listwise ranking loss functions
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        max_answers: Maximum number of answers to include per question
        
    Returns:
        Batch dictionary with tokenized questions and multiple answers
    """
    batch_items = []
    
    for item in batch_data:
        question = item['question']
        q_text = question['title'] + " " + clean_html(question['body'])
        q_id = question['id']
        
        answers = item['answers']
        if len(answers) < 2:  # Need at least 2 answers for ranking
            continue
            
        # Limit number of answers
        answers = answers[:min(max_answers, len(answers))]
        
        # Tokenize question
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        
        # Tokenize all answers
        a_input_ids = []
        a_attention_masks = []
        scores = []
        
        for answer in answers:
            a_text = clean_html(answer['body'])
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                 padding='max_length', truncation=True, return_tensors='pt')
            
            a_input_ids.append(a_encoding['input_ids'])
            a_attention_masks.append(a_encoding['attention_mask'])
            scores.append(float(answer['score']))
        
        # Stack answer tensors
        a_input_ids = torch.cat(a_input_ids, dim=0)
        a_attention_masks = torch.cat(a_attention_masks, dim=0)
        scores = torch.tensor(scores, dtype=torch.float32)
        
        # Normalize scores to [0, 1]
        if torch.max(scores) > 0:
            scores = scores / torch.max(scores)
        
        # Add to batch
        batch_items.append({
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'a_input_ids': a_input_ids,
            'a_attention_masks': a_attention_masks,
            'scores': scores,
            'question_id': q_id,
            'answer_count': len(scores)
        })
    
    return batch_items


def standardized_test_transform(
    batch_data: List[Dict], 
    tokenizer, 
    max_length: int,
    **kwargs
) -> Tuple[Dict, int]:
    """
    Creates a standardized test batch with positive, hard negative, and 
    normal negative samples for each question.
    
    This test transform is strategy-agnostic and provides consistent
    evaluation across all training approaches. The format includes:
    - Original positive answer (highest scored)
    - Hard negative answers (lower-ranked answers to same question)
    - Normal negatives (answers from other questions)
    
    Args:
        batch_data: List of raw data items with questions and ranked answers
        tokenizer: Tokenizer for processing text
        max_length: Maximum sequence length
        
    Returns:
        Tuple of (batch dict for standardized evaluation, document count)
    """
    test_items = []
    
    for item in batch_data:
        question = item['question']
        q_id = question['id']
        q_text = question['title'] + " " + clean_html(question['body'])
        
        # Get all answers for this question, sorted by score
        answers = item['answers']
        if len(answers) < 2:
            continue
            
        sorted_answers = sorted(answers, key=lambda a: float(a['score']), reverse=True)
        
        # Tokenize question
        q_encoding = tokenizer(q_text, max_length=max_length, 
                             padding='max_length', truncation=True, return_tensors='pt')
        
        # Get all answers
        all_answers = []
        
        # Top answer is positive
        if sorted_answers:
            pos_answer = sorted_answers[0]
            pos_text = clean_html(pos_answer['body'])
            pos_encoding = tokenizer(pos_text, max_length=max_length, 
                                  padding='max_length', truncation=True, return_tensors='pt')
            
            all_answers.append({
                'input_ids': pos_encoding['input_ids'].squeeze(0),
                'attention_mask': pos_encoding['attention_mask'].squeeze(0),
                'score': float(pos_answer['score']),
                'rank': 0,
                'answer_id': pos_answer['id'],
                'is_positive': True,
                'is_hard_negative': False
            })
        
        # Add hard negatives (lower-ranked answers to same question)
        for i, answer in enumerate(sorted_answers[1:min(6, len(sorted_answers))]):  # Up to 5 hard negatives
            a_text = clean_html(answer['body'])
            a_encoding = tokenizer(a_text, max_length=max_length, 
                                padding='max_length', truncation=True, return_tensors='pt')
            
            all_answers.append({
                'input_ids': a_encoding['input_ids'].squeeze(0),
                'attention_mask': a_encoding['attention_mask'].squeeze(0),
                'score': float(answer['score']),
                'rank': i+1,
                'answer_id': answer['id'],
                'is_positive': False,
                'is_hard_negative': True
            })
        
        # Add test item
        test_items.append({
            'q_input_ids': q_encoding['input_ids'].squeeze(0),
            'q_attention_mask': q_encoding['attention_mask'].squeeze(0),
            'question_id': q_id,
            'answers': all_answers
        })
    
    # For batch-level negative sampling, add answers from other questions
    # as normal negatives to each question's answer pool
    for i, item in enumerate(test_items):
        for j, other_item in enumerate(test_items):
            if i != j:  # Different question
                # Add top answer from other question as a normal negative
                other_answers = other_item['answers']
                if other_answers:
                    other_top = other_answers[0]  # Top answer
                    
                    # Add to this question's answer pool
                    item['answers'].append({
                        'input_ids': other_top['input_ids'],
                        'attention_mask': other_top['attention_mask'],
                        'score': 0.0,  # Lower score for negatives
                        'rank': 999,  # High rank for negatives
                        'answer_id': other_top.get('answer_id', 'unknown'),
                        'is_positive': False,
                        'is_hard_negative': False,
                        'from_question_id': other_item['question_id']
                    })
    
    # Count documents - for eval we don't track this as closely
    doc_count = 0
    for item in test_items:
        # One question + all its answers
        doc_count += 1 + len(item['answers'])
    
    return test_items, doc_count


# Factory function to get the transform function by name
def get_batch_transform(transform_name: str) -> Callable:
    """
    Get a batch transform function by name
    
    Args:
        transform_name: Name of the transform function
        
    Returns:
        Batch transform function
        
    Raises:
        ValueError: If the transform name is not recognized
    """
    transforms = {
        'infonce': infonce_batch_transform,
        'multiple_positives': multiple_positives_batch_transform,
        'hard_negative': hard_negative_batch_transform,
        'triplet': triplet_batch_transform,
        'listwise': listwise_batch_transform,
        'standardized_test': standardized_test_transform
    }
    
    if transform_name not in transforms:
        raise ValueError(f"Unknown transform: {transform_name}. Available transforms: {list(transforms.keys())}")
    
    return transforms[transform_name]

---
